2025-04-12 15:27
Status: #idea
Tags: [[分布式]] [[分布式基础理论]]


# 1 什么是共识
分布式系统中充满了各种潜在的错误场景，网络数据包可能丢失、顺序紊乱、重复发送或者延迟，节点还可能宕机。“在充满不确定性的环境中，就某个决策达成共识”是软件工程领域最具挑战性的问题之一。

在汉语中，“共识”和“一致”意思相似，但在计算机领域，它们具有截然不同的含义。
- **共识**（Consensus）：指所有节点就某项操作（如选主、原子事务提交、日志复制、分布式锁管理等）达成一致的实现过程。
- **一致性**（Consistency）：描述多个节点的数据是否保持一致，关注数据最终达到稳定状态的结果。

在分布式系统中，节点故障是不可避免的，但部分节点故障不应该影响系统整体状态。通过增加节点数量，依据“少数服从多数”原则，只要多数节点（至少 N/2+1N）达成一致，其状态即可代表整个系统。这种依赖多数节点实现容错的机制称为 **Quorum 机制**。
> [!note] Quorum 机制
> - 3 节点集群：Quorum 为 2，允许 1 个节点故障。
> - 4 节点集群：Quorum 为 ⌈4/2⌉+1=3⌈4/2⌉+1=3，允许 1 个节点故障。
> - 5 节点集群：Quorum 为 ⌈5/2⌉+1=3⌈5/2⌉+1=3，允许 2 个节点故障。
> 
> 你注意到了吗？3 节点和 4 节点集群的故障容忍能力一样。因此，通常情况下，针对容错的分布式系统无需使用 4 个节点。

基于 Quorum 的机制，通过“少数服从多数”协商机制达成一致的决策，从而对外表现为一致的运行结果。这一过程被称为节点间的“协商共识”。一旦解决共识问题，便可提供一套屏蔽内部复杂性的抽象机制，为应用层提供一致性保证，满足多种需求。
- **主节点选举**：在主从复制数据库中，所有节点需要就“谁来当主节点”达成一致。如果由于网络问题导致节点间无法通信，很容易引发争议。若争议未解决，可能会出现多个节点同时认为自己是主节点的情况，这就是分布式系统中最棘手的问题之一 —— “脑裂”。
- **原子事务提交**：对于支持跨节点或跨分区事务的数据库，可能会发生部分节点事务成功、部分节点事务失败的情况。为维护事务的原子性（即 ACID 特性），所有节点必须就事务的最终结果达成一致。
- **分布式锁管理**：当多个请求尝试访问共享资源时，共识机制可确保所有节点一致认定“谁成功获取了锁”。即使发生网络故障或节点异常，也能避免锁争议，从而防止并发冲突或数据不一致。
- **日志复制**：日志复制指将主节点的操作日志同步到从节点。在这一过程中，所有节点必须确保日志条目的顺序一致，即日志条目必须以相同顺序写入。

# 2 日志与复制状态机
如果统计分布式系统有多少块基石，“日志”一定是其中之一。
这里“日志”并不是常见的通过 log4j 或 syslog 输出的文本。而是 MySQL 中的 binlog（Binary Log）、MongoDB 中的 Oplog（Operations Log）、Redis 中的 AOF（Append Only File）、PostgreSQL 中的 WAL（Write-Ahead Log）...。它们虽然名称不同，但共同特点是**只能追加、完全有序的记录序列**。

有序的日志记录了“何时发生了什么”，这一点可以通过以下两种数据复制模型来理解。
- **主备模型**（Primary-backup）：又称“状态转移”模型，主节点（Master）负责执行如“+1”、“-2”的操作，将操作结果（如“1”、“3”、“6”）记录到日志中，备节点（Slave）根据日志直接同步结果。
- **复制状态机模型**（State-Machine Replication）：又称“操作转移”模型，日志记录的不是最终结果，而是具体的操作指令，如“+1”、“-2”。指令按照顺序被依次复制到各个节点（Peer）。如果每个节点按顺序执行这些指令，各个节点最终将达到一致的状态。
无论哪一种模型，它们都揭示了：“**顺序是节点之间保持一致性的关键因素**”。如果打乱了操作的顺序，就会得到不同的运算结果。

共识算法通过消息，将日志广播至所有节点，它们就日志什么位置，记录什么达成共识。换句话说，所有的节点中，都有着相同顺序的日志序列。
节点内的进程（图中的 State Machine）按顺序执行日志序列，操作具有全局顺序。因此，所有节点最终将达到一致的状态。多个这样的进程结合有序日志，就构成了 Apache Kafka、Zookeeper、etcd、CockroachDB 等分布式系统中的关键组件。

# 3 Paxos 算法详解
## 3.1 简介
Paxos 算法是 Leslie Lamport（[莱斯利·兰伯特](https://zh.wikipedia.org/wiki/%E8%8E%B1%E6%96%AF%E5%88%A9%C2%B7%E5%85%B0%E4%BC%AF%E7%89%B9)）在 **1990** 年提出了一种分布式系统 **共识** 算法。这也是第一个被证明完备的共识算法（前提是不存在拜占庭将军问题，也就是没有恶意节点）。
**共识算法的作用是让分布式系统中的多个节点之间对某个提案（Proposal）达成一致的看法**。提案的含义在分布式系统中十分宽泛，像哪一个节点是 Leader 节点、多个事件发生的顺序等等都可以是一个提案。
兰伯特当时提出的 Paxos 算法主要包含 2 个部分:
- **Basic Paxos 算法**：描述的是多节点之间如何就某个值(提案 Value)达成共识。
- **Multi-Paxos 思想**：描述的是执行多个 Basic Paxos 实例，就一系列值达成共识。Multi-Paxos 说白了就是执行多次 Basic Paxos ，核心还是 Basic Paxos 。

由于 Paxos 算法在国际上被公认的非常难以理解和实现，因此不断有人尝试简化这一算法。到了 2013 年才诞生了一个比 Paxos 算法更易理解和实现的共识算法—[Raft 算法](https://javaguide.cn/distributed-system/theorem&algorithm&protocol/raft-algorithm.html) 。更具体点来说，Raft 是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现。
针对没有恶意节点的情况，除了 Raft 算法之外，当前最常用的一些共识算法比如 **ZAB 协议**、 **Fast Paxos** 算法都是基于 Paxos 算法改进的。

针对存在恶意节点的情况，一般使用的是 **工作量证明（POW，Proof-of-Work）**、 **权益证明（PoS，Proof-of-Stake ）** 等共识算法。这类共识算法最典型的应用就是区块链，就比如说前段时间以太坊官方宣布其共识机制正在从工作量证明(PoW)转变为权益证明(PoS)。
区块链系统使用的共识算法需要解决的核心问题是 **拜占庭将军问题** ，这和我们日常接触到的 ZooKeeper、Etcd、Consul 等分布式中间件不太一样。

下面我们来对 Paxos 算法的定义做一个总结：
- Paxos 算法是兰伯特在 1990 年提出了一种分布式系统共识算法。
- 兰伯特当时提出的 Paxos 算法主要包含 2 个部分: Basic Paxos 算法和 Multi-Paxos 思想。
- Raft 算法、ZAB 协议、 Fast Paxos 算法都是基于 Paxos 算法改进而来。

## 3.2 Basic Paxos 算法
### 3.2.1 问题和背景
假如我们设计一个由三个节点 A、B、C 组成分布式集群，提供只读的 KV 存储服务。所有的节点必须要先对只读变量的值（提案）达成共识，然后所有的节点再一起创建这个只读变量。
当有多个客户端访问这个系统，试图创建同一个只读变量时，集群中所有的节点该如何达成共识，实现各个节点中的 x 值的一致呢？
![[Pasted image 20250410223011.png|图3.2.2.1 x值一致性|500]]

实现多个节点 x 值一致的复杂度主要来源于以下两个因素的共同影响：
1. 系统内部各个节点的**通信是不可靠的**，总会发生诸如**机器宕机**或**网络异常**（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。
2. 客户端**写入是并发的**，如果是串行的修改数据，仅单纯使用少数服从多数原则，就足以保证数据被正确读写。而并发访问就变成了“**分布式环境下多个节点并发操作共享数据**”的问题。

我们把上面的背景问题总结转化，其实就是下面 2 个核心需求：
1. **安全性** Safety：
	- 一个变量只会被确定一个值；
	- 一个变量只有值被确定之后，才能被学习。
2. **活性** Liveness：
	- 提案最终会被接受；
	- 一个提案被接受之后，最终会被所有的 Learner 学习到；
	- 必须在有限时间内做出决议（不能有太多轮投票）。

### 3.2.2 基础概念
Basic Paxos 中存在 3 个重要的角色：
- **提议者（Proposer）**：也可以叫做协调者（coordinator），提议者负责接受客户端的请求并发起提案。提案信息通常包括提案编号 (Proposal ID) 和提议的值 (Value)。
- **接受者（Acceptor）**：也可以叫做投票员（voter），负责对提议者的提案进行投票，同时需要记住自己的投票历史；
- **学习者（Learner）**：如果有超过半数接受者就某个提议达成了共识，那么学习者就需要接受这个提议，并就该提议作出运算，然后将运算结果返回给客户端。

为了减少实现该算法所需的节点数，一个节点可以身兼多个角色。并且，一个提案被选定需要被半数以上的 Acceptor 接受。这样的话，Basic Paxos 算法还具备容错性，在少于一半的节点出现故障时，集群仍能正常工作。

### 3.2.3 解决思路推导
Basic Paxos 问题背景相信已经讲清楚了，那怎么解决？
#### 3.2.3.1 只有一个Acceptor
简单的方案如同图 3.2.3.1 所示，多个提议节点、单个决策节点，决策节点接受第一个发给它的值，作为被选中的值。但如果决策节点故障，整个系统就会不可用。
![[Pasted image 20250410223805.png|图3.2.3.1]]

#### 3.2.3.2 多个Acceptor
##### 3.2.3.2.1 Acceptor约束
为了克服单点故障问题，借鉴多数派的机制，思路是写入一半以上的节点，如果集群中有 N 个节点，客户端需要写入 W >= N/2 + 1 个节点。使用多数派的机制后最多可容忍 (N-1)/2 个节点故障。
但是问题还是存在：**每个Acceptor节点该接受几个提案呢**？如果我们希望即使只有一个Proposer提出了一个value，该value也最终被选定。那么，就得到下面的约束：
==P1：一个Acceptor必须接受它收到的第一个提案。==

但是，这又会引出另一个问题：如果每个Proposer分别提出不同的value，发给不同的Acceptor。根据P1，Acceptor分别接受自己收到的value，就导致不同的value被选定。出现了不一致。如下图：
![[Pasted image 20250411092515.png|图 3.2.3.2]]
刚刚是因为『一个提案只要被一个Acceptor接受，则该提案的value就被选定了』才导致了出现上面不一致的问题。因此，我们需要加一个规定：
==规定：一个提案被选定需要被半数以上的Acceptor接受==
这个规定又暗示了：『一个Acceptor必须能够接受不止一个提案！』不然可能导致最终没有value被选定。比如上图的情况。v1、v2、v3都没有被选定，因为它们都只被一个Acceptor的接受。

重新设计提案，给每个提案加上一个提案编号，表示提案被提出的顺序。令『**提案=提案编号+value**』。虽然允许多个提案被选定，但必须保证所有被选定的提案都具有相同的value值。否则又会出现不一致。于是有了下面的约束：
==P2：如果某个value为v的提案被选定了，那么每个编号更高的被选定提案的value必须也是v。==
一个提案只有被Acceptor接受才可能被选定，因此我们可以把P2约束改写成对Acceptor接受的提案的约束P2a。
==P2a：如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v。==

但是，考虑如下的情况：假设总的有5个Acceptor。Proposer2提出[M1,V1]的提案，Acceptor2~5（半数以上）均接受了该提案，于是对于Acceptor2~5和Proposer2来讲，它们都认为V1被选定。
Acceptor1刚刚从宕机状态恢复过来（之前Acceptor1没有收到过任何提案），此时Proposer1向Acceptor1发送了[M2,V2]的提案（V2≠V1且M2>M1），对于Acceptor1来讲，这是它收到的第一个提案。根据P1（一个Acceptor必须接受它收到的第一个提案。）,Acceptor1必须接受该提案（Acceptor1 不知道V2有没有被半数以上节点选定）。
![[Pasted image 20250411094327.png|图 P2a]]

##### 3.2.3.2.2 Proposer约束
P2a是对Acceptor接受的提案约束，但其实提案是Proposer提出来的，所有我们可以对Proposer提出的提案进行约束。得到P2b：
==P2b：如果某个value为v的提案被选定了，那么之后任何Proposer提出的编号更高的提案的value必须也是v。==
如何确保在某个value为v的提案被选定后，Proposer提出的编号更高的提案的value都是v呢？只要满足P2c即可：
==P2c：对于任意的N和V，如果提案[N, V]被提出，那么存在一个半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个：==
- ==S中每个Acceptor都没有接受过编号小于N的提案。==
- ==S中Acceptor接受过的最大编号的提案的value为V。==

基于此，就需要一个**两阶段（2-phase）协议，对于已经选定的值，后面的提案要放弃自己的提议，提出已经被选中的值**。例如，图P2a 中的 Proposer1 发起提案之前，先广播给 Acceptor2-4 这 3 个节点，询问是否已经有接受的提案，如果已有，则撤销自己的提案，并且改成 [M2, V2]。
先广播的这个请求，又叫Prepare请求。

### 3.2.4 算法描述
对于 Acceptor，我们给它定义几个变量：
1. PrepareN：自己接收到的最大 Prepare 请求的编号
2. AcceptN：自己最终选定的编号
3. AcceptV：自己最终选定的值

Paxos算法分为**两个阶段**。具体如下：
1. 阶段一：准备阶段
	1. Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare(N)请求。
	2. Acceptor接受到Prepare请求后
		1. 如果N<=PrepareN，就不响应or响应error
		2. 否则N>PrepareN，令PrepareN=N，响应(Pok,AcceptN,AcceptV)或(Pok,null,null)
2. 阶段二：批准阶段
	1. 如果 Proposer 收到超过半数的 Pok，就发出Accept(N,V)请求。如果响应中有提案，则 V=响应中最大的AcceptN对应的AcceptV；否则 V=自己定的值
		1. 否则Pok数未过半，重新获取N（N递增，不重复），发起Prepare请求（回到第一阶段）
	2. Acceptor接受到 Accept 请求后
		1. 如果 N>=PrepareN，接受提案，令 AcceptN=N，AcceptV=V。回复Aok
		2. 否则 N<PrepareN，不接受。不响应or响应error
	3. 如果 Proposer 收到超过半数的 Aok，则确定 V 被选定，并广播给所有 learner；否则重新发起Prepare请求（回到第一阶段）

## 3.3 Multi Paxos 思想
Basic Paxos 算法的仅能就单个值达成共识，达成共识至少需要两次网络往返，高并发情况下还可能导致活锁：
![[image-7.png]]
因此Basic Paxos几乎只是用来做理论研究，并不直接应用在实际工程中。

实际应用中几乎都需要连续确定多个值，而且希望能有更高的效率。Multi-Paxos正是为解决此问题而提出。Multi-Paxos基于Basic Paxos做了两点改进：
1. 针对每一个要确定的值，运行一次Paxos算法实例（Instance），形成决议。每一个Paxos实例使用唯一的Instance ID标识。
2. 在所有Proposers中选举一个Leader，由Leader唯一地提交Proposal给Acceptors进行表决。这样没有Proposer竞争，解决了活锁问题。在系统中仅有一个Leader进行Value提交的情况下，Prepare阶段就可以跳过，从而将两阶段变为一阶段，提高效率。

可执行一次Basic Paxos实例来选举出一个Leader。在Leader宕机之后服务临时不可用，需要重新选举Leader继续服务。
Multi-Paxos允许有多个自认为是Leader的节点并发提交Proposal而不影响其安全性，这样的场景即退化为Basic Paxos。

由于兰伯特提到的 Multi-Paxos 思想缺少代码实现的必要细节(比如怎么选举领导者)，所以在理解和实现上比较困难。
Chubby和Boxwood均使用Multi-Paxos。ZooKeeper使用的Zab也是Multi-Paxos的变形。
Raft 算法就是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现，实际项目中可以优先考虑 Raft 算法。

# 4 Raft 算法详解
## 4.1 背景
> [!tip] 词义
> Raft 是 Re{liable|plicated|dundant} And Fault-Tolerant，即可靠、复制、冗余和容错，组合起来的单词。同时，Raft 在英文有“筏”的含义，隐喻一艘帮助你逃离 Paxos 小岛的救生筏。

Paxos 算法理解起来非常晦涩。此外，论文虽然提到了 Multi Paxos，但缺少实现细节。虽然所有的共识系统都是从 Paxos 算法开始的，但工程师们实现过程中有很多难以逾越的难题，往往不得已开发出与 Paxos 完全不一样的算法，这导致 Lamport 的证明并没有太大价值。所以，很长的一段时间内，实际上并没有一个被大众广泛认同的 Paxos 算法。

2013 年，斯坦福大学的学者 Diego Ongaro 和 John Ousterhout 发表了论文 《[In Search of an Understandable Consensus Algorithm](https://www.thebyte.com.cn/consensus/raft.html#footnote1)》，提出了 Raft 算法。Raft 论文开篇描述了 Raft 的证明和 Paxos 等价，详细阐述了算法如何实现。也就是说，Raft 天生就是 Paxos 算法的工程化。

不同于Paxos算法直接从分布式一致性问题出发推导出来，**Raft算法则是从多副本状态机的角度提出，用于管理多副本状态机的日志复制**。
Raft实现了和Paxos相同的功能，它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等。
同时，Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。

## 4.2 基础概念
### 4.2.1 节点类型
Raft 提出了领导者角色，通过选举机制“分享”提案权利：
- **领导者**（Leader）：负责处理所有客户端请求，将请求转换为“日志”复制到其他节点，不断地向所有节点广播心跳消息：“你们的领导还在，不要发起新的选举”。
- **跟随者**（Follower）：接收、处理领导者的消息，并向领导者反馈日志的写入情况。当领导者心跳超时时，他会主动站起来，推荐自己成为候选人。
- **候选人**（Candidate）：候选人属于过渡角色，他向所有的节点广播投票消息，如果他赢得多数选票，那么他将晋升为领导者。

在正常的情况下，只有一个服务器是 Leader，剩下的服务器是 Follower。Follower 是被动的，它们不会发送任何请求，只是响应来自 Leader 和 Candidate 的请求。

### 4.2.2 任期
联想到现实世界中的领导人都有一段不等的任期。自然，Raft 算法中也对应的概念 —— **“任期”（term）**。Raft 中的任期是一个递增的数字，贯穿于 Raft 的选举、日志复制和一致性维护过程中。
- **选举过程**：任期确保了领导者的唯一性。在一次任期内，只有获得多数选票的节点才能成为领导者。
- **日志一致性**：任期号会附加到每条日志条目中，帮助集群判断日志的最新程度。
- **冲突检测**：通过比较任期号，节点可以快速判断自己是否落后，并切换到跟随者状态。

每个节点都会存储当前的 term 号，当服务器之间进行通信时会交换当前的 term 号：
- 如果有服务器发现自己的 term 号比其他人小，那么他会更新到较大的 term 值。
- 如果一个 Candidate 或者 Leader 发现自己的 term 过期了，他会立即退回成 Follower。
- 如果一台服务器收到的请求的 term 号是过期的，那么它会拒绝此次请求。

### 4.2.3 日志
- `entry`：每一个事件成为 entry，只有 Leader 可以创建 entry。entry 的内容为`<term,index,cmd>`，其中 cmd 表示客户端请求的具体操作内容，也就是可以应用到状态机的操作。
- `log`：由 entry 构成的数组，每一个 entry 都有一个表明自己在 log 中的 index。只有 Leader 才可以改变其他节点的 log。entry 总是先被 Leader 添加到自己的 log 数组中，然后再发起共识请求，获得同意后才会被 Leader 提交给状态机。Follower 只能从 Leader 获取新日志和当前的 commitIndex，然后把对应的 entry 应用到自己的状态机中。

## 4.3 领导者选举
raft 使用心跳机制来触发 Leader 的选举。
如果一台服务器能够收到来自 Leader 或者 Candidate 的有效信息，那么它会一直保持为 Follower 状态，并且刷新自己的 electionElapsed，重新计时。
Leader 会向所有的 Follower 周期性发送心跳来保证自己的 Leader 地位。如果一个 Follower 在一个周期内没有收到心跳信息，就叫做**选举超时**，然后它就会认为此时没有可用的 Leader，并且**等待一段随机的时间后**发起一次Leader选举。

为了开始新的选举，Follower 会自增自己的 term 号并且转换状态为 Candidate 并给自己投一票。然后他会向所有节点发起 **RequestVote RPC** 请求，消息示例如下：
```json
{
  "term": 5, // 候选者的当前任期号，用于通知接收方当前选举属于哪个任期。
  "candidateId": 3, // 候选者的节点 ID，标识请求投票的节点。
  "lastLogIndex": 12, // 候选者日志的最后一条日志的索引，用于比较日志的完整性。
  "lastLogTerm": 4//候选者日志的最后一条日志的任期号，用于进一步比较日志的新旧程度。
}
```
其他节点收到投票消息后，根据下面的条件判断是否投票：
- Candidate 的日志至少与投票者的日志一样新（根据最后一条日志的任期号和索引号判断）。
- 当前节点尚未在本任期投票。
RequestVote 响应的示例如下：
```json
{
  "term": 5, //接收方的当前任期号，用于告知候选者最新的任期号。如果候选者发现该值比自己大，会转为跟随者。
  "voteGranted": true//是否投票给候选者，true 表示同意，false 表示拒绝。
}
```

Candidate 的状态会持续到以下情况发生：
- 赢得选举
- 其他节点赢得选举
- 一轮选举结束，无人胜出
赢得选举的条件是：一个 Candidate 在一个任期内收到了来自集群内的多数选票`（N/2+1）`，就可以成为 Leader。

在 Candidate 等待选票的时候，它可能收到其他节点声明自己是 Leader 的心跳，此时有两种情况：
- 该 Leader 的 term 号大于等于自己的 term 号，说明对方已经成为 Leader，则自己回退为 Follower。
- 该 Leader 的 term 号小于自己的 term 号，那么会拒绝该请求并让该节点更新 term。

下图概述了 Raft 集群 Leader 选举过程：
![[image-8.png]]

## 4.4 日志同步
Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 **AppendEntries RPC** 复制日志条目：
```json
{
  "term": 5, // 领导者的任期号
  "leaderId": "leader-123",
  "prevLogIndex": 8, // 前一日志条目的索引
  "prevLogTerm": 4, // 前一日志条目的任期
  "entries": [
    { "index": 9, "term": 5, "command": "set x=4" }, // 要复制的日志条目
  ],
  "leaderCommit": 7// Leader 的“已提交”状态的日志条目索引号，表示大多数
}
```
当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。
某些Followers可能没有成功的复制日志，Leader会无限的重试 AppendEntries RPC直到所有的Followers最终存储了所有的日志条目。

Raft日志同步保证如下两点：
- 如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。
- 如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的。

第一条特性源于Leader在一个term内在给定的一个log index最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变。
第二条特性源于 AppendEntries 的一个简单的一致性检查。当发送一个 AppendEntries RPC 时，Leader会把新日志条目紧接着之前的条目的log index和term都包含在里面。如果Follower没有在它的日志中找到log index和term都相同的日志，它就会拒绝新的日志条目。

---
# 5 引用
分布式共识：https://www.thebyte.com.cn/consensus/summary.html
[Paxos算法](https://www.cnblogs.com/linbingdong/p/6253479.html)