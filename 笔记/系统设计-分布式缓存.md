2025-12-02 20:23
Status: #idea
Tags:

# 1 分布式缓存设计
## 1.1 简介
面试官问我，设计一个高可用的分布式缓存系统，支持多机房部署和动态扩容。需要保证数据的一致性和高可用，并且要考虑到多机器的路由系统要怎么做。

先明确核心设计原则：**分片存储（支撑扩容）+ 多副本（支撑高可用）+ 跨机房部署（容灾）+ 客户端路由（高性能）**，整体架构：
1. 客户端路由层（槽位映射+就近访问）
2. 集群管理层（节点注册/心跳/槽位分配）
3. 数据层（分片存储+主从副本+跨机房同步）

## 1.2 核心模块详细设计
### 1.2.1 路由系统设计
路由系统的核心目标：**快速定位数据所在节点 + 支持动态扩容 + 容错**，分三部分设计：

#### 1.2.1.1 分片算法选型：虚拟槽分区（参考 Redis Cluster）
原理：将整个缓存空间划分为固定数量的 “槽位”（如 16384 个），每个槽位对应一段数据（通过哈希函数 `CRC16(key) % 槽位总数` 映射）；集群中每个节点负责一部分连续槽位，槽位与节点的映射关系是路由的核心。

#### 1.2.1.2 路由实现模式：客户端路由
- 架构：客户端直接缓存 “槽位→节点地址” 映射表，无需中间代理（如 Twemproxy/Codis），减少网络开销。
- 核心流程：
    1. 客户端初始化时，从集群配置中心（如 Consul）拉取最新的槽位 - 节点映射表，缓存到本地；
    2. 集群节点变动时，从集群配置中心监听自动更新本地缓存的映射表
    3. 重定向机制（兜底）：槽位从旧节点迁移到新节点，迁移期间槽位处于 “迁移中” 状态：
	    - 客户端请求该槽位时，旧节点会返回 “迁移中” 响应，并携带新节点地址；
	    - Go 客户端收到响应后，自动重试到新节点，同时更新本地映射表，保证路由不中断。

### 1.2.2 多机房部署设计（容灾 + 低延迟）
多机房部署的核心是 “**跨机房副本 + 就近访问 + 脑裂防护**”。
高可用的核心是 “**故障检测 + 自动切换**”，避免单点故障。

#### 1.2.2.1 副本分布策略
- 3 副本部署：每个槽位的主副本（写节点）和 2 个从副本（读节点）分布在不同机房，例如：
    - 机房 A：主副本（处理写请求）；
	    - 也可以在这个机房放从副本，分担读请求
    - 机房 B：从副本 1（同步主数据，处理机房 B 读请求 + 容灾备份）；
    - 机房 C：从副本 2（同步主数据，处理机房 C 读请求 + 容灾备份）；
- 原则：避免单机房故障导致整个分片不可用，同时保证每个机房有读节点，降低跨机房读延迟。

#### 1.2.2.2 数据同步策略
- 写流程：客户端→机房 A 主节点（写入成功）→异步同步到机房 B/C 从节点（追求性能）；
- 一致性增强：关键场景下启用 “半同步复制”（主节点等待至少 1 个从节点同步完成后，再向客户端返回成功），避免主节点宕机导致数据丢失；
- 跨机房网络优化：
	- 同步数据时采用批量压缩（如 gzip），减少网络带宽占用；
	- 监控跨机房网络延迟，若延迟过高，自动切换同步模式为 “定时批量同步”。

#### 1.2.2.3 高可用（故障自动恢复）
- 问题：多机房网络分区时，不同机房的节点可能各自选举主节点，导致数据不一致；
- 解决方案：基于 Raft 协议的 “Quorum 机制”（选举和写操作需超过半数副本同意）：
    - 3 副本场景下，需至少 2 个副本在线且同意，才能选举主节点或执行写操作；
    - 若机房 A 与 B/C 断开网络，机房 A 的主节点因无法获得半数副本同意，自动降级为从节点；机房 B/C 的从节点选举新主，保证集群一致性。
- 选主和Consul:
	- 注意，Consul可以简化 Raft 选主逻辑，但不能完全替代。因为：
		- 无法保证「选出来的主节点数据最新」：Consul 只能告诉我们「哪些节点是健康的」，但不知道「健康节点的数据同步进度」。
		- 没有「Quorum 机制」，无法防脑裂：Consul 的服务信息同步是「最终一致」的，机房 A 的节点可能仍认为自己是健康的，机房 B/C 的节点也认为自己是健康的
	- Consul的功能：
		- 快速筛选「候选主节点」（替代 Raft 的部分成员发现逻辑）：主节点故障后，Raft 无需自己发送心跳检测节点状态，直接通过 Consul API 查询「该槽位的健康副本节点」
		- 存储「Raft 集群配置」：Raft 集群需要维护「成员列表（哪些节点是副本）、主节点标识、日志元数据（最后一条日志索引）」等信息，这些可直接存在 Consul KV 中
		- 同步「选举结果」（给客户端和集群节点）
		- 辅助「跨机房选举」（利用 Consul 多机房特性）：Consul 原生支持跨机房集群，可帮 Raft 解决「跨机房副本发现」问题
- 选主实现流程：无需从零实现完整 Raft 算法（复杂且易出错），推荐基于成熟的 Raft 库（如 `etcd-io/raft`、`hashicorp/raft`），结合 Consul 做「辅助增强」，核心流程如下：
	1. 初始化：Raft 集群与 Consul 绑定
		- 3 个副本节点启动时，通过 Consul 注册服务（携带槽位 ID、副本角色、Raft 成员信息）；
		- 副本节点从 Consul KV 中读取 Raft 配置，初始化 Raft 成员列表（无需手动配置）。
	2. 主节点故障检测
		- Consul 原生健康检查（TCP/HTTP）发现主节点故障，标记为「不健康」；
		- 副本节点通过 Watch Consul 服务变更，感知到主节点故障，触发 Raft 选举。
	3. 主从选举流程（Consul + Raft 协作）
		1. 副本节点收到通知，从 Consul 获取健康副本列表
		2. 副本节点启动 Raft 选举，基于 Raft 协议做 3 件事：1. 筛选数据最新的候选者；2. 发起投票，满足 Quorum（≥2 票）；3. 选出新主
		3. 新主节点更新 Consul KV（leader 字段）和自身 Service Metadata（role=master）
		4. 所有副本节点 + 客户端 Watch Consul 变更，同步新主信息
		5. 客户端路由更新，写请求路由到新主；副本节点同步新主数据

### 1.2.3 动态扩容设计（无感知 + 平滑迁移）
动态扩容的核心是 “**槽位迁移而非数据重哈希**”，流程如下：

#### 1.2.3.1 扩容触发条件
- 自动触发：监控节点内存使用率（如超过 70%）、CPU 使用率（如超过 80%）；
- 手动触发：通过集群管理 API（Go 实现 HTTP 接口）手动添加节点。

#### 1.2.3.2 扩容核心流程（无感知）
1. **新增节点**：新节点加入集群，向 Consul 注册节点信息（IP、端口、机房、负载等）；
2. **槽位分配**：集群管理模块（基于 Consul 的分布式锁避免并发冲突）从现有节点中拆分部分槽位（如从每个旧节点拆分 1/4 槽位），分配给新节点；
3. **数据迁移**：
    - 源节点将槽位数据批量迁移到目标节点（新节点），每个槽位分批次（如每批 1000 条）迁移，避免阻塞；
    - 源节点（旧节点）将该槽位的请求通过重定向机制转发到新节点；
    - 目标节点接收数据并写入，完成后向源节点发送 “迁移完成” 确认；
4. **路由更新**：集群管理模块将新的槽位 - 节点映射同步到 Consul，客户端通过监听从而自动更新本地缓存的映射表。

#### 1.2.3.3 缩容流程
与扩容反向：将待下线节点的槽位迁移到其他节点，迁移完成后，该节点从 Etcd 注销，客户端自动停止路由请求，实现无感知下线。

### 1.2.4 数据一致性保障
分布式缓存的一致性需 “**结合业务场景权衡**”，优先保证 “最终一致性”，关键场景增强为 “强一致性”：

##### 1.2.4.1.1 写一致性
- 写请求仅路由到主节点，主节点写入成功后，异步同步到从节点（默认最终一致性）；
- 强一致性需求（如支付缓存）：启用半同步复制，主节点等待至少 1 个从节点同步完成（复制偏移量一致），再返回客户端成功，牺牲部分性能换取一致性。

##### 1.2.4.1.2 读一致性
- 普通读：路由到同机房从节点（最终一致性，低延迟）；
- 强一致读：客户端携带 “一致性标记”，路由到主节点（如用户刚更新数据后，需立即读取最新值）；
- 避免脏读：从节点同步时记录 “复制偏移量”，客户端读从节点时，若从节点偏移量低于主节点最新偏移量，自动重试或路由到主节点。

##### 1.2.4.1.3 故障恢复一致性
- 主节点故障时，通过 Raft 协议选举 “复制偏移量最大” 的从节点作为新主，保证新主数据最新；
- 故障节点恢复后，自动作为从节点同步新主的数据，同步完成后再加入集群提供服务，避免数据不一致。

## 1.3 拓展设计
### 1.3.1 跨机房写延迟处理
例如有的槽在A机房，然后有些客户端离A机房比较远，而离B机房比较近，这些客户端的读请求可以从B机房的从节点读，但是写请求需要路由到A机房的主节点，距离比较远。
可以使用分层解决方案（从低成本到高收益，优先落地无架构改动的方案）。

#### 1.3.1.1 第一层：低成本优化（无架构改动，仅优化传输 / 客户端逻辑）
核心思路：不改变主节点机房，仅优化跨机房写的 “传输效率”，快速降低延迟 10%-30%。

##### 1.3.1.1.1 写请求批量处理（客户端侧优化）
跨机房网络延迟的 “往返开销” 是固定的，通过批量合并写请求，摊薄单次请求的延迟成本。
适用于非实时写场景（如日志上报、统计数据更新），可接受 5-10ms 的写延迟叠加。

##### 1.3.1.1.2 跨机房传输优化（网络层优化）
- 数据压缩：写请求的 Value 较大时（如 JSON 字符串），用 gzip/Brotli 压缩后传输（Go 用 `compress/gzip` 库），减少网络传输字节数，降低延迟；
- 连接复用：Go 客户端使用长连接池（如 `redis/go-redis` 的 `PoolSize` 配置），避免每次写请求都建立 TCP 连接（TCP 三次握手耗时≈1-3ms）；
- 协议优化：使用 HTTP/2 或 QUIC 协议（替代 TCP），减少头部开销和重传延迟（Consul 原生支持 HTTP/2，可复用 Consul 的连接池）；
- 路由优化：通过 BGP Anycast 或专线连接跨机房，规避公网路由波动导致的延迟飙升。

#### 1.3.1.2 第二层：架构级优化（核心解决 “写就近” 问题，收益最大）
核心思路：打破 “虚拟槽绑定固定主节点机房” 的限制，让主节点机房 “跟着客户端走”，从根源上减少跨机房写。

##### 1.3.1.2.1 节点机房亲和性（动态迁移主节点）
核心逻辑：**根据客户端的地理分布，动态调整虚拟槽的主节点机房**—— 将高频写客户端所在机房的从节点，升级为该槽位的主节点，让写请求无需跨机房。
- 实现步骤（结合 Consul+Raft）：
    1. **客户端机房标识**：客户端注册到 Consul 时，携带自身机房信息（如 `Meta: {"dc": "dc-2"}`）；
    2. **写热度统计**：Consul 通过服务监控，统计每个虚拟槽的写请求来源机房分布（如槽位 123 的 80% 写请求来自 dc-2）；
    3. **主节点迁移触发**：当某机房的写请求占比超过阈值（如 60%），且当前主节点不在该机房时，触发主节点迁移；
    4. **Raft 安全迁移**：
        - 集群管理模块通过 Consul 获取该槽位在目标机房（如 dc-2）的从节点（数据已同步完成）；
        - 发起 Raft ConfChange 请求，将目标机房的从节点升级为主节点（Raft 保证数据一致性，迁移期间写请求不中断）；
        - 迁移完成后，更新 Consul 的槽位 - 主节点映射，客户端自动路由到新主节点（dc-2），写请求就近访问。
- 关键保障：
    - 迁移期间：Raft 协议保证写请求不丢失（旧主节点将写请求转发给新主，或客户端重试到新主）；
    - 防抖动：设置迁移冷却时间（如 30 分钟），避免因短期流量波动导致主节点频繁迁移。

##### 1.3.1.2.2 单元化部署（虚拟槽按机房分片）
核心逻辑：**将虚拟槽按机房拆分，每个机房负责一部分槽位的 “本地主节点”**，客户端写请求优先路由到本地机房的主节点。
- 实现方案：
    1. **槽位机房分区**：将 16384 个虚拟槽按机房数量拆分（如 3 个机房，每个机房负责≈5461 个槽位），每个槽位的主节点固定在对应机房，从节点分布在其他机房；
    2. **客户端路由优化**：客户端通过 Consul 获取本地机房负责的槽位范围，写请求优先路由到本地机房的主节点；若需访问其他机房的槽位（跨机房写），再路由到目标机房主节点；
    3. **业务适配**：结合业务数据的 “地理属性”（如用户 ID 按地区哈希），让用户数据的槽位刚好落在其所在地区的机房，实现 “本地写、本地读”。
- 优势：彻底避免大部分跨机房写，延迟最低；
- 限制：依赖业务数据的地理分布特性，若数据无明显地理属性（如全局配置），仍会存在跨机房写。

##### 1.3.1.2.3 多活主节点（真多活架构，最高复杂度）
核心逻辑：**允许同一虚拟槽在多个机房拥有 “主节点”**，客户端写请求可路由到本地主节点，通过 Raft 协议的 “跨机房 Quorum” 保证数据一致性（适用于核心业务，对延迟和一致性要求都极高）。
- 实现方案（基于 Multi-Raft）：
    1. 每个槽位的 Raft 集群包含多个机房的节点（如 dc-1、dc-2、dc-3 各 1 个节点，共 3 个副本）；
    2. 客户端写请求路由到本地机房的节点，该节点作为 Raft 的 “提议者” 发起写请求；
    3. Raft 协议要求 “超过半数副本同意” 才能提交写请求（如 3 副本需 2 个同意），跨机房节点通过专线同步日志；
    4. 写成功后，本地主节点立即返回客户端，其他机房的主节点通过 Raft 同步数据。
- 优势：写请求完全就近，延迟最低（仅本地机房网络延迟）；
- 挑战：
    - 跨机房 Raft 同步需专线保障低延迟（否则 Quorum 等待时间过长）；
    - 实现复杂，需基于 `etcd-io/raft` 二次开发跨机房日志同步优化；
    - 集群资源开销高（每个槽位需多机房副本）。

## 1.4 TODO
- 负载均衡
- 数据倾斜

## 1.5 总结
- 分布式缓存需要满足高性能、高可用、一致性的要求，需要这些核心技术：分片集群(高性能)、主从同步(高可用)、客户端路由(高性能)、多机房部署(容灾)、动态扩容(高可用)、一致性权衡、负载均衡
- 核心技术分析：
	- 路由系统设计：
		- 分片算法：类似于Redis Cluster，使用虚拟槽分片算法
		- 客户端路由：
			- 初始化和虚拟槽变动时，都从Consul获取虚拟槽和节点的映射信息
			- 重定向机制(兜底)
	- 多机房部署：
		- 副本分布策略：每个槽位的主从节点分布在不同机房，其他机房的从节点用作容灾
		- 数据同步策略：
			- 最终一致：主节点异步同步数据到从节点
			- 一致性增强：关键场景下启用 “半同步复制”（主节点等待至少 1 个从节点同步完成后，再向客户端返回成功）
		- 主从选举：
			- 当Consul监控主节点心跳发现超时，触发主从选举
			- 从节点通过Consul获取健康节点列表，使用raft算法选举，数据同步进度最新和得票>=Quorum的成为新主
			- 最后通过Consul更新从节点和客户端路由信息
	- 动态扩容设计
		- 扩容可以自动触发(系统负载过高)、手动触发(手动加节点)
		- 扩容流程：注册新节点，虚拟槽划分，数据迁移，路由更新
		- 缩容流程：类似扩容
	- 一致性权衡
		- 写：普通写异步同步，特殊情况半同步复制
		- 读：普通读读从节点，特殊情况读主节点

---
# 2 引用