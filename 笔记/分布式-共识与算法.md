2025-04-12 15:27
Status: #idea
Tags: [[分布式]] [[分布式-基础理论]]


# 1 什么是共识
分布式系统中充满了各种潜在的错误场景，**网络数据包可能丢失、顺序紊乱、重复发送或者延迟，节点还可能宕机**。“在充满不确定性的环境中，就某个决策达成共识”是软件工程领域最具挑战性的问题之一。

在汉语中，“共识”和“一致”意思相似，但在计算机领域，它们具有截然不同的含义。
- **共识**（Consensus）：指所有节点就某项操作（如选主、原子事务提交、日志复制、分布式锁管理等）达成一致的实现过程。
- **一致性**（Consistency）：描述多个节点的数据是否保持一致，关注数据最终达到稳定状态的结果。

在分布式系统中，节点故障是不可避免的，但部分节点故障不应该影响系统整体状态。通过增加节点数量，依据“少数服从多数”原则，只要多数节点（至少 N/2+1N）达成一致，其状态即可代表整个系统。这种**依赖多数节点实现容错的机制称为 Quorum 机制**。
> [!note] Quorum 机制
> - 3 节点集群：Quorum 为 2，允许 1 个节点故障。
> - 4 节点集群：Quorum 为 ⌈4/2⌉+1=3⌈4/2⌉+1=3，允许 1 个节点故障。
> - 5 节点集群：Quorum 为 ⌈5/2⌉+1=3⌈5/2⌉+1=3，允许 2 个节点故障。
> 
> 你注意到了吗？3 节点和 4 节点集群的故障容忍能力一样。因此，通常情况下，针对容错的分布式系统无需使用 4 个节点。

基于 Quorum 的机制，通过“少数服从多数”协商机制达成一致的决策，从而对外表现为一致的运行结果。这一过程被称为节点间的“协商共识”。一旦解决共识问题，便可提供一套屏蔽内部复杂性的抽象机制，为应用层提供一致性保证，满足多种需求。
- **主节点选举**：在主从复制数据库中，所有节点需要就“谁来当主节点”达成一致。如果由于网络问题导致节点间无法通信，很容易引发争议。若争议未解决，可能会出现多个节点同时认为自己是主节点的情况，这就是分布式系统中最棘手的问题之一 —— “脑裂”。
- **原子事务提交**：对于支持跨节点或跨分区事务的数据库，可能会发生部分节点事务成功、部分节点事务失败的情况。为维护事务的原子性（即 ACID 特性），所有节点必须就事务的最终结果达成一致。
- **分布式锁管理**：当多个请求尝试访问共享资源时，共识机制可确保所有节点一致认定“谁成功获取了锁”。即使发生网络故障或节点异常，也能避免锁争议，从而防止并发冲突或数据不一致。
- **日志复制**：日志复制指将主节点的操作日志同步到从节点。在这一过程中，所有节点必须确保日志条目的顺序一致，即日志条目必须以相同顺序写入。


> [!note] 总结
> 分布式系统的共识，就是 Quorum 节点（大多数节点，保证了容错性）就某项操作达成一致的实现过程。例如选主（谁当上了主）、分布式锁管理（谁拿到了锁）。
> 
> 分布式系统中充满了各种潜在的错误场景：消息延迟、丢失、乱序、重复，节点宕机，网络分区等。因此达成共识是一件很有挑战性的事情。

# 2 日志与复制状态机
如果统计分布式系统有多少块基石，“日志”一定是其中之一。
这里“日志”并不是常见的通过 log4j 或 syslog 输出的文本。而是 MySQL 中的 binlog（Binary Log）、MongoDB 中的 Oplog（Operations Log）、Redis 中的 AOF（Append Only File）、PostgreSQL 中的 WAL（Write-Ahead Log）...。它们虽然名称不同，但共同特点是**只能追加、完全有序的记录序列**。

有序的日志记录了“何时发生了什么”，这一点可以通过以下两种数据复制模型来理解。
- **主备模型**（Primary-backup）：又称“状态转移”模型，主节点（Master）负责执行如“+1”、“-2”的操作，将操作结果（如“1”、“3”、“6”）记录到日志中，备节点（Slave）根据日志直接同步结果。
- **复制状态机模型**（State-Machine Replication）：又称“操作转移”模型，日志记录的不是最终结果，而是具体的操作指令，如“+1”、“-2”。指令按照顺序被依次复制到各个节点（Peer）。如果每个节点按顺序执行这些指令，各个节点最终将达到一致的状态。
无论哪一种模型，它们都揭示了：“**顺序是节点之间保持一致性的关键因素**”。如果打乱了操作的顺序，就会得到不同的运算结果。

**共识算法通过消息，将日志广播至所有节点，它们就日志什么位置，记录什么达成共识。换句话说，所有的节点中，都有着相同顺序的日志序列**。
节点内的进程（图中的 State Machine）按顺序执行日志序列，操作具有全局顺序。因此，所有节点最终将达到一致的状态。多个这样的进程结合有序日志，就构成了 Apache Kafka、Zookeeper、etcd、CockroachDB 等分布式系统中的关键组件。


> [!note] 总结
> 日志是只能追加、完全有序的记录序列（例如 MySQL binlog）。
> 
> 共识算法通过消息将日志广播至所有节点，所有的节点都有着相同顺序的日志序列。
> 
> 节点按顺序执行日志序列，所有节点最终将达到一致的状态

# 3 Paxos 算法详解
## 3.1 简介
Paxos 算法是 Leslie Lamport（[莱斯利·兰伯特](https://zh.wikipedia.org/wiki/%E8%8E%B1%E6%96%AF%E5%88%A9%C2%B7%E5%85%B0%E4%BC%AF%E7%89%B9)）在 **1990** 年提出了一种分布式系统 **共识** 算法。这也是第一个被证明完备的共识算法（前提是不存在拜占庭将军问题，也就是没有恶意节点）。
**共识算法的作用是让分布式系统中的多个节点之间对某个提案（Proposal）达成一致的看法**。提案的含义在分布式系统中十分宽泛，像哪一个节点是 Leader 节点、多个事件发生的顺序等等都可以是一个提案。
兰伯特当时提出的 Paxos 算法主要包含 2 个部分:
- **Basic Paxos 算法**：描述的是多节点之间如何就某个值(提案 Value)达成共识。
- **Multi-Paxos 思想**：描述的是执行多个 Basic Paxos 实例，就一系列值达成共识。Multi-Paxos 说白了就是执行多次 Basic Paxos ，核心还是 Basic Paxos 。

由于 Paxos 算法在国际上被公认的非常难以理解和实现，因此不断有人尝试简化这一算法。到了 2013 年才诞生了一个比 Paxos 算法更易理解和实现的共识算法—[Raft 算法](https://javaguide.cn/distributed-system/theorem&algorithm&protocol/raft-algorithm.html) 。更具体点来说，Raft 是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现。
针对没有恶意节点的情况，除了 Raft 算法之外，当前最常用的一些共识算法比如 **ZAB 协议**、 **Fast Paxos** 算法都是基于 Paxos 算法改进的。

针对存在恶意节点的情况，一般使用的是 **工作量证明（POW，Proof-of-Work）**、 **权益证明（PoS，Proof-of-Stake ）** 等共识算法。这类共识算法最典型的应用就是区块链，就比如说前段时间以太坊官方宣布其共识机制正在从工作量证明(PoW)转变为权益证明(PoS)。
区块链系统使用的共识算法需要解决的核心问题是 **拜占庭将军问题** ，这和我们日常接触到的 ZooKeeper、Etcd、Consul 等分布式中间件不太一样。

下面我们来对 Paxos 算法的定义做一个总结：
- Paxos 算法是兰伯特在 1990 年提出了一种分布式系统共识算法。
- 兰伯特当时提出的 Paxos 算法主要包含 2 个部分: Basic Paxos 算法和 Multi-Paxos 思想。
- Raft 算法、ZAB 协议、 Fast Paxos 算法都是基于 Paxos 算法改进而来。

## 3.2 Basic Paxos 算法
### 3.2.1 问题和背景
假如我们设计一个由三个节点 A、B、C 组成分布式集群，提供只读的 KV 存储服务。所有的节点必须要先对只读变量的值（提案）达成共识，然后所有的节点再一起创建这个只读变量。
当有多个客户端访问这个系统，试图创建同一个只读变量时，集群中所有的节点该如何达成共识，实现各个节点中的 x 值的一致呢？
![[Pasted image 20250410223011.png|图3.2.2.1 x值一致性|500]]

实现多个节点 x 值一致的复杂度主要来源于以下两个因素的共同影响：
1. 系统内部各个节点的**通信是不可靠的**，总会发生诸如**机器宕机**或**网络异常**（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。
2. 客户端**写入是并发的**，如果是串行的修改数据，仅单纯使用少数服从多数原则，就足以保证数据被正确读写。而并发访问就变成了“**分布式环境下多个节点并发操作共享数据**”的问题。

我们把上面的背景问题总结转化，其实就是下面 2 个核心需求：
1. **安全性** Safety：
	- 一个变量只会被确定一个值；
	- 一个变量只有值被确定之后，才能被学习。
2. **活性** Liveness：
	- 提案最终会被接受；
	- 一个提案被接受之后，最终会被所有的 Learner 学习到；
	- 必须在有限时间内做出决议（不能有太多轮投票）。

### 3.2.2 基础概念
Basic Paxos 中存在 3 个重要的角色：
- **提议者（Proposer）**：也可以叫做协调者（coordinator），提议者负责接受客户端的请求并发起提案。提案信息通常包括提案编号 (Proposal ID) 和提议的值 (Value)。
- **接受者（Acceptor）**：也可以叫做投票员（voter），负责对提议者的提案进行投票，同时需要记住自己的投票历史；
- **学习者（Learner）**：如果有超过半数接受者就某个提议达成了共识，那么学习者就需要接受这个提议，并就该提议作出运算，然后将运算结果返回给客户端。

为了减少实现该算法所需的节点数，一个节点可以身兼多个角色。并且，一个提案被选定需要被半数以上的 Acceptor 接受。这样的话，Basic Paxos 算法还具备容错性，在少于一半的节点出现故障时，集群仍能正常工作。

### 3.2.3 解决思路推导
Basic Paxos 问题背景相信已经讲清楚了，那怎么解决？
#### 3.2.3.1 只有一个Acceptor
简单的方案如同图 3.2.3.1 所示，多个提议节点、单个决策节点，决策节点接受第一个发给它的值，作为被选中的值。但如果决策节点故障，整个系统就会不可用。
![[Pasted image 20250410223805.png|图3.2.3.1]]

#### 3.2.3.2 多个Acceptor
##### 3.2.3.2.1 Acceptor约束
为了克服单点故障问题，借鉴多数派的机制，思路是写入一半以上的节点，如果集群中有 N 个节点，客户端需要写入 W >= N/2 + 1 个节点。使用多数派的机制后最多可容忍 (N-1)/2 个节点故障。
但是问题还是存在：**每个Acceptor节点该接受几个提案呢**？如果我们希望即使只有一个Proposer提出了一个value，该value也最终被选定。那么，就得到下面的约束：
==P1：一个Acceptor必须接受它收到的第一个提案。==

但是，这又会引出另一个问题：如果每个Proposer分别提出不同的value，发给不同的Acceptor。根据P1，Acceptor分别接受自己收到的value，就导致不同的value被选定。出现了不一致。如下图：
![[Pasted image 20250411092515.png|图 3.2.3.2]]
刚刚是因为『一个提案只要被一个Acceptor接受，则该提案的value就被选定了』才导致了出现上面不一致的问题。因此，我们需要加一个规定：
==规定：一个提案被选定需要被半数以上的Acceptor接受==
这个规定又暗示了：『一个Acceptor必须能够接受不止一个提案！』不然可能导致最终没有value被选定。比如上图的情况。v1、v2、v3都没有被选定，因为它们都只被一个Acceptor的接受。

还要考虑到，如果两个节点同时发起提案，就会导致提案冲突。如下图，S1 向 S1、S2、S3 发起提案（red）。同时，S5 也向 S3、S4、S5 发起提案（blue）。它们的提案 Quorum 都达成了，也就是说一个提案有两个值被批准，这显然破坏了一致性原则。
![[image-12.png]]
你会发现提案冲突发生在 S3，S3 是两个 Quorum 的交集点，它的时间线上有两个不同的值被批准。
我们知道，设计程序的一个基本常识是，如果多个线程同时操作某个共享变量，一定要加上互斥锁，不然会出现各种意外情况。不难发现，**S3 问题的本质是“在分布式环境下并发操作共享变量的问题”**。
由于分布式环境中随时可能发生通信故障，我们不能粗暴“套用”进程加锁机制来解决 S3 的问题。我们可以借鉴“乐观锁”的思路，加上“版本号”。首先，S1 发起提案，S3 收到 S1 提案时，应该意识到 S5 发起的提案（blue）的 Quorum 已经达成，S1 提案（red）已经失效。根据先到先得原则，S1 应该更新自己的提案值（red 替换为 blue），这个操作相当于对提案编号（乐观锁中的 version）“锁定”，防止之后出现多个冲突的提案编号。

重新设计提案，给每个提案加上一个提案编号，表示提案被提出的顺序。令『**提案=提案编号+value**』。虽然允许多个提案被选定，但必须保证所有被选定的提案都具有相同的value值。否则又会出现不一致。于是有了下面的约束：
==P2：如果某个value为v的提案被选定了，那么每个编号更高的被选定提案的value必须也是v。==
一个提案只有被Acceptor接受才可能被选定，因此我们可以把P2约束改写成对Acceptor接受的提案的约束P2a。
==P2a：如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v。==

但是，考虑如下的情况：假设总的有5个Acceptor。Proposer2提出[M1,V1]的提案，Acceptor2~5（半数以上）均接受了该提案，于是对于Acceptor2~5和Proposer2来讲，它们都认为V1被选定。
Acceptor1刚刚从宕机状态恢复过来（之前Acceptor1没有收到过任何提案），此时Proposer1向Acceptor1发送了[M2,V2]的提案（V2≠V1且M2>M1），对于Acceptor1来讲，这是它收到的第一个提案。根据P1（一个Acceptor必须接受它收到的第一个提案。）,Acceptor1必须接受该提案（Acceptor1 不知道V2有没有被半数以上节点选定）。
![[Pasted image 20250411094327.png|图 P2a]]

##### 3.2.3.2.2 Proposer约束
P2a是对Acceptor接受的提案约束，但其实提案是Proposer提出来的，所有我们可以对Proposer提出的提案进行约束。得到P2b：
==P2b：如果某个value为v的提案被选定了，那么之后任何Proposer提出的编号更高的提案的value必须也是v。==
如何确保在某个value为v的提案被选定后，Proposer提出的编号更高的提案的value都是v呢？只要满足P2c即可：
==P2c：对于任意的N和V，如果提案[N, V]被提出，那么存在一个半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个：==
- ==S中每个Acceptor都没有接受过编号小于N的提案。==
- ==S中Acceptor接受过的最大编号的提案的value为V。==

基于此，就需要一个**两阶段（2-phase）协议，对于已经选定的值，后面的提案要放弃自己的提议，提出已经被选中的值**。例如，图P2a 中的 Proposer1 发起提案之前，先广播给 Acceptor2-4 这 3 个节点，询问是否已经有接受的提案，如果已有，则撤销自己的提案，并且改成 [M2, V2]。
先广播的这个请求，又叫Prepare请求。

### 3.2.4 算法描述
对于 Acceptor，我们给它定义几个变量：
1. PrepareN：自己收到的最大 Prepare 请求的编号
2. AcceptN：自己最终选定的编号
3. AcceptV：自己最终选定的值

Paxos算法分为**两个阶段**。具体如下：
1. 阶段一：准备阶段
	1. Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare(N)请求。
	2. Acceptor接受到Prepare请求后
		1. 如果N<=PrepareN，就不响应or响应error
		2. 否则N>PrepareN，令PrepareN=N，响应(Pok,AcceptN,AcceptV)或(Pok,null,null)
2. 阶段二：批准阶段
	1. 如果 Proposer 收到超过半数的 Pok，就发出Accept(N,V)请求。如果响应中有提案，则 V=响应中最大的AcceptN对应的AcceptV；否则 V=自己定的值
		1. 否则Pok数未过半，重新获取N（N递增，不重复），发起Prepare请求（回到第一阶段）
	2. Acceptor接受到 Accept 请求后
		1. 如果 N>=PrepareN，接受提案，令 AcceptN=N，AcceptV=V。回复Aok
		2. 否则 N<PrepareN，不接受。不响应or响应error
	3. 如果 Proposer 收到超过半数的 Aok，则确定 V 被选定，并广播给所有 learner；否则重新发起Prepare请求（回到第一阶段）

## 3.3 Multi Paxos 思想
Basic Paxos 算法的仅能就单个值达成共识，达成共识至少需要两次网络往返，高并发情况下还可能导致活锁：
![[image-7.png]]
因此Basic Paxos几乎只是用来做理论研究，并不直接应用在实际工程中。

实际应用中几乎都需要连续确定多个值，而且希望能有更高的效率。Multi-Paxos正是为解决此问题而提出。Multi-Paxos基于Basic Paxos做了两点改进：
1. 针对每一个要确定的值，运行一次Paxos算法实例（Instance），形成决议。每一个Paxos实例使用唯一的Instance ID标识。
2. 在所有Proposers中选举一个Leader，由Leader唯一地提交Proposal给Acceptors进行表决。这样没有Proposer竞争，解决了活锁问题。在系统中仅有一个Leader进行Value提交的情况下，Prepare阶段就可以跳过，从而将两阶段变为一阶段，提高效率。

可执行一次Basic Paxos实例来选举出一个Leader。在Leader宕机之后服务临时不可用，需要重新选举Leader继续服务。
Multi-Paxos允许有多个自认为是Leader的节点并发提交Proposal而不影响其安全性，这样的场景即退化为Basic Paxos。

由于兰伯特提到的 Multi-Paxos 思想缺少代码实现的必要细节(比如怎么选举领导者)，所以在理解和实现上比较困难。
Chubby和Boxwood均使用Multi-Paxos。ZooKeeper使用的Zab也是Multi-Paxos的变形。
Raft 算法就是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现，实际项目中可以优先考虑 Raft 算法。

## 3.4 总结
- Paxos 包含 2 个部分: Basic Paxos 算法和 Multi-Paxos 思想。
	- Multi-Paxos 思想在前者的基础上，就一系列提案达成共识，并提升效率。
- Basic Paxos 算法有三个角色：提议者（Proposer）、接受者（Acceptor）、学习者（Learner）。
- 提案包含两个部分：编号N和值V。
- Basic Paxos 算法是一个二阶段算法：
	1. 准备阶段：提议者向所有接受者发Prepare请求。接受者如果判断提案编号>自己收到的最大编号，就返回OK，以及如果有的话还会返回自己曾接收过的提案
	2. 接受阶段：提议者收到半数以上Prepare请求的ok响应，就会向所有接受者发Accept请求。请求中的提案值=所有响应中最大编号的值 or 没有的话就用自己选的值
		1. 接受者如果判断提案编号>=自己收到的最大编号，就接受提案返回ok。
		2. 提议者收到半数以上Accept请求的ok响应，则确定值被选定，并广播给所有学习者
- 问题：
	1. 为什么要两阶段？因为可能有其他提议者已经提出了提案并被大多数接受者接受，为了避免同时提出个多个不同的提案，需要有个查询确认的过程。
	2. 为什么需要编号？接受者可能会收到多个提议者的提案，所以它需要用编号排序提案的顺序，决定要接受哪个提案。类似于乐观锁中版本号的概念。
- Multi-Paxos 思想在所有提议者中选举一个Leader，由Leader唯一地提交提案。这样只需要一阶段提交就行，提升了效率；并且没有提议者竞争，解决了活锁问题。
- 但 Multi-Paxos 思想缺少代码实现的必要细节(比如怎么选举领导者)，所以在理解和实现上比较困难。而Raft 算法就是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现。

Basic Paxos算法推导正确性的一些约束：
1. 要有多个 Acceptor 节点。这样避免了单点故障。
2. 一个Acceptor必须接受它收到的第一个提案。这样即使只有一个Proposer提出了一个提案，该提案也最终被选定。
3. 一个提案被选定需要被半数以上的Acceptor接受。这样避免了多个提案都被选定。
	- 这又暗示了，一个Acceptor必须能够接受不止一个提案，不然可能导致最终没有value被选定。
	- 例如三个Proposer分别向三个不同的Acceptor发送提案，每个Acceptor都只接受了一个，导致出现平票问题。
4. 如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v。
	- 这样保证了允许多个提案被接受：因为所有被选定的提案都具有相同的value值，就不会出现不一致。
5. 如果某个value为v的提案被选定了，那么之后任何Proposer提出的编号更高的提案的value必须也是v。这样就保证上面一条约束能得到满足。
6. 对于任意的N和V，如果提案[N, V]被提出，那么存在一个半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个。这样就保证上面一条约束能得到满足。
	- S中每个Acceptor都没有接受过编号小于N的提案。
	- S中Acceptor接受过的最大编号的提案的value为V。

# 4 Raft 算法详解
## 4.1 背景
> [!tip] 词义
> Raft 是 Re{liable|plicated|dundant} And Fault-Tolerant，即可靠、复制、冗余和容错，组合起来的单词。同时，Raft 在英文有“筏”的含义，隐喻一艘帮助你逃离 Paxos 小岛的救生筏。

Paxos 算法理解起来非常晦涩。此外，论文虽然提到了 Multi Paxos，但缺少实现细节。虽然所有的共识系统都是从 Paxos 算法开始的，但工程师们实现过程中有很多难以逾越的难题，往往不得已开发出与 Paxos 完全不一样的算法，这导致 Lamport 的证明并没有太大价值。所以，很长的一段时间内，实际上并没有一个被大众广泛认同的 Paxos 算法。

2013 年，斯坦福大学的学者 Diego Ongaro 和 John Ousterhout 发表了论文 《[In Search of an Understandable Consensus Algorithm](https://www.thebyte.com.cn/consensus/raft.html#footnote1)》，提出了 Raft 算法。Raft 论文开篇描述了 Raft 的证明和 Paxos 等价，详细阐述了算法如何实现。也就是说，Raft 天生就是 Paxos 算法的工程化。

不同于Paxos算法直接从分布式一致性问题出发推导出来，**Raft算法则是从多副本状态机的角度提出，用于管理多副本状态机的日志复制**。
Raft实现了和Paxos相同的功能，它将一致性分解为多个子问题：Leader选举（Leader election）、日志复制（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等。
同时，Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。

## 4.2 基础概念
### 4.2.1 节点类型
Raft 提出了领导者角色，通过选举机制“分享”提案权利：
- **领导者**（Leader）：负责处理所有客户端请求，将请求转换为“日志”复制到其他节点，不断地向所有节点广播心跳消息：“你们的领导还在，不要发起新的选举”。
- **跟随者**（Follower）：接收、处理领导者的消息，并向领导者反馈日志的写入情况。当领导者心跳超时时，他会主动站起来，推荐自己成为候选人。
- **候选人**（Candidate）：候选人属于过渡角色，他向所有的节点广播投票消息，如果他赢得多数选票，那么他将晋升为领导者。

在正常的情况下，只有一个服务器是 Leader，剩下的服务器是 Follower。Follower 是被动的，它们不会发送任何请求，只是响应来自 Leader 和 Candidate 的请求。

### 4.2.2 任期
联想到现实世界中的领导人都有一段不等的任期。自然，Raft 算法中也对应的概念 —— **“任期”（term）**。Raft 中的任期是一个递增的数字，贯穿于 Raft 的选举、日志复制和一致性维护过程中。
- **选举过程**：任期确保了领导者的唯一性。在一次任期内，只有获得多数选票的节点才能成为领导者。
- **日志一致性**：任期号会附加到每条日志条目中，帮助集群判断日志的最新程度。
- **冲突检测**：通过比较任期号，节点可以快速判断自己是否落后，并切换到跟随者状态。

每个节点都会存储当前的 term 号，当服务器之间进行通信时会交换当前的 term 号：
- 如果有服务器发现自己的 term 号比其他人小，那么他会更新到较大的 term 值。
- 如果一个 Candidate 或者 Leader 发现自己的 term 过期了，他会立即退回成 Follower。
- 如果一台服务器收到的请求的 term 号是过期的，那么它会拒绝此次请求。

### 4.2.3 日志
- `entry`：每一个事件成为 entry，只有 Leader 可以创建 entry。entry 的内容为`<term,index,cmd>`，其中 cmd 表示客户端请求的具体操作内容，也就是可以应用到状态机的操作。index 是entry在日志中的唯一编号（从 1 开始递增），标识位置
- `log`：由 entry 构成的数组，每一个 entry 都有一个表明自己在 log 中的 index。只有 Leader 才可以改变其他节点的 log。entry 总是先被 Leader 添加到自己的 log 数组中，然后再发起共识请求，获得同意后才会被 Leader 提交给状态机。Follower 只能从 Leader 获取新日志和当前的 commitIndex，然后把对应的 entry 应用到自己的状态机中。
- **只有 Leader 能创建日志条目**，Follower 只能接收 Leader 同步的日志（保证日志来源唯一）。

## 4.3 领导者选举
### 4.3.1 心跳机制
raft 使用心跳机制来触发 Leader 的选举。
如果一台服务器能够收到来自 Leader 或者 Candidate 的有效信息，那么它会一直保持为 Follower 状态，并且刷新自己的 electionElapsed，重新计时。
Leader 会向所有的 Follower 周期性发送心跳来保证自己的 Leader 地位。如果一个 Follower 在一个周期内没有收到心跳信息，就叫做**选举超时**，然后它就会认为此时没有可用的 Leader，并且**等待一段随机的时间后（避免活锁）**发起一次Leader选举。

### 4.3.2 RequestVote RPC 请求
为了开始新的选举，Follower 会**自增自己的 term 号**并且转换状态为 Candidate 并**给自己投一票**。然后他会向所有节点发起 **RequestVote RPC** 请求，消息示例如下：
```json
{
  "term": 5, // 候选者的当前任期号，用于通知接收方当前选举属于哪个任期。
  "candidateId": 3, // 候选者的节点 ID，标识请求投票的节点。
  "lastLogIndex": 12, // 候选者日志的最后一条日志的索引，用于比较日志的完整性。
  "lastLogTerm": 4//候选者日志的最后一条日志的任期号，用于进一步比较日志的新旧程度。
}
```
其他节点收到投票消息后，根据下面的条件判断是否投票：
- 当前节点尚未在本任期投票。
- 请求者的任期号 ≥ 自身当前任期号
- Candidate 的日志至少与投票者的日志一样新：如果最后一条log entry的term更大，则term大的更新；如果term一样大，则log index更大的更新。
RequestVote 响应的示例如下：
```json
{
  "term": 5, //接收方的当前任期号，用于告知候选者最新的任期号。如果候选者发现该值比自己大，会转为跟随者。
  "voteGranted": true//是否投票给候选者，true 表示同意，false 表示拒绝。
}
```

### 4.3.3 选举结果
Candidate 的状态会持续到以下情况发生：
- 赢得选举
- 其他节点赢得选举
- 一轮选举结束，无人胜出
赢得选举的条件是：一个 Candidate 在一个任期内收到了来自集群内的多数选票`（N/2+1）`，就可以成为 Leader。

在 Candidate 等待选票的时候，它可能收到其他节点声明自己是 Leader 的心跳，此时有两种情况：
- 该 Leader 的 term 号大于等于自己的 term 号，说明对方已经成为 Leader，则自己回退为 Follower。
- 该 Leader 的 term 号小于自己的 term 号，那么会拒绝该请求并让该节点更新 term。

下图概述了 Raft 集群 Leader 选举过程：
![[image-8.png]]

### 4.3.4 如何选择节点的数量？
Raft 日志复制过程需要等待多数节点确认。节点越多，等待的延迟也相应增加。所以说，以 Raft 构建的分布式系统并不是节点越多越好。如 etcd，推荐使用 3 个节点，对高可用性要求较高，且能容忍稍高的性能开销，可增加至 5 个节点，如果超出 5 个节点，可能得不偿失。

## 4.4 日志复制
### 4.4.1 AppendEntries RPC 请求
Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 **AppendEntries RPC** 复制日志条目：
```json
{
  "term": 5, // 领导者的任期号
  "leaderId": "leader-123",
  "prevLogIndex": 8, // 前一日志条目的索引
  "prevLogTerm": 4, // 前一日志条目的任期
  "entries": [
    { "index": 9, "term": 5, "command": "set x=4" }, // 要复制的日志条目
  ],
  "leaderCommit": 7// Leader 的“已提交”状态的日志条目索引号
}
```
当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。**如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了**。
某些Followers可能没有成功的复制日志，Leader会无限的重试 AppendEntries RPC直到所有的Followers最终存储了所有的日志条目。

#### 4.4.1.1 Follower 怎么提交日志
注意：follower自己不知道最新发过来的日志有没有被提交，它是通过leader发送的leaderCommit知道之前哪些日志被提交了。leader发送心跳的时候也会带上leaderCommit信息。具体的处理过程：
1. 对比自己的 `commitIndex` 和 RPC 中的 `leaderCommit`；
2. 如果 `leaderCommit > 自己的 commitIndex`，则将自己的 `commitIndex` 更新为 `min(leaderCommit, 自己的 lastLogIndex)`；
3. 最后，将所有索引 **小于等于新 commitIndex** 且 **未提交** 的日志条目，标记为 “已提交”，并应用到本地状态机。
	- `commitIndex`的来源是「leader 提交了自己任期的日志」，而 leader 提交自己任期日志的过程，会**强制让所有前置日志（包括前任 term 日志）被复制到多数节点**。参见日志冲突处理

### 4.4.2 Raft日志同步保证
Raft日志同步保证如下两点：
- 如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。
- 如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的。

第一条特性源于Leader在一个term内在给定的一个log index最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变。
第二条特性源于 AppendEntries 的一个简单的一致性检查。当发送一个 AppendEntries RPC 时，Leader会把新日志条目紧接着之前的条目的log index和term都包含在里面。如果Follower没有在它的日志中找到log index和term都相同的日志，它就会拒绝新的日志条目，然后就是日志冲突处理。

### 4.4.3 日志冲突处理
一般情况下，Leader和Followers的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，Leader崩溃可能会导致日志不一致：旧的Leader可能没有完全复制完日志中的所有条目。
![[image-9.png]]
一个Follower可能会丢失掉Leader上的一些条目，也有可能包含一些Leader没有的条目，也有可能两者都会发生（我猜脑裂的情况下可能会出现？）。丢失的或者多出来的条目可能会持续多个任期。

Leader通过强制Followers复制它的日志来处理日志的不一致，Followers上的不一致的日志会被Leader的日志覆盖。
具体来说，Leader 每次AppendEntries失败后尝试前一个日志条目，直到成功找到和Follower的日志一致位点，然后向后逐条覆盖Followers在该位置之后的条目。

## 4.5 安全性
### 4.5.1 选举限制
Leader 需要保证自己存储全部已经提交的日志条目。这样才可以使日志条目只有一个流向：从 Leader 流向 Follower，Leader 永远不会覆盖已经存在的日志条目。
每个 Candidate 发送 RequestVoteRPC 时，都会带上最后一个 entry 的信息。所有节点收到投票信息时，会对该 entry 进行比较，如果发现自己的更新，则拒绝投票给该 Candidate。
判断日志新旧的方式：如果两个日志的 term 不同，term 大的更新；如果 term 相同，更长的 index 更新。

### 4.5.2 日志提交限制
Leader只能提交当前term的已经复制到大多数服务器上的日志，旧term日志的提交要通过日志冲突处理来间接提交。之所以要这样，是为了保证安全性 —— 避免提交未被多数节点确认的旧日志。

举一个例子：
1. 假设有五个节点A、B、C、D、E
2. 前leader A接受客户端请求，把日志1复制到B上，还没确认是否被大多数follower复制完，前leader A就挂了
3. 经过选举后，B被选为了新leader。它有日志1，但是其它Follower没有，并且日志1是旧term的
4. 所以B需要在新的请求来到时，例如生成日志2，再把日志1的(term,index)作为最新信息发给Follower
5. Follower发现自己没有日志1，于是拒绝请求。然后进入日志冲突流程，B会把日志1、日志2都发给Follower。
6. 在得到大多数Follower的确认后，日志1就这样被日志2的请求间接提交了。

#### 4.5.2.1 问题1：和客户端的一致性问题
但是这样有一个问题：前 leader A 复制日志1的时候崩溃了，那么在客户端看来它的操作失败了，所以客户端应该认为日志1没有写入Raft集群。而Raft集群在客户端不知情的情况下提交了日志1。

之所以产生这种分歧，是因为Raft 的 "提交" 是**纯粹的集群内部状态**，与客户端是否收到响应**完全无关**。即使客户端从未收到确认，只要日志最终被复制到多数节点并满足条件，就会被提交。
Raft 协议保证的是**集群内部节点之间的日志一致性**，而非**客户端与集群状态的自动同步**，这两者是两个不同层面的问题。如果要求 "只有客户端确认才算成功"，会引入复杂的跨网络状态同步问题。
客户端要想准确感知操作结果，需要自行处理，例如进行幂等重试。

#### 4.5.2.2 问题2：日志最终能否被提交是概率发生的
如果新leader是C，因为C一开始没有被前 leader A 复制日志 1，所以 C 和其他节点复制日志的时候，也就不会间接提交日志1了。

**这是协议设计的必然特性，而非缺陷**。Raft 协议的核心目标是**保证集群内部日志一致性**，而非 "让所有可能的日志都被提交"。
1. 概率性的实际影响被协议机制大幅降低
	- **选举偏向拥有更多日志的节点**：Raft 的日志比较机制使拥有最新最长日志的节点更容易被选为 leader，这减少了 "丢失日志" 的概率。
	- **多数派交集特性**：在 5 节点集群中，任何两个多数派（≥3 节点）必然有交集，这使未完成日志被复制到多数节点的可能性增加。
2. 这是权衡一致性与性能的必要设计。如果 Raft 要求 "**必须确认所有前任日志都被提交或丢弃后才能选举新 leader**"，会带来以下问题：
	- 选举延迟增加：新 leader 必须先检查和处理所有前任日志。
	- 实现复杂度剧增：需要额外机制追踪所有可能的未完成日志。
	- 可用性下降：在网络分区等情况下，集群恢复时间延长。

### 4.5.3 节点崩溃
如果 Leader 崩溃，集群中的节点在 electionTimeout 时间内没有收到 Leader 的心跳信息就会触发新一轮的选主，**在选主期间整个集群对外是不可用的**。
如果 Follower 和 Candidate 崩溃，处理方式会简单很多。之后发送给它的 RequestVoteRPC 和 AppendEntriesRPC 会失败。由于 raft 的所有请求都是幂等的，所以失败的话会无限的重试。如果崩溃恢复后，就可以收到新的请求，然后选择追加或者拒绝 entry。

### 4.5.4 时间与可用性
raft 的要求之一就是安全性不依赖于时间：系统不能仅仅因为一些事件发生的比预想的快一些或者慢一些就产生错误。为了保证上述要求，最好能满足以下的时间条件：
`broadcastTime << electionTimeout << MTBF`
- `broadcastTime`：向其他节点并发传递消息的平均响应时间；
- `electionTimeout`：选举超时时间；
- `MTBF(mean time between failures)`：单台机器的平均健康时间；

`broadcastTime`应该比`electionTimeout`小一个数量级，为的是使`Leader`能够持续发送心跳信息（heartbeat）来阻止`Follower`开始选举。
`electionTimeout`也要比`MTBF`小几个数量级，为的是使得系统稳定运行。当`Leader`崩溃时，大约会在整个`electionTimeout`的时间内不可用；我们希望这种情况仅占全部时间的很小一部分。

由于`broadcastTime`和`MTBF`是由系统决定的属性，因此需要决定`electionTimeout`的时间。
一般来说，broadcastTime 一般为 `0.5～20ms`，electionTimeout 可以设置为 `10～500ms`，MTBF 一般为一两个月。

## 4.6 日志压缩
在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响可用性。Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃。
每个副本独立的对自己的系统状态进行snapshot，并且只能对已经提交的日志记录进行snapshot。
Snapshot中包含以下内容：
- 日志元数据。最后一条已提交的 log entry的 log index和term。这两个值在snapshot之后的第一条log entry的AppendEntries RPC的完整性检查的时候会被用上。
- 系统当前状态。

当Leader要发给某个日志落后太多的Follower的log entry被丢弃，Leader会将snapshot发给Follower；或者当新加进一台机器时，也会发送snapshot给它。发送snapshot使用InstalledSnapshot RPC。

做snapshot既不要做的太频繁，否则消耗磁盘带宽；也不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响可用性。推荐当日志达到某个固定的大小做一次snapshot。
做一次snapshot可能耗时过长，会影响正常日志同步。可以通过使用**copy-on-write技术（类似Redis）** 避免snapshot过程影响正常日志同步。

## 4.7 成员变更
### 4.7.1 问题背景
在前面的内容中，我们假设集群节点数固定，即集群的 Quorum 也保持不变。然而，在生产环境中，集群通常需要进行节点变更，例如因故障移除节点或扩容增加节点等。对于旨在实现容错能力的算法来说，显然不能通过“关闭集群、更新配置并重启系统”的方式来实现。

在讨论如何实现成员动态变更之前，我们需要先搞明白 Raft 集群中“配置”（configuration）的概念：配置说明集群由哪些节点组成。例如，一个集群有三个节点（Server 1、Server 2、Server 3），该集群的配置就是 [Server1、Server2、Server3]。
如果把“配置”当成 Raft 中的“特殊日志”。这样一来，成员动态变更需求就可以转化为“配置日志”的一致性问题。但需要注意的是，各个节点中的日志“应用”（apply）到状态机是异步的，不可能同时操作。这种情况下，apply “配置日志”很容易导致“脑裂”问题。

举个具体例子，假设有一个由三个节点 [Server1、Server2 和 Server3] 组成的 Raft 集群，当前的配置为 Cold。现在，我们计划增加两个节点 [Server1、Server2、Server3、Server4、Server5]，新的配置为 Cnew。
由于日志提交是异步处理的，假设 Server1 和 Server2 比较迟钝，仍在使用老配置 Cold，而 Server3、Server4、Server5 的状态机已经应用了新配置 Cnew：
- 假设 Server5 触发选举并赢得 Server3、Server4、Server5 的投票（满足 Cnew 配置下的 Quorum 3 要求），成为领导者；
- 同时，假设 Server1 也触发选举并赢得 Server1、Server2 的投票（满足 Cold 配置下的 Quorum 2 要求），成为领导者。

一个集群存在两个领导者也就是“脑裂”，同一个日志索引可能会对应不同的日志条目，最终导致集群数据不一致。
上述问题的根本原因在于，成员变更过程中形成了两个没有交集的 Quorum，即 [Server1, Server2] 和 [Server3, Server4, Server5] 各自为营。

### 4.7.2 联合共识算法
为了解决这一问题，Raft提出了两阶段的成员变更方法。集群先从旧成员配置Cold切换到一个过渡成员配置，称为联合共识（joint consensus），联合共识是旧成员配置Cold和新成员配置Cnew的组合Cold U Cnew，一旦联合共识Cold U Cnew被提交，系统再切换到新成员配置Cnew。

Raft两阶段成员变更过程如下：
1. Leader收到成员变更请求从Cold切成Cold,new；
2. Leader在本地生成一个新的log entry：**C_joint**，其内容是Cold∪Cnew，代表当前时刻新旧成员配置共存，写入本地日志，同时将该log entry复制至Cold∪Cnew中的所有副本。**在此之后新的日志同步需要保证得到Cold和Cnew两个多数派的确认**；
3. Follower收到C_joint的log entry后更新本地日志，并且此时就以该配置作为自己的成员配置；
4. 如果Cold和Cnew中的两个多数派确认了C_joint这条日志，Leader就提交这条log entry并切换到Cnew；
5. 接下来Leader生成一条新的log entry，其内容是新成员配置Cnew，同样将该log entry写入本地日志，同时复制到Follower上；
6. Follower收到新成员配置Cnew后，将其写入日志，并且从此刻起，就以该配置作为自己的成员配置，并且如果发现自己不在Cnew这个成员配置中会自动退出；
7. Leader收到Cnew的多数派确认后，表示成员变更成功，后续的日志只要得到Cnew多数派确认即可。Leader给客户端回复成员变更执行成功。

异常分析：
1. C_joint日志在提交之前，在这个阶段，C_joint中的所有节点有可能处于Cold的配置下，也有可能处于C_joint的配置下，如果这个时候原Leader宕机了，无论是发起新一轮投票的节点当前的配置是Cold还是C_joint，都需要Cold的节点同意投票，所以不会出现两个Leader
2. C_joint提交之后，Cnew下发之前，此时所有C_joint的配置已经在Cold和Cnew的大多数节点上，如果集群中的节点超时，那么肯定只有有C_joint配置的节点才能成为Leader，所以不会出现两个Leader
3. Cnew下发以后，Cnew提交之前，此时集群中的节点可能有三种，Cold的节点（可能一直没有收到请求）， C_joint的节点，Cnew的节点，其中Cold的节点因为没有最新的日志的，集群中的大多数节点是不会给他投票的，剩下的持有Cnew和C_joint的节点，无论是谁发起选举，都需要Cnew同意，那么也是不会出现两个Leader
4. Cnew提交之后，这个时候集群处于Cnew配置下运行，只有Cnew的节点才可以成为Leader，这个时候就可以开始下一轮的成员变更了。

两阶段成员变更比较通用且容易理解，但是实现比较复杂，同时两阶段的变更协议也会在一定程度上影响变更过程中的服务可用性，因此我们期望增强成员变更的限制，以简化操作流程。

### 4.7.3 单成员变更
Diego Ongaro 后来又提出一种更为简化的方案 — “单成员变更”（Single Server Changes）。该方案思想的核心是，既然同时提交多个成员变更可能引发问题，那么每次只提交一个成员变更，需要添加多个成员，就执行多次单成员变更操作。这样不就没有问题了么！
单成员变更方案很容易穷举所有情况，如下图所示，穷举奇/偶数集群下节点添加/删除情况。**如果每次只操作一个节点，Cold 的 Quorum 和 Cnew 的 Quorum 一定存在交集。交集节点只会进行一次投票，要么投票给 Cold，要么投票给 Cnew**。因此，不可能出现两个符合条件的 Quorum，也就不会出现两个领导者。
![[image-11.png]]
以(b)为例，Cold 为 [Server1、Server2、Server3]，该配置的 Quorum 为 2，Cnew 为 [Server1、Server2、Server3、Server4]，该配置的 Quorum 为 3。假设 Server1、Server2 比较迟钝，还在用 Cold ，其他节点的状态机已经应用 Cnew：
- 假设 Server1 触发选举，赢得 Server1，Server2 的投票，满足 Cold Quorum 要求，当选领导者；
- 假设 Server3 也触发选举，赢得 Server3，Server4 的投票，但不满足Cnew 的 Quorum 要求，选举失效。

变更的流程如下：
1. 向Leader提交一个成员变更请求，请求的内容为服务节点的是添加还是移除，以及服务节点的地址信息
2. Leader在收到请求以后，回向日志中追加一条ConfChange的日志，其中包含了Cnew，后续这些日志会随着AppendEntries的RPC同步所有的Follower节点中
3. 当ConfChange的日志被添加到日志中是立即生效，所以Cnew的节点可以进行投票（注意：不是等到提交以后才生效）
4. 当ConfChange的日志被复制到Cnew的大多数时，那么就可以对日志进行提交了

## 4.8 分片机制
再思考一个问题，Raft 算法属于“强领导者”（Strong Leader）模型，领导者负责所有写入操作，它的写瓶颈就是 Raft 集群的写瓶颈。那么，该如何突破 Raft 集群的写瓶颈呢？
一种方法是使用哈希算法将数据划分成多个独立部分（分片）。例如，将一个 100TB 规模数据的系统分成 10 部分，每部分只需处理 10TB。这种根据规则（范围或哈希）将数据分散处理的策略，被称为**分片机制（Sharding）**。
分片机制广泛应用于 Prometheus、Elasticsearch 、ClickHouse 等大数据系统。理论上，只要机器数量足够，分片机制就能支持任意规模的数据。

## 4.9 实际应用
- **etcd**：（读音近似 “**伊西迪**”）Kubernetes 核心存储，负责集群元数据管理，提供分布式锁和服务发现，所有变更通过 Raft 日志复制，多数派确认后提交
- **Consul**：服务发现与配置中心，Raft 保证 server 间数据一致性，支持多数据中心部署
- **RocketMQ**：4.5 版本后引入 DLedger (基于 Raft)，实现 Broker 主从自动故障转移和数据同步

## 4.10 总结
- 不同于Paxos算法直接从分布式一致性问题出发推导出来，Raft算法则是从多副本状态机的角度提出。
- Raft用于管理多副本状态机的日志复制。
- Raft 它将一致性分解为多个子问题：Leader选举（Leader election）、日志复制（Log replication）、和成员变更（Membership change）。
- Raft算法基础概念：
	1. 节点类型：领导者（Leader）、 跟随者（Follower）、候选人（Candidate）
	2. 任期：递增的数字，用于leader选举、日志新旧判断、冲突检测等。
	3. 日志：只能追加、完全有序的条目序列，只有 Leader 能创建日志条目，条目中包含任期、下标和指令
- 实际应用：etcd、Consul、RocketMQ

Raft算法核心思想：
1. Leader选举：raft 使用心跳机制来触发 Leader 的选举。
	1. Leader 会向所有的 Follower 周期性发送心跳来保证自己的 Leader 地位。
	2. 如果一个 Follower 在一个周期内没有收到心跳信息，就叫做**选举超时**
		- Leader 崩溃，在选主期间整个集群对外是不可用的
	3. Follower**等待一段随机的时间后（避免活锁）** 转变成Candidate，自增自己的任期号、给自己投一票并向其他节点发送投票请求。
	4. 每个 Candidate 发送投票请求时，都会带上最后一个日志条目的信息和自己的任期号。其他节点是否给它投票取决于三个条件：
		1. 投票者尚未在本任期投票。
		2. 请求者的任期号 ≥ 投票者当前任期号
		3. 请求者的日志至少和投票者的日志一样新：最后一条日志条目的任期更大则更新；如果任期一样大，则下标更大的更新。这确保 Leader 的日志是集群中是最完整的
	5. 一个 Candidate 在一个任期内收到了来自集群内的多数选票`（N/2+1）`，就可以成为 Leader，之后继续发送周期性心跳。
2. 日志复制：
	1. 简单流程：Leader把客户端请求作为日志条目加入到它的日志中，然后向Follower复制日志条目。如果一个日志条目被同步到大多数Follower上，则认为可以提交了。
	2. Leader的日志请求包含任期号、前一条日志的任期号和下标、当前要同步的日志条目、Leader已提交的日志索引
	3. Follower收到日志请求后，要验证两个条件，通过后会写入并返回成功：
		- Leader 的任期号 ≥ 自己当前任期号（确保 Leader 合法）；
		- 自己日志中存在 “前一条日志”，避免日志断层。
	4. 日志冲突处理：当Leader和Follower日志不一致时，Leader通过强制Followers复制它的日志来处理日志的不一致。
	5. 日志提交限制：Leader只能提交当前任期已经复制到大多数Follower上的日志，旧任期日志的提交要通过日志冲突处理来间接提交。之所以要这样，是为了保证安全性 —— 避免提交未被多数节点确认的旧日志。
3. 成员变更：联合共识算法，核心是避免脑裂问题
	1. 原来的集群配置叫Cold，新的叫Cnew。Leader节点会先写一个Cold和Cnew的并集配置到自己的日志中，叫Cjoint，自己也切换到Cjoint状态
	2. 这个状态下的Leader节点向Cold、Cnew两个配置中的节点发送Cjoint日志，并且要得到这两个配置分别的大多数节点确认，才能提交Cjoint。
		- C_joint在提交之前，如果Leader宕机了，无论发起选举的节点配置是Cold还是C_joint，都需要Cold大多数节点的同意，所以不会出现两个Leader
	3. 提交Cjoint后，Leader节点向Follower节点发送Cnew日志，收到Cnew配置的大多数节点确认之后提交Cnew。并且follower如果发现自己不在Cnew这个成员配置中会自动退出；
		- Cnew在提交之前，此时集群中的节点可能有三种：Cold、 C_joint、Cnew。Cold的节点因为没有最新的日志的，集群中的大多数节点是不会给他投票的；剩下无论是谁发起选举，都需要Cnew大多数节点的同意，所以也不会出现两个Leader
	4. Cnew提交后，集群切换到Cnew配置的状态，成员变更完成

# 5 Gossip 协议
## 5.1 六度分隔理论
说到社交网络，就不得不提著名的六度分隔理论。1967年，哈佛大学的心理学教授Stanley Milgram想要描绘一个连结人与社区的人际连系网。做过一次连锁信实验，结果发现了“六度分隔”现象。简单地说：“你和任何一个陌生人之间所间隔的人不会超过六个，也就是说，最多通过六个人你就能够认识任何一个陌生人。

数学解释该理论：若每个人平均认识260人，其六度就是260^6 =1,188,137,600,000。消除一些节点重复，那也几乎覆盖了整个地球人口若干多多倍，这也是Gossip协议的雏形。

## 5.2 简介
**Gossip 协议是一种允许在分布式系统中共享状态的去中心化通信协议，通过这种通信协议，我们可以将信息传播给网络或集群中的所有成员**。
Gossip协议基本思想就是：一个节点想要分享一些信息给网络中的其他的一些节点。于是，它周期性的随机选择一些节点，并把信息传递给这些节点。这些收到信息的节点接下来会做同样的事情，即把这些信息传递给其他一些随机选择的节点。
一般而言，信息会周期性的传递给N个目标节点，而不只是一个。这个N被称为fanout（这个单词的本意是扇出）。

使用Gossip协议的有：Redis Cluster、Consul、Apache Cassandra等。

## 5.3 消息传播模式
Gossip 设计了两种可能的消息传播模式：**反熵（Anti-Entropy）** 和 **传谣（Rumor-Mongering）**。

### 5.3.1 反熵（Anti-Entropy）
你可以把反熵中的熵理解为节点之间数据的混乱程度/差异性，反熵就是指消除不同节点中数据的差异，提升节点间数据的相似度，从而降低熵值。

集群中的节点，每隔段时间就随机选择某个其他节点，然后通过互相交换自己的**所有数据**来消除两者之间的差异，实现数据的最终一致性。
在实现反熵的时候，主要有推、拉和推拉三种方式：
- 推方式，就是将自己的所有副本数据，推给对方，修复对方副本中的熵。
- 拉方式，就是拉取对方的所有副本数据，修复自己副本中的熵。
- 推拉就是同时修复自己副本和对方副本中的熵。

在我们实际应用场景中，一般不会采用随机的节点进行反熵，而是可以设计成一个闭环。这样的话，我们能够在一个确定的时间范围内实现各个节点数据的最终一致性，而不是基于随机的概率。像 InfluxDB 就是这样来实现反熵的：
![[image-124.png]]
- 节点 A 推送数据给节点 B，节点 B 获取到节点 A 中的最新数据。
- 节点 B 推送数据给 C，节点 C 获取到节点 A，B 中的最新数据。
- 节点 C 推送数据给 A，节点 A 获取到节点 B，C 中的最新数据。
- 节点 A 再推送数据给 B 形成闭环，这样节点 B 就获取到节点 C 中的最新数据。

虽然反熵很简单实用，但是，节点过多或者节点动态变化的话，反熵就不太适用了。这个时候，我们想要实现最终一致性就要靠 **谣言传播(Rumor mongering)** 。

### 5.3.2 谣言传播(Rumor mongering)
谣言传播指的是分布式系统中的一个节点一旦有了新数据之后，就会变为活跃节点，活跃节点会周期性地联系其他节点向其发送**新数据**，直到所有的节点都存储了该新数据。
谣言传播比较适合节点数量比较多的情况，不过，这种模式下要尽量避免传播的信息包不能太大，避免网络消耗太大。

## 5.4 总结
Gossip 的特点（优势）：
1. 扩展性：网络可以允许节点的任意增加和减少，新增加的节点的状态最终会与其他节点一致。
2. 容错：网络中任何节点的宕机和重启都不会影响 Gossip 消息的传播，Gossip 协议具有天然的分布式系统容错特性。
3. 去中心化：Gossip 协议不要求任何中心节点，所有节点都可以是对等的，任何一个节点无需知道整个网络状况，只要网络是连通的，任意一个节点就可以把消息散播到全网。
4. 一致性收敛：Gossip 协议中的消息会以一传十、十传百一样的指数级速度在网络中快速传播，因此系统状态的不一致可以在很快的时间内收敛到一致。消息传播速度达到了 logN。
5. 简单：Gossip 协议的过程极其简单，实现起来几乎没有太多复杂性。

Gossip 的缺陷：
1. 消息的延迟：由于 Gossip 协议中，节点只会随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网的，因此使用 Gossip 协议会造成不可避免的消息延迟。不适合用在对实时性要求较高的场景下。
2. 消息冗余：Gossip 协议规定，节点会定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤，因此就不可避免的存在消息重复发送给同一节点的情况，造成了消息的冗余，同时也增加了收到消息的节点的处理压力。而且，由于是定期发送，因此，即使收到了消息的节点还会反复收到重复消息，加重了消息的冗余。

# 6 ZAB 协议
## 6.1 ZAB 概述
Zab协议 的全称是 **Zookeeper Atomic Broadcast** （Zookeeper原子广播）。Zookeeper 是通过 Zab 协议来保证分布式事务的最终一致性。
Zab协议是为分布式协调服务Zookeeper专门设计的一种 **支持崩溃恢复** 的 **原子广播协议** ，是Zookeeper保证数据一致性的核心算法。Zab借鉴了Paxos算法，但又不像Paxos那样，是一种通用的分布式一致性算法。**它是特别为Zookeeper设计的支持崩溃恢复的原子广播协议**。

在Zookeeper中主要依赖Zab协议来实现数据一致性，基于该协议，zk实现了一种主备模型（即Leader和Follower模型）的系统架构来保证集群中各个副本之间数据的一致性。 这里的主备系统架构模型，就是指**只有一台客户端（Leader）负责处理外部的写事务请求，然后Leader客户端将数据同步到其他Follower节点**。
Zookeeper 客户端会随机的链接到 zookeeper 集群中的一个节点，如果是读请求，就直接从当前节点中读取数据；如果是写请求，那么节点就会向 Leader 提交事务，Leader 接收到事务提交，会广播该事务，只要超过半数节点写入成功，该事务就会被提交。

**Zab 协议的特性**：
1. Zab 协议需要确保那些**已经在 Leader 服务器上提交（Commit）的事务最终被所有的服务器提交**。
2. Zab 协议需要确保**丢弃那些只在 Leader 上被提出而没有被提交的事务**。
    

## 6.2 ZooKeeper 概述
zookeeper是Apache开源的顶级项目，用于提供一个高性能、高可用，且具有严格的顺序访问控制能力（主要是写操作的严格顺序性）的分布式协调服务。可用于在分布式环境中保证数据的一致性，拥有非常广泛地使用场景。
主要的使用场景如下：
1. **实现数据的发布和订阅** ：可以把一些可能变化的配置数据放入zookeeper服务器，客户端可以订阅并监听这个配置，一旦配置更新，就可以重新获取配置。
2. **命名服务** ：有时分布式系统需要一些命名，比如提供RPC服务地址的名称、全局唯一不能重复的主键等，利用zookeeper节点不重名的特点可以进行命名服务。
3. **master选举**：有时分布式系统中存在多台服务器，必须选出一台master来执行特定的计算任务，master挂掉后，自动重新选出一台服务器担任master执行任务。这种情况可以使用zookeeper来实现，多个客户端创建同一个临时节点，只有一个可以成功，成功的节点就是master，其他客户端监听该节点，一旦被删除，说明master宕机，重新开始新的选举。
4. **负载均衡**：在消息队列的集群中，消息生产者需要比较均衡地将消息投递到不同的消息代理上，这里就涉及到负载均衡的使用。
5. **分布式锁**： 后台接口分布式部署了以后，为了避免出现不同服务器的线程同时修改同一个数据引起并发问题，需要进行跨主机跨进程的线程同步，这是就用到分布式锁，主要基于zookeeper的临时有序节点来实现。
6. **分布式队列** ：客户端提交的任务信息可以保存到zookeeper中，利用zookeeper有序节点的特性，实现一个先进先出的队列，zookeeper可以记录任务提交的拓扑信息并保持任务的有序调度。
7. **分布式协调和通知** ：可以实现不同机器之间心跳监测（临时有序节点是否存在）、数据通信（向节点写入数据并监听变化）等场景。
8. **集群元数据管理** ：每个集群的机器可以向zookeeper添加一个临时有序节点，只要节点存在表示机器存活。利用这个特点可以完成集群服务器的监控。还可以将主机的状态信息写入zookeeper的节点，监控中心订阅这些节点的数据来获得主机的实时信息。

## 6.3 基本概念
1. **数据节点（dataNode）**：zk 数据模型中的最小数据单元，数据模型是一棵树，由斜杠（ / ）分割的路径名唯一标识，数据节点可以存储数据内容及一系列属性信息，同时还可以挂载子节点，构成一个层次化的命名空间。
2. **事务及 zxid**：事务是指能够改变 Zookeeper 服务器状态的操作，一般包括数据节点的创建与删除、数据节点内容更新和客户端会话创建与失效等操作。对于每个事务请求，zk 都会为其分配一个全局唯一的事务 ID，即 zxid，是一个 64 位的数字，高 32 位表示该事务发生的集群选举周期 epoch（集群每发生一次 leader 选举，值加 1），低 32 位表示该事务在当前选择周期内的递增次序（leader 每处理一个事务请求，值加 1，发生一次 leader 选择，低 32 位要清 0）。
3. **事务日志**：所有事务操作都是需要记录到日志文件中的，可通过 dataLogDir 配置文件目录，文件是以写入的第一条事务 zxid 为后缀，方便后续的定位查找。zk 会采取“磁盘空间预分配”的策略，来避免磁盘随机查询频率，提升 zk 服务器对事务请求的影响能力。默认设置下，每次事务日志写入操作都会实时刷入磁盘，也可以设置成非实时（写到内存文件流，定时批量写入磁盘），但那样断电时会带来丢失数据的风险。
4. **数据快照**：数据快照是 zk 数据存储中另一个非常核心的运行机制。数据快照用来记录 zk 服务器上某一时刻的全量内存数据内容，并将其写入到指定的磁盘文件中，可通过 dataDir 配置文件目录。可配置参数 snapCount，设置两次快照之间的事务操作个数，zk 节点记录完事务日志时，会统计判断是否需要做数据快照（距离上次快照，事务操作次数等于snapCount/2~snapCount 中的某个值时，会触发快照生成操作，随机值是为了避免所有节点同时生成快照，导致集群影响缓慢）。
5. **消息队列**：Leader 服务器与每一个 Follower 服务器之间都维护了一个单独的 FIFO 消息队列进行收发消息，使用队列消息可以做到异步解耦。 Leader 和 Follower 之间只需要往队列中发消息即可。如果使用同步的方式会引起阻塞，性能要下降很多。

## 6.4 ZooKeeper 角色
ZooKeeper 有以下几种核心角色：
1. 领导者（leader），负责进行投票的发起和决议，更新系统状态
2. 学习者（learner），包括跟随者（follower）和观察者（observer），follower用于接受客户端请求并想客户端返回结果，在选主过程中参与投票
    - Observer可以接受客户端连接，将写请求转发给leader，但observer不参加投票过程，只同步leader的状态，observer的目的是为了扩展系统，提高读取速度
3. 客户端（client），请求发起方

![[image-152.png]]

### 6.4.1 角色状态
ZAB协议中存在着以下几种：
1. LOOKING：当前Server不知道leader是谁，正在搜寻
2. LEADING：当前Server即为选举出来的leader
3. FOLLOWING：leader已经选举出来，当前Server与之同步
4. Observing：这是 Zookeeper 引入 Observer 之后加入的，Observer 不参与选举，是只读节点，跟 Zab 协议没有关系。

在ZooKeeper的整个生命周期中，每个节点都会在Looking、Following、Leading状态间不断转换.
启动之初，ZooKeeper所有节点初始状态为Looking，这时集群会尝试选举出一个Leader节点，选举出的Leader节点切换为Leading状态；当节点发现集群中已经选举出Leader则该节点会切换到Following状态，然后和Leader节点保持同步；当Follower节点与Leader失去联系时Follower节点则会切换到Looking状态，开始新一轮选举。
### 6.4.2 什么是Observer？
zk3.3开始引入的角色，观察最新状态，并变更。与Follower不同只是不参与投票、选举，只提供非事务服务。
- Zookeeper需保证高可用和强一致性；
- 为了支持更多的客户端，需要增加更多Server；
- Server增多，投票阶段延迟增大，影响性能；
- 权衡伸缩性和高吞吐率，引入Observer
- Observer不参与投票；
- Observers接受客户端的连接，并将写请求转发给leader节点；
- 加入更多Observer节点，提高伸缩性，同时不影响吞吐率

### 6.4.3 节点的持久状态
- history：当前节点接收到事务 Proposal 的Log
- acceptedEpoch：Follower 已经接受的 Leader 更改 epoch 的 newEpoch 提议。
- currentEpoch：当前所处的 Leader 年代
- lastZxid：history 中最近接收到的Proposal 的 zxid（最大zxid）

## 6.5 Zab 协议实现的效果
1. **使用一个单一的主进程（Leader）来接收并处理客户端的事务请求**（也就是写请求），并采用了Zab的原子广播协议，将服务器数据的状态变更以 **事务proposal** （事务提议）的形式广播到所有的副本（Follower）进程上去。
2. **保证一个全局的变更序列被顺序引用**： Zookeeper是一个树形结构，很多操作都要先检查才能确定是否可以执行，比如P1的事务t1可能是创建节点"/a"，t2可能是创建节点"/a/bb"，只有先创建了父节点"/a"，才能创建子节点"/a/b"。
    为了保证这一点，Zab要保证同一个Leader发起的事务要按顺序被apply，同时还要保证只有先前Leader的事务被apply之后，新选举出来的Leader才能再次发起事务。
3. **当主进程出现异常的时候，整个zk集群依旧能正常工作**。
    

## 6.6 Zab协议核心
Zab协议的核心：**定义了事务请求的处理方式**。
1. 所有的事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被叫做 **Leader服务器**。其他剩余的服务器则是 **Follower服务器**。
2. Leader服务器 负责将一个客户端事务请求，转换成一个 **事务Proposal**，并将该 Proposal 分发给集群中所有的 Follower 服务器，也就是向所有 Follower 节点发送数据广播请求（或数据复制）
3. 分发之后Leader服务器需要等待所有Follower服务器的反馈（Ack请求），**在Zab协议中，只要超过半数的Follower服务器进行了正确的反馈**后（也就是收到半数以上的Follower的Ack请求），那么 Leader 就会再次向所有的 Follower服务器发送 Commit 消息，要求其将上一个事务proposal 进行提交。

## 6.7 Zab 协议如何保证数据一致性
假设两种异常情况：
1. 一个事务在 Leader 上提交了，并且过半的 Follower 都响应 Ack 了，但是 Leader 在 Commit 消息发出之前挂了。
2. 假设一个事务在 Leader 提出之后，Leader 挂了。
    
Zab 协议崩溃恢复要求满足以下两个要求：
1. 确保已经被 Leader 提交的 Proposal 必须最终被所有的 Follower 服务器提交。
2. 确保丢弃已经被 Leader 提出的但是没有被提交的 Proposal。
    
根据上述要求，Zab协议需要保证选举出来的Leader需要满足以下条件：
1. **新选举出来的 Leader 不能包含未提交的 Proposal** 。 即新选举的 Leader 必须都是已经提交了 Proposal 的 Follower 服务器节点。
2. **新选举的 Leader 节点中含有最大的 zxid** 。 这样做的好处是可以避免 Leader 服务器检查 Proposal 的提交和丢弃工作。
    

## 6.8 Zab 如何数据同步
完成 Leader 选举后（新的 Leader 具有最高的zxid），在正式开始工作之前（接收事务请求，然后提出新的 Proposal），Leader 服务器会首先确认事务日志中的所有的 Proposal 是否已经被集群中过半的服务器 Commit。

Leader服务器会为每一个Followe服务器都准备一个队列，并将没有被各Follower服务器同步的事务以Proposal消息形式逐个发送到Follower服务器，并在每一个Proposal消息后紧跟着再发送一个Commit消息，以表示这个事务已经被提交。等到Follower服务器将所有尚未同步的事务Proposal都从Leader服务器上同步过来并成功应用到本地数据库中后，Leader服务器就会将该Follower服务器加入到真正可用的Follower列表中，并开始之后的其他流程。

### 6.8.1 Zab 数据同步过程中，如何处理需要丢弃的 Proposal
每当选举产生一个新的 Leader ，就会从这个 Leader 服务器上取出本地事务日志充最大编号 Proposal 的 zxid，并从 zxid 中解析得到对应的 epoch 编号，然后再对其加1，之后该编号就作为新的 epoch 值，并将低32位数字归零，由0开始重新生成zxid。
**Zab 协议通过 epoch 编号来区分 Leader 变化周期**，能够有效避免不同的 Leader 错误的使用了相同的 zxid 编号提出了不一样的 Proposal 的异常情况。
基于以上策略，当一个包含了上一个 Leader 周期中尚未提交过的事务 Proposal 的服务器启动时，当这台机器加入集群中，以 Follower 角色连上 Leader 服务器后，Leader 服务器会根据自己服务器上最后提交的 Proposal 来和 Follower 服务器的 Proposal 进行比对，比对的结果肯定是 Leader 要求 Follower 进行一个回退操作，回退到一个确实已经被集群中过半机器 Commit 的最新 Proposal。

## 6.9 Zab 协议基本模式
Zab 协议包括两种基本的模式：**崩溃恢复** 和 **消息广播**。
### 6.9.1 崩溃恢复
当整个集群启动过程中，或者当 Leader 服务器出现网络中断、崩溃退出或重启等异常时，亦或是集群中不存在超过半数的服务器与Leader保存正常通信，Zab协议就会 **进入崩溃恢复模式**，选举产生新的Leader。
当选举产生了新的 Leader，同时集群中有过半的机器与该 Leader 服务器完成了**状态同步（即数据同步）**之后，Zab协议就会退出崩溃恢复模式，**进入消息广播模式**。
这时，如果有一台遵守Zab协议的服务器加入集群，因为此时集群中已经存在一个Leader服务器在广播消息，那么该新加入的服务器自动进入恢复模式：找到Leader服务器，并且完成数据同步。同步完成后，作为新的Follower一起参与到消息广播流程中。
崩溃恢复主要包括两部分：**Leader选举** 和 **数据恢复**。Leader 选举算法不仅仅需要让 Leader 自己知道自己已经被选举为 Leader ，同时还需要让集群中的所有其他机器也能够快速感知到选举产生的新 Leader 服务器。
### 6.9.2 消息广播
在zookeeper集群中，数据副本的传递策略就是采用消息广播模式。zookeeper中数据副本的同步方式与 2PC 相似，但是却又不同。2PC 要求协调者必须等到所有的参与者全部反馈ACK确认消息后，再发送commit消息。要求所有的参与者要么全部成功，要么全部失败。二段提交会产生严重的阻塞问题。
Zab协议中 Leader 等待 Follower 的ACK反馈消息是指“只要半数以上的Follower成功反馈即可，不需要收到全部Follower反馈”。
下面是消息广播具体步骤：
1. 客户端发起一个写操作请求。
2. Leader 服务器将客户端的请求转化为事务 Proposal 提案，同时为每个 Proposal 分配一个全局的ID，即zxid。
3. Leader 服务器为每个 Follower 服务器分配一个单独的队列，然后将需要广播的 Proposal 依次放到队列中取，并且根据 FIFO 策略进行消息发送。
4. Follower 接收到 Proposal 后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 Ack 响应消息。
5. Leader 接收到超过半数以上 Follower 的 Ack 响应消息后，即认为消息发送成功，可以发送 commit 消息。
6. Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交。Follower 接收到 commit 消息后，会将上一条事务提交。

## 6.10 Zab 的四个阶段

### 6.10.1 选举阶段（Leader Election）

节点在一开始都处于选举节点，只要有一个节点得到超过半数节点的票数，它就可以当选准 Leader，只有到达第三个阶段（也就是同步阶段），这个准 Leader 才会成为真正的 Leader。

**Zookeeper 规定所有有效的投票都必须在同一个 轮次 中，每个服务器在开始新一轮投票时，都会对自己维护的 logicalClock 进行自增操作**。

每个服务器在广播自己的选票前，会将自己的投票箱（recvset）清空。该投票箱记录了所受到的选票。 例如：Server_2 投票给 Server_3，Server_3 投票给 Server_1，则Server_1的投票箱为(2,3)、(3,1)、(1,1)（每个服务器都会默认给自己投票）。前一个数字表示投票者，后一个数字表示被选举者。票箱中只会记录每一个投票者的最后一次投票记录，如果投票者更新自己的选票，则其他服务器收到该新选票后会在自己的票箱中更新该服务器的选票。

这一阶段的目的就是为了选出一个准 Leader ，然后进入下一个阶段。协议并没有规定详细的选举算法，后面会提到实现中使用的 Fast Leader Election。
![[zoo选举.webp]]
### 6.10.2 发现阶段（Descovery）
在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。这个一阶段的主要目的是**发现当前大多数节点接收的最新提议**，并且准 leader 生成新的 epoch，让 followers 接受，更新它们的 accepted Epoch。
![[image-153.png]]
一个 follower 只会连接一个 leader，如果有一个节点 f 认为另一个 follower p 是 leader，f 在尝试连接 p 时会被拒绝，f 被拒绝之后，就会进入 Phase 0。

**同时 Leader 也会比较和 Follower 的日志，并更新最新的日志。后面的快速选举算法不用这里的日志同步**。
### 6.10.3 Synchronization（同步阶段）

**同步阶段主要是利用 Leader 前一阶段获得的最新 Proposal 历史，同步集群中所有的副本**。只有当 quorum（超过半数的节点） 都同步完成，准 Leader 才会成为真正的 Leader。Follower 只会接收 zxid 比自己 lastZxid 大的 Proposal
![[image-154.png]]
### 6.10.4 Broadcast（广播阶段）

到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 Leader 可以进行消息广播。同时，如果有新的节点加入，还需要对新节点进行同步。 需要注意的是，Zab 提交事务并不像 2PC 一样需要全部 Follower 都 Ack，只需要得到 quorum（超过半数的节点）的Ack 就可以。
![[image-155.png]]
## 6.11 协议实现

协议的 Java 版本实现跟上面的定义略有不同，选举阶段使用的是 Fast Leader Election（FLE），它包含了发现阶段。因为FLE会选举拥有最新提议的历史节点作为 Leader，这样就省去了发现最新提议的步骤。实际的实现将发现和同步阶段合并为 Recovery Phase（恢复阶段），所以，Zab 的实现实际上有三个阶段：

1. **选举（Fast Leader Election）**
    
2. **恢复（Recovery Phase）**
    
3. **广播（Broadcast Phase）**
    

### 6.11.1 Fast Leader Election（快速选举）

前面提到的 FLE 会选举拥有最新Proposal history （lastZxid最大）的节点作为 Leader，这样就省去了发现最新提议的步骤。**这是基于拥有最新提议的节点也拥有最新的提交记录**。

成为 Leader 的条件：

1. 选 epoch 最大的
    
2. 若 epoch 相等，选 zxid 最大的
    
3. 若 epoch 和 zxid 相等，选择 server_id 最大的（zoo.cfg中的myid）
    

选举过程如下：

1. 节点在选举开始时，都默认投票给自己，当接收其他节点的选票时:
    
    - 如果接收到的投票根据上面的原则大于自己，就认可当前收到的投票，并更新自己选票的相应值为该节点值，表示选举该节点。
        
    - 如果接收到的投票根据上面的原则小于自己，那么就坚持自己的投票，不做任何变更。
        
2. 重新发出选票
    
3. 经过第二次投票后，集群中每台机器都会再次收到其他机器的投票，然后开始统计，如果一台机器收到超过了半数的相同投票，那么这个投票对应的 myid机器即为 Leader，其他节点会设置自己的状态为 Following。
    

### 6.11.2 Recovery Phase（恢复阶段）

这一阶段 Follower 发送他们的 lastZxid 给 Leader，Leader 根据 lastZxid 决定如何同步数据。这里的实现跟前面的 Phase 2 有所不同：Follower 收到 TRUNC 指令会终止 L.lastCommitedZxid 之后的 Proposal ，收到 DIFF 指令会接收新的 Proposal。
![[webp.webp]]
history.lastCommitedZxid：最近被提交的 Proposal zxid history.oldThreshold：被认为已经太旧的已经提交的 Proposal zxid

## 6.12 ZAB 协议常见的误区

**写入节点后的数据，立马就能被读到，这是错误的**。zk 写入是必须通过 leader 串行的写入，而且只要一半以上的节点写入成功即可。而任何节点都可提供读取服务。例如：zk，有 1~5 个节点，写入了一个最新的数据，最新数据写入到节点 1~3，会返回成功。然后读取请求过来要读取最新的节点数据，请求可能被分配到节点 4~5 。而此时最新数据还没有同步到节点4~5。会读取不到最近的数据。如果想要读取到最新的数据，可以在读取前使用 **sync 命令**。

**zk启动节点不能偶数台，这也是错误的**。zk 是需要一半以上节点才能正常工作的。例如创建 4 个节点，半数以上正常节点数是 3。也就是最多只允许一台机器 down 掉。而 3 台节点，半数以上正常节点数是 2，也是最多允许一台机器 down 掉。4 个节点，多了一台机器的成本，但是健壮性和 3 个节点的集群一样。基于成本的考虑是不推荐的。

## 6.13 ZK性能问题

客户端对ZK的更新操作都是永久的，不可回退的，也就是说，一旦客户端收到一个来自server操作成功的响应，那么这个变更就永久生效了。为做到这点，ZK会将每次更新操作以事务日志的形式写入磁盘，写入成功后才会给予客户端响应。明白这点之后，你就会明白磁盘的吞吐性能对于ZK的影响了，磁盘写入速度制约着ZK每个更新操作的响应。

事务日志的写性能确实对ZK性能，尤其是更新操作的性能影响很大，为了尽量减少ZK在读写磁盘上的性能损失，可以考虑使用单独的磁盘作为事务日志的输出（使用单独的挂载点用于事务日志的输出）

ZK的事务日志输出是一个顺序写文件的过程，本身性能是很高的，所以尽量保证不要和其它随机写的应用程序共享一块磁盘，尽量避免对磁盘的竞争。

---
# 7 引用
[分布式共识](https://www.thebyte.com.cn/consensus/summary.html)
[Paxos算法](https://www.cnblogs.com/linbingdong/p/6253479.html)
[Raft算法-知乎](https://zhuanlan.zhihu.com/p/32052223)
[Raft算法](https://blog.csdn.net/zhousenshan/article/details/137998763)