2025-04-29 22:00
Status: #idea
Tags: [[MySQL]]

# 1 为什么要分库分表
1. 数据量突破单库单表瓶颈
	- 单表数据量达到千万级及以上，查询、插入、更新操作响应变慢。
	- 单库磁盘空间不足，无法满足业务持续增长的数据存储需求。
2. 高并发访问导致性能下降
	- 单库或单表面临高并发读写压力，出现连接数耗尽、锁等待频繁等问题。
	- 核心业务高峰期（如电商秒杀、直播带货），单库单表无法支撑突发流量。
3. 业务模块隔离需求
	- 不同业务模块（如用户、订单、商品）数据量和访问模式差异大，需独立资源避免相互影响。
	- 部分业务需特殊配置（如高可用、加密存储），需通过分库实现独立管理。
4. 数据生命周期管理需求
	- 数据存在冷热差异，需对历史冷数据进行归档或清理，按时间分表便于操作。
	- 不同时期数据查询频率不同，需拆分后针对性优化存储和索引策略。

# 2 分库
## 2.1 水平分库
![[image-108.png]]
1. 概念：以**字段**为依据，按照一定策略（hash、range等），将一个**库**中的数据拆分到多个**库**中。
2. 结果：
	- 每个**库**的**结构**都一样；
	- 每个**库**的**数据**都不一样，没有交集；
	- 所有**库**的**并集**是全量数据；
3. 应用场景：
	- **系统绝对并发量上来了**，分表难以根本上解决问题，并且还没有明显的业务归属来垂直分库。
	- **冷热分离**

## 2.2 垂直分库
![[image-109.png]]
1. 概念：以**表**为依据，按照业务归属不同，将不同的**表**拆分到不同的**库**中。
2. 结果：
	- 每个**库**的**结构**都不一样；
	- 每个**库**的**数据**也不一样，没有交集；
	- 所有**库**的**并集**是全量数据；
3. 场景：**可以抽象出单独的业务模块**，到这一步，基本上就可以服务化了。

# 3 分表
## 3.1 水平分表
![[image-110.png]]
1. 概念：以**字段**为依据，按照一定策略（hash、range等），将一个**表**中的数据拆分到多个**表**中。
2. 结果：
	- 每个**表**的**结构**都一样；
	- 每个**表**的**数据**都不一样，没有交集；
	- 所有**表**的**并集**是全量数据；
3. 场景：
	- 系统绝对并发量并没有上来，只是**单表的数据量太多**，影响了SQL效率，加重了CPU负担，以至于成为瓶颈。
	- **历史数据归档**

## 3.2 垂直分表
![[image-111.png]]
1. 概念：以**字段**为依据，按照字段的活跃性，将**表**中字段拆到不同的**表**（主表和扩展表）中。
2. 结果：
	- 每个**表**的**结构**都不一样；
	- 每个**表**的**数据**也不一样，一般来说，每个表的**字段**至少有一列交集，一般是主键，用于关联数据；
	- 所有**表**的**并集**是全量数据；
3. 系统绝对并发量并没有上来，表的记录并不多，但是**字段多，并且热点数据和非热点数据在一起**，单行数据所需的存储空间较大。以至于数据库缓存的数据行减少，查询时会去读磁盘数据产生大量的随机读IO，产生IO瓶颈。

# 4 水平分片策略
## 4.1 核心概念
- **分片键**：拆分数据的依据字段（如`user_id`、`create_time`），是水平分片的核心，直接决定数据分布和查询效率。
	- 分片键选择原则：优先选 “查询频率高、分布均匀、变更少” 的字段（如`user_id`、`create_time`）
- **分片粒度**：分库（跨数据库实例）或分表（同数据库内跨表），通常 “分库 + 分表” 组合使用（如先按`user_id`分 8 个库，每个库内按`create_time`分 12 张表）。
- **路由规则**：根据分片键定位数据所在分片的逻辑（由中间件如 Sharding-JDBC、MyCAT 实现）。
- **如何分片的原则**：数据的生命周期，数据的查询方式、变更方式有哪些，QPS高不高，数据量大不大

## 4.2 主流水平分片策略详解
### 4.2.1 哈希切分
以分片键的哈希值为依据，通过 “取模” 或 “一致性哈希” 将数据均匀分配到预设分片。
应用场景：
- 读写并发高，需分散单分片压力（如秒杀订单、直播礼物记录）。
- 分片键查询频率高（如 “按用户 ID 查询订单”“按设备 ID 查询日志”）；

#### 4.2.1.1 普通哈希取模
`分片索引 = hash(分片键) % 分片数量`。例：分片键`user_id=1001`，hash 后值为`2005`，分片数量 = 8 → `2005%8=5` → 数据写入第 5 个分片。

优点：
- 实现简单

缺点：
- 数据分布要求均匀，无明显热点（如电商用户订单、社交平台用户动态）；
- 无法高效支持范围查询（如 “查询用户近 3 个月订单” 需跨多个分片）
- 扩容 / 缩容时需大规模数据迁移

##### 4.2.1.1.1 扩容时数据迁移流程
###### 4.2.1.1.1.1 准备阶段
1. 评估和规划
	- 确认当前分片数 N 和目标分片数 M（建议 M 为 N 的倍数，如 4→8→16）
		- 这样的话，**老库的旧分片 ID = 新分片 ID 的 “低 1 位”**。老库原本承载的 “旧分片 0~3”，在新规则下直接对应 “新分片 0~3”，老库的数据不需要整体迁移，只需把 “新规则下属于新分片 4~7” 的数据从老库中分离出来，迁移到 4 个新库即可。
	- 计算需迁移数据比例：约 (M-N)/M
	- 准备新服务器资源（数量 = M-N）
2. 搭建新分片环境
	- 新建 M-N 个数据库实例，配置与原实例一致
	- 在新实例上创建完整表结构（与原表相同）
	- 配置分库分表中间件（如 ShardingSphere、MyCAT），添加新分片信息
	- 老库资源升级（可选）：检查老库的 CPU、内存、磁盘是否满足扩容后的负载。

###### 4.2.1.1.1.2 数据迁移阶段（核心）
**方案 A：双写迁移（适合不可停机系统）**
1. 应用改造支持双写：全量迁移期间及之后，主动同步所有新写操作（增删改）
	```java
	// 伪代码：双写实现
	void writeData(String key, Object value) {
	  // 写入旧分片
	  oldShard.write(key, value);
	  // 同时写入新分片（按新规则计算）
	  newShard.write(key, value);
	}
	```
2. 然后全量数据迁移：使用数据迁移工具（如 ShardingSphere Migration、DataX）从原库导出数据，按新分片规则将数据导入新库
	- 主流数据迁移工具支持通过配置新分片规则，自动完成 “读旧数据→按新规则路由→写新库” 的全流程，无需手动编写大量迁移代码
	- 工具的自动化迁移流程如下：
		1. 工具连接原分片集群（旧库4个：db0~db3）
		2. 按表/分库顺序，批量读取旧库中的全量数据（如先读db0.t_order，再读db1.t_order...）
		3. 对每条数据，提取分片键（如user_id）
		4. 按预设的新分片规则（hash(user_id) % 8）计算目标新库/表
		5. 工具连接目标分片集群（新库8个：db0~new_db3）
		6. 将数据写入对应的目标新库/表
			- 如果数据迁移前后分片id不变，则不需要迁移
		7. 重复步骤2~6，直到所有旧库数据迁移完成
3. 全量迁移完成后，最后增量数据同步（被动兜底）：全量迁移期间及之后，所有数据库变更（增删改）补全到新库，避免双写遗漏、双写失败等场景。它是基于 binlog 的 “事后补全”，是数据一致性的 “第二道保障”（兜底）。
	- 部署 CDC 工具（如 Canal）监听原库 binlog
	- 将变更数据按新分片规则同步到新库
	- 监控同步延迟，确保数据一致

**方案 B：停机迁移（适合允许短时间中断的系统）**
1. 暂停写操作
	- 选择业务低峰期，暂停应用写请求
	- 记录此时 binlog 位置，确保数据一致性
2. 全量迁移

###### 4.2.1.1.1.3 切换与验证阶段
1. 路由规则切换
	- 更新中间件配置，启用新分片规则（hash (key)% M）
	- 重启应用或重新加载配置，确保使用新路由
2. 验证与收尾
	- 随机抽查部分数据，确认在新分片存在且内容正确
	- 监控系统性能，观察新分片负载是否均衡
3. 清理老库中已迁移到新库的数据（释放磁盘空间）

##### 4.2.1.1.2 缩容场景 (N→M，M<N) 数据迁移步骤
###### 4.2.1.1.2.1 准备阶段
- 确认当前分片数 N 和目标分片数 M（建议 N 为 M 的倍数）
- 计算需迁移数据比例：约 (N-M)/N
- 确定哪些节点将被保留（建议选择连续编号节点，如保留 0-3，移除 4-7）
- 准备临时存储：由于缩容涉及数据合并，需准备临时存储或额外节点处理冲突

###### 4.2.1.1.2.2 数据迁移阶段
1. 应用改造支持数据重定向
	- 配置中间件，将指向即将删除节点的数据请求重定向到保留节点
	- 或修改应用逻辑，在写入前检查目标分片是否在保留集合中
2. 数据迁移（核心）
	- **方案 A：单阶段迁移**
	```
	for 每个要删除的旧分片S in (M..N-1):
		for 分片S中的每条数据:
			计算新分片 = hash(分片键) % M
			将数据从S迁移到新分片
			删除S中的原数据
	```
	- **方案 B：双写过渡（适合不能停机系统）**
		- 应用同时写入原分片和新分片（按新规则）
		- 当新分片数据完全同步后，切换路由规则
		- 验证后删除旧分片

###### 4.2.1.1.2.3 切换与验证阶段
1. 路由规则切换
	- 更新中间件配置，启用新分片规则（hash (key)% M）
	- 确保所有请求路由到保留的 M 个节点
2. 数据验证与清理
	- 验证保留节点数据完整性（特别是被迁移数据）
	- 删除废弃节点，释放资源

#### 4.2.1.2 一致性哈希
流程：
1. 构建 0~2³² 的哈希环，将所有分片节点（库 / 表）映射到环上。
	- 可添加虚拟节点避免数据倾斜：将一个分片节点计算多个 `Hash` 值分散到圆环的不同位置
2. 计算分片键的哈希值，映射到环上某点；
3. 顺时针找到最近的分片节点，即为数据存储目标。

优点：
- 扩容 / 缩容时数据迁移量极少（最核心优势）：扩容 / 缩容时，仅需迁移 “新增 / 删除节点相邻区间” 的数据，迁移量与节点数量无关，仅与 “数据分布区间大小” 相关（通常仅 10%~20%）。
- 支持节点异构与负载均衡（实际场景刚需）：通过「虚拟节点」，灵活调整数据分布比例，让性能强的节点承载更多数据，实现 “负载与节点能力匹配”。
- 容错性更强（高可用场景必备）：若某个节点宕机，该节点上的老数据都会暂时不可用。
	- 但一致性哈希可正常路由新数据，而普通哈希没办法路由。
	- 普通哈希必须加一个新节点顶替、或者修复宕机节点；而一致性哈希暂时无需特殊处理
	- 普通哈希恢复成本高，而一致性哈希恢复后只需迁移部分数据即可。
	- **一致性哈希把故障 “隔离在局部”**

缺点：
- 实现与运维复杂度更高：
	- 哈希环的节点映射与维护（物理节点→虚拟节点的映射关系）；
	- 跨节点数据迁移的断点续传与一致性保障（需工具支持虚拟节点级别的数据拆分）。
- 数据分布均匀性依赖虚拟节点配置：
	- 普通哈希只要分片键分布均匀（如 user_id、order_id），数据天然均匀分布到各个节点
	- 一致性哈希：若虚拟节点配置不好，会出现 “数据倾斜”
- 无法高效支持范围查询

适用场景：
1. 集群规模较大（物理节点≥8 个），且需频繁扩容 / 缩容（如 SaaS 平台租户库、直播平台日志集群）；
2. 节点性能异构（如混合高配 / 低配服务器），需按节点能力分配负载；
3. 高可用要求高（不允许节点故障导致大规模数据迁移），如金融交易系统、核心订单系统。

### 4.2.2 范围分片
#### 4.2.2.1 实现方式
按分片键的数值 / 时间范围划分分片，每个分片存储固定区间的数据：
- **数值范围**：分片键为连续数值（如`user_id`、`order_id`），按区间拆分（如 1~100 万、100 万～200 万）；
- **时间范围**：分片键为时间字段（如`create_time`），按天 / 周 / 月 / 季度拆分（如 202401、202402）。

#### 4.2.2.2 应用场景
- 高频按范围查询（如 “查询 2024 年 3 月订单”“查询用户 ID 50 万～100 万的用户信息”）；
- 数据有明显时间 / 数值生命周期（如日志、账单、历史订单，需归档冷数据）；
- 分片键为自增 ID（如`order_id`），数据按顺序写入（无明显热点倾斜）。

缺点：
- 热点数据集中（如时间分片时，最新月份的订单分片写入压力最大）
- 扩容时需新增分片区间，无法像哈希分片那样均匀拆分现有数据

### 4.2.3 列表分片
#### 4.2.3.1 实现方式
按**分片键的枚举值列表**划分分片，每个分片关联固定的枚举值集合：
- 明确指定 “分片键取值→分片” 的映射关系，例：
    分片 0：`province_id ∈ [1,2,3]`（北京、天津、河北）；
    分片 1：`province_id ∈ [4,5,6]`（上海、江苏、浙江）；
    分片 2：`province_id ∈ [7,8,...]`（其他省份）。

#### 4.2.3.2 应用场景
- 分片键为离散的枚举值（如省份、城市、订单状态、商户类型）；
- 业务需按枚举维度隔离数据（如不同地区的物流数据、不同状态的订单）；
- 枚举值数量少且稳定（不会频繁新增枚举值）。

缺点：
- 枚举值新增时需扩容分片，修改路由规则（如新增省份需调整分片映射）
- 若枚举值对应的数据量不均（如某省份用户量是其他省份 10 倍），会导致分片倾斜
- 枚举值过多时，路由规则维护复杂

### 4.2.4 复合分片
#### 4.2.4.1 实现方式
结合两种及以上分片策略，采用 “多层分片” 逻辑：
- 通常是 “粗粒度分片 + 细粒度分片” 的组合，例：
    第一层（分库）：`hash(user_id) % 8`（按用户 ID 哈希分 8 个库）；
    第二层（分表）：`create_time` 按月分表（每个库内拆分为 12 张表）；
- 核心：先按一个字段分散大压力（如用户 ID 哈希分库），再按另一个字段优化查询（如时间分表）。

#### 4.2.4.2  应用场景
- 高并发、多查询维度的核心业务（如电商订单、直播礼物、社交动态）；
- 需同时支持 “分片键精准查询” 和 “范围查询”（如按用户 ID 查订单 + 按时间查订单）；

缺点：
- 路由逻辑复杂
- 跨分片查询难度高
- 数据迁移、扩容复杂度高（需同时考虑两层分片的调整）

## 4.3 各策略核心对比与选型建议
| 策略   | 核心优势          | 核心局限           | 选型关键                  |
| ---- | ------------- | -------------- | --------------------- |
| 哈希分片 | 数据均匀，高并发适配    | 不支持范围查询，扩容迁移麻烦 | 分片键查询为主，无范围查询需求       |
| 范围分片 | 范围查询高效，冷数据易归档 | 热点集中，数据分布不均    | 时间 / 数值范围查询频繁，数据有生命周期 |
| 列表分片 | 业务维度隔离，查询直观   | 枚举值依赖，易倾斜      | 分片键为离散枚举值，业务隔离需求强     |
| 复合分片 | 多查询场景适配，热点分散  | 路由复杂，跨分片查询难    | 核心业务，需同时支持精准查询和范围查询   |

一些建议：
1. 热点问题解决方案：时间分片的热点（最新分片）可通过 “热点分片拆分”（如最新月份再按哈希分表）；列表分片的倾斜可通过 “动态调整枚举值映射”（如将大省份拆分到多个分片）。
2. 中间件适配：Sharding-JDBC 支持所有策略，MyCAT 适合代理层分片，NewSQL（TiDB/OceanBase）可自动分片（无需手动设计）。
3. 对于按时间范围分片，考虑数据归档策略。例如超过 3 年的历史数据迁移到低成本存储（如阿里云 OSS+ClickHouse），支持离线查询，主库仅保留近 3 年热数据

# 5 多张有关联的表如何分库分表
## 5.1 背景
假设我做了一个抢红包系统，有红包明细表、用户发布红包表、用户抢红包表。
需要能快速查询:
1. 红包被哪些人抢了
2. 用户发布了哪些红包
3. 用户抢了哪些红包。 

这三种查询并发量都很大，而我有N个库，所以我做了对三张表分别做了哈希分库，加速它们的查询。这样做可以吗？

## 5.2 问题
### 5.2.1 数据一致性难以保障
抢红包是典型的 “写扩散” 场景（发布红包时需写 “用户发布红包表”+“红包明细表”；抢红包时需写 “用户抢红包表”+“红包明细表”）：
- 原方案中，三张表分库规则不同，写操作需要跨多个库（比如发布红包时，写`publisher_id`对应的库 +`red_packet_id`对应的库）；
- 跨库写无法用本地事务保障一致性，只能用分布式事务（如 2PC、TCC），但分布式事务会降低并发性能，且复杂度极高（比如网络波动导致某一个库写失败，需要回滚其他库）。

### 5.2.2 维护成本高
三张表独立分库，意味着：
- 扩容时需要分别处理三张表的分片迁移（比如一致性哈希扩容，要维护三张表的哈希环）；
- 同一红包的信息分散在多个库。排查问题时，需要跨多个库定位数据（比如查一个红包的全链路，要去`publisher_id`库、`red_packet_id`库、`grabber_id`库找数据）。

### 5.2.3 热点红包的单分片过载问题
因为同一红包的信息分散在多个库，对于热点红包，难以做热点隔离等优化。

## 5.3 改进方案
以 “红包 ID（red_packet_id）” 作为三张表的 “主分片键”，统一分库规则；再通过 “冗余索引表” 优化非主维度查询。既保证表间关联高效，又兼顾多维度查询，还能保障一致性。

### 5.3.1 统一三张表的分库规则（核心改进）
#### 5.3.1.1 分片规则设计
- 主分片键：`red_packet_id`（红包 ID，全局唯一，比如雪花算法生成）；
- 分库算法：一致性哈希（支持动态扩容，迁移量少）；
- 分库逻辑：`所有表的分片 = 一致性哈希(red_packet_id) % N`（N 为库数量）。

#### 5.3.1.2 三张表的分库效果
- 红包明细表（t_red_packet_detail）：按`red_packet_id`分库 → 同一红包的所有抢红包明细都在同一个库；
- 用户发布红包表（t_red_packet_publish）：按`red_packet_id`分库 → 红包的发布信息和明细在同一个库；
- 用户抢红包表（t_red_packet_grab）：按`red_packet_id`分库 → 同一红包的抢红包记录和明细在同一个库。

#### 5.3.1.3 解决的核心问题
- 表间关联无跨库：比如 “查用户 A 发布的红包 B 的明细”→ 先通过冗余表找到`red_packet_id=B`，再路由到对应库，直接 JOIN 三张表（同一库内 JOIN，性能极高）；
- 数据一致性易保障：发布红包时，写 “发布表 + 明细表” 在同一个库，用本地事务即可（无需分布式事务）；抢红包时，写 “抢红包表 + 明细表” 也在同一个库，本地事务搞定。

### 5.3.2 用 “冗余索引表” 优化非主维度查询
统一分库后，主维度查询（按红包 ID 查明细）效率极高，但另外两个维度（按发布用户 ID 查红包、按抢用户 ID 查红包）会变成 “跨库扫描”—— 解决方案：**新增两个 “冗余索引表”，专门优化这两个维度**。

#### 5.3.2.1 冗余表 1：用户发布红包索引表（t_publisher_index）
- 用途：优化 “查询用户发布了哪些红包”；
- 分片键：`publisher_id`（发布用户 ID）；
- 分库算法：一致性哈希（publisher_id）；
- 表结构（仅存核心查询字段，不存冗余数据）
- 查询流程：用户 A 查发布的红包 → 按`publisher_id=A`路由到索引表的对应库 → 拿到所有`red_packet_id` → 按`red_packet_id`分别路由到主库，查询发布表 + 明细表的完整数据（或直接返回索引表的核心字段，满足列表页需求）。

#### 5.3.2.2 冗余表 2：用户抢红包索引表（t_grabber_index）
- 用途：优化 “查询用户抢了哪些红包”；
- 分片键：`grabber_id`（抢红包用户 ID）；
- 分库算法：一致性哈希（grabber_id）；
- 查询流程：用户 B 查抢的红包 → 按`grabber_id=B`路由到索引表的对应库 → 拿到所有`red_packet_id` → 按`red_packet_id`路由到主库，查询明细的完整数据。

#### 5.3.2.3 冗余表的一致性保障
- 写入时机：发布红包时，先写主库的 “发布表 + 明细表”，再异步写 “发布索引表”（用消息队列，如 RabbitMQ，确保最终一致性）；抢红包时，先写主库的 “抢红包表 + 明细表”，再异步写 “抢红包索引表”；
- 容错机制：消息队列重试 3 次，失败则记录日志，定时任务补写（抢红包系统允许短暂的索引延迟，比如 1 秒内同步完成，用户无感知）。

#### 5.3.2.4 问题：冗余表不也要面临单独分片迁移的问题吗？
我们可以通过 “算法选择 + 工具自动化 + 统一节奏”，让这种成本远小于方案带来的收益：
1. 辅助表的分库算法选择「一致性哈希 + 虚拟节点」，扩容时迁移量仅 10%~20%（远低于普通哈希取模的 50%+）：
	- 例：发布索引表按`publisher_id`一致性哈希分 8 库，扩容到 16 库时，仅需迁移 “新增节点相邻区间” 的 12.5% 数据，而非 50%；
	- 工具支持：ShardingSphere-JDBC 内置一致性哈希实现，配置时仅需指定`virtual-nodes: 1024`，无需手动维护哈希环。
2. 辅助表 “只分库不分表”，进一步降低维护成本
	- 抢红包系统的辅助表有两个特点：
		- 发布索引表：一个用户发布的红包数量有限
		- 抢红包索引表：一个用户抢的红包数量也有限
	- 核心原则：辅助表尽量 “少分层”（只分库，或仅按时间轻度分表），避免 “分库 + 分表” 的多层维护复杂度。
3. 维护成本的增加远小于核心收益
	- 辅助表带来的维护成本（每年 1~2 次扩容，每次 1~2 小时），对比其带来的核心收益，是完全值得的
	- 非主维度查询效率提升 10~100 倍
	- 避免数据不一致导致的业务故障
	- 热点隔离方案可落地

### 5.3.3 热点隔离（解决热门红包问题）
热门红包（如 10 万元红包、明星发的红包）会导致主库中对应的分片被海量抢红包请求冲击（每秒几千甚至几万次读写），需要单独隔离：

#### 5.3.3.1 热点红包识别
规则：发布时标记 “热点红包”（如金额 > 1 万、发布者是 VIP 用户）；或实时监控分片负载（如某分片 QPS>5000，自动标记为热点）。

#### 5.3.3.2 热点隔离方案
- 方案 1：热点分片单独拆分 → 把热点红包的`red_packet_id`映射到专门的 “热点库”（比如 N+1、N+2 库），与普通库物理隔离，避免冲击普通业务；
- 方案 2：缓存承接热点 → 抢红包请求先经过 Redis（比如用 Redis 的 List 存储红包金额，LPOP 弹出抢红包），异步同步到数据库（明细表 + 抢红包表），Redis 抗高并发，数据库异步写入；
- 方案 3：限流削峰 → 对热门红包设置每秒抢红包次数上限（如每秒 1000 次），避免分片被压垮。

# 6 面试题：为什么系统设计没考虑分库分表
## 6.1 面试官的考察重点
例如面试官问新实例系统设计，为什么没做分库分表。
面试官核心是想考察你对系统架构设计的**合理性判断能力**和**业务发展的前瞻性思考**。面试官的考察重点：
1. 对分库分表适用场景的理解，是否清楚其使用前提。
2. 系统设计时的权衡思路，能否结合业务实际做决策。
3. 对业务增长的预判能力，是否考虑过后续架构扩容方案。

## 6.2 合适的回答逻辑
1. 明确初期不做分库分表的核心原因，结合业务规模说明。
    - 初期业务数据量小，单库单表完全能支撑并发和存储需求。
    - 分库分表会增加系统复杂度，提高开发和维护成本，不符合初期快速迭代的需求。
2. 优化措施与性能提升：展示你已经采取的优化措施，说明通过其他手段解决了性能问题。
	- 例如索引优化、查询优化、缓存策略、读写分离等
3. 补充后续的规划，体现前瞻性。
    - 已制定数据增长监控方案，当数据量或并发达到预设阈值时，会启动分库分表改造。
    - 提前调研了分库分表的方案，如垂直分表、水平分库等，确保后续能平滑过渡。
    - 技术选型方面，对主流的分库分表中间件进行了调研，包括 ShardingSphere-JDBC 和 MyCAT
    - 对于按时间范围分片，考虑数据归档策略。例如超过 3 年的历史数据迁移到低成本存储（如阿里云 OSS+ClickHouse），支持离线查询，主库仅保留近 3 年热数据

---
# 7 引用
[分库分表知乎](https://zhuanlan.zhihu.com/p/137368446)