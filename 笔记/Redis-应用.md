2025-05-01 11:23
Status: #idea
Tags: [[Redis]]


# 1 旁路缓存
我们也把 Redis 称为**旁路缓存**，也就是说，读取缓存、读取数据库和更新缓存的操作都需要在应用程序中来完成。
Redis 缓存的两种类型：**只读缓存和读写缓存**。只读缓存能加速读请求，而读写缓存可以同时加速读写请求。而且，读写缓存又有两种数据写回策略。

## 1.1 只读缓存
应用要读取数据的话，会先调用 Redis `GET` 接口，查询数据是否存在。
而所有的数据写请求，会直接发往后端的数据库，在数据库中增删改。对于删改的数据来说，如果 Redis 已经缓存了相应的数据，应用需要把这些缓存的数据删除，Redis 中就没有这些数据了。
当应用再次读取这些数据时，会发生缓存缺失，应用会把这些数据从数据库中读出来，并写到缓存中。这样一来，这些数据后续再被读取时，就可以直接从缓存中获取了，能起到加速访问的效果。

只读缓存直接在数据库中更新数据的好处是，所有最新的数据都在数据库中，而数据库是提供数据可靠性保障的，这些数据不会有丢失的风险。当我们需要缓存图片、短视频这些用户只读的数据时，就可以使用只读缓存这个类型了。

## 1.2 读写缓存
对于读写缓存来说，除了读请求会发送到缓存进行处理（直接在缓存中查询数据是否存在)，所有的写请求也会发送到缓存，在缓存中直接对数据进行增删改操作。

同步直写是指，写请求发给缓存的同时，也会发给后端数据库进行处理，等到缓存和数据库都写完数据，才给客户端返回。这样，即使缓存宕机或发生故障，最新的数据仍然保存在数据库中，这就提供了数据可靠性保证。
~~而异步写回策略，则是优先考虑了响应延迟。此时，所有写请求都先在缓存中处理。等到这些增改的数据要被从缓存中淘汰出来时，缓存将它们写回后端数据库。这样一来，处理这些数据的操作是在缓存中进行的，很快就能完成。只不过，如果发生了掉电，而它们还没有被写回数据库，就会有丢失的风险了。~~

读写缓存相对于只读模式，当数据被修改后的下一次查询不需要从数据库读。

# 2 缓存和数据库的一致性
## 2.1 只读缓存
### 2.1.1 先删除缓存，再更新数据库
假设线程 A 删除缓存值后，还没有来得及更新数据库，线程 B 就开始读取数据了，那么这个时候，线程 B 会发现缓存缺失，就只能去数据库读取：
- 线程 B 读取到了旧值；
- 线程 B 是在缓存缺失的情况下读取的数据库，所以，它还会把旧值写入缓存，这可能会导致其他线程从缓存中读到旧值。

**延迟双删：在线程 A 更新完数据库值以后，我们可以让它先 sleep 一小段时间，再进行一次缓存删除操作**。
之所以要加上 sleep 的这段时间，就是为了让线程 B 能够先从数据库读取数据，再把缺失的数据写入缓存，然后，线程 A 再进行删除。所以，线程 A sleep 的时间，就需要大于线程 B 读取数据再写入缓存的时间。
这个时间怎么确定呢？建议你在业务程序运行的时候，统计下线程读数据和写缓存的操作时间，以此为基础来进行估算。

严格来说，在极端情况下，延迟双删还是会存在数据不一致情况。比如:A线程删除了缓存，准备更新数据库之前，B线程进来没有缓存命中，去数据库查找到了旧值，但是B在准备更新缓存时被阻塞了；这时候A更新了数据库，sleep了，然后删除缓存，这时候B拿着旧数据再来更新缓存，就导致了数据不一致问题。

### 2.1.2 情况二：先更新数据库值，再删除缓存值
如果线程 A 更新了数据库中的值，但还没来得及删除缓存值，线程 B 就开始读取数据了，就会读到缓存的旧值。
不过删除缓存一般很快，所以不一致的窗口比较小。

#### 2.1.2.1 还有一个概率很低的缓存不一致问题
1. 请求A读缓存未命中
2. 请求A读数据库中的值20
3. 请求B将数据库更新为30
4. 请求B删除缓存
5. 请求A将缓存更新为20

缓存的写入远远快于数据库的写入，所以在实际中很难出现请求B已经更新了数据库并且清空了缓存，请求A才更新完缓存的情况。

### 2.1.3 两者对比
| 比较维度      | 先更新DB再删除缓存 | 延时双删             |
| --------- | ---------- | ---------------- |
| **实现复杂度** | 简单         | 较复杂（需要延时机制）      |
| **性能**    | 较好（1次删除）   | 稍差（2次删除+延时）      |
| **一致性保证** | 较好         | 更好（解决更多边界情况）     |
| **失败影响**  | 可能导致长期不一致  | 第一次失败同左，第二次失败影响小 |
| **适用场景**  | 一般业务场景     | 对一致性要求高的场景       |

## 2.2 读写缓存
### 2.2.1 先更新缓存，再更新数据库
如果更新缓存成功，但数据库更新失败，此时缓存中是最新值，数据库中是旧值，后续读请求会直接命中缓存，但得到的是最新值，短期对业务影响不大。
但是，一旦缓存过期或者满容后被淘汰，读请求就会从数据库中重新加载旧值到缓存中，之后的读请求会从缓存中得到旧值，对业务产生影响。

如果存在并发读写，也会产生不一致：
1. 先更新缓存，再更新数据库，写+读并发：线程A先更新缓存成功，之后线程B读取数据，此时线程B命中缓存，读取到最新值后返回，之后线程A更新数据库成功。这种场景下，虽然线程A还未更新完数据库，数据库会与缓存存在短暂不一致，但在这之前进来的读请求都能直接命中缓存，获取到最新值，所以对业务没影响。
2. 先更新缓存，再更新数据库，写+写并发：线程A和线程B同时更新同一条数据，**更新缓存的顺序是先A后B，但是更新数据库的顺序是先B后A**，这也会导致数据库和缓存的不一致。

### 2.2.2 先更新数据库，再更新缓存
如果更新数据库成功，但缓存更新失败，此时数据库中是最新值，但缓存中是旧值，后续的读请求会直接命中缓存，得到的是旧值。

如果存在并发读写，也会产生不一致：
1. 先更新数据库，再更新缓存，写+读并发：线程A先更新数据库，之后线程B读取数据，此时线程B会命中缓存，读取到旧值，之后线程A更新缓存成功，后续的读请求会命中缓存得到最新值。这种场景下，线程A未更新完缓存之前，在这期间的读请求会短暂读到旧值，对业务短暂影响。
2. 先更新数据库，再更新缓存，写+写并发：线程A和线程B同时更新同一条数据，更新数据库的顺序是先A后B，但更新缓存时顺序是先B后A，这会导致数据库和缓存的不一致。

### 2.2.3 总结
也就是说，在读写缓存模式下，写+读并发对业务的影响较小，而**写+写并发时，会造成数据库和缓存的不一致**。对于写请求，需要配合分布式锁使用。

读写缓存模式由于会同时更新数据库和缓存，优点是，缓存中一直会有数据，如果更新操作后会立即再次访问，可以直接命中缓存，能够降低读请求对于数据库的压力（没有了只读缓存的删除缓存导致缓存缺失和再加载的过程）。
缺点是，如果更新后的数据，之后很少再被访问到，会导致缓存中保留的不是最热的数据，缓存利用率不高（只读缓存中保留的都是热数据），所以读写缓存比较适合用于读写相当的业务场景。

# 3 Redis 分布式锁
## 3.1 基础概念
**分布式锁，是控制分布式系统之间同步访问共享资源的一种方式**。如果不同的系统或是同一个系统的不同主机之间共享了一个或一组资源，那么访问这些资源的时候，往往需要互斥来防止彼此干扰来保证一致性，在这种情况下，便需要使用到分布式锁。

## 3.2 必备要素
- 互斥性：在任意时刻，只有一个客户端持有锁。  
- 原子性：加锁、续期、释放锁都需要做到原子操作  
- 防死锁：如果客户端获得锁后自身出现异常，锁能够在一段时间后自动释放，资源不会被锁死。  
- 可续期：当执行耗时较大任务时，无法提前预估加锁时间，在任务执行过程中对锁做续期。  
- 防误解：加锁解锁必须为同一客户端，避免出现误解锁的情况。  
* 一致性：锁管理器在主从切换等操作后，锁需要在切换到新的master后保持原状态。

## 3.3 Redis分布式锁常见问题
### 3.3.1 死锁问题
基于SETNX命令实现最简单的分布式锁，多个client申请加锁时，如果A线程加锁成功，其他client的SETNX命令都会失败，必须等待A线程主动释放锁，但是如果A线程执行异常，没有主动释放锁，就会造成死锁，其他client将永远无法获取锁。  
解决方法：设置TTL，使用命令 SET key value PX milliseconds NX，保证原子性。

### 3.3.2 锁续期：业务执行时间没法确定
对于任务执行时间超出预期，或者任务执行时间难以评估的情况，不能确保一定在租期内结束任务并释放锁，当释放锁之前已经达到租期，该锁会被其他线程获取，使锁的互斥性失效。  
解决方法：使用watch dog机制，定期对锁续期

#### 3.3.2.1 锁续期失败怎么办？
看门狗续期失败（比如网络中断、Redis 节点故障），最终会导致锁因 TTL 到期被自动释放。可能会导致导致数据并发修改、脏写等问题。
下面是分层解决方案，从 “预防” 到 “兜底”，层层递进。

##### 3.3.2.1.1 第一层：优化续期机制，降低续期失败概率（主动预防）
首先要减少 “续期失败” 的发生，而不是只等失败后补救：
- **续期重试 + 退避策略**：看门狗单次续期失败（比如网络抖动），不要直接放弃，而是加入重试逻辑。
    示例：续期失败后，重试 3 次，每次间隔采用「指数退避」（100ms→200ms→400ms），避免频繁重试给 Redis / 网络加压；同时设置重试超时（比如总超时 1s），超过后判定续期失败（防止无限阻塞）。
- **Redis 高可用架构**：续期失败可能是 Redis 单点故障导致，所以生产环境必须用「主从 + 哨兵」或 Redis Cluster。

##### 3.3.2.1.2 第二层：设置 “兜底超时”，给临界区留足执行时间（被动兜底）
看门狗的核心是 “动态续期”，但不能完全替代 “初始超时时间”—— 必须设置一个「最大兜底超时时间」，且满足：`最大兜底超时时间 > 临界区代码的最长执行时间`。
如果你的临界区代码（比如订单支付、库存扣减）最长执行 20 秒，那么初始锁超时可以设为 30 秒，看门狗每 10 秒续期一次（续期后超时仍为 30 秒）。
关键：这个 “最长执行时间” 需要通过压测确定，不能拍脑袋（比如压测后发现 99.9% 的请求 20 秒内完成，就设 30 秒兜底）。

##### 3.3.2.1.3 第三层：临界区 “锁有效性校验”，主动放弃非法执行（核心兜底）
即使前面两层没拦住（比如网络中断超过 1 秒，续期重试失败，且兜底超时还没到），也要在临界区执行过程中，**定期校验 “自己是否还持有锁”**：
1. 临界区代码执行到关键节点（比如每 5 秒，或执行数据库操作前），主动向 Redis 发起请求：“校验当前锁的 value 是否还是我的标识？”；
2. 如果校验失败（锁已被释放或被其他线程持有），立即中断临界区执行，释放本地资源（比如关闭数据库连接），并返回失败（或触发补偿逻辑）。

##### 3.3.2.1.4 第四层：业务层面幂等 + 冲突检测，最终一致性兜底
如果前面的机制都失效（极端情况），必须靠业务逻辑兜底，避免数据不一致：
- 「幂等设计」：确保同一临界区操作重复执行，结果也一致。
    示例：扣库存时，用「订单号 + 商品 ID」作为唯一键，即使两个线程同时执行扣减，数据库也会通过唯一索引拒绝重复操作；支付场景用「支付流水号」做幂等，避免重复扣款。
- 「冲突检测」：用乐观锁 / 版本号机制，检测数据是否被其他线程修改。
    示例：更新数据时，带上版本号（`UPDATE stock SET num=num-1, version=version+1 WHERE id=1 AND version=5`）；如果版本号不匹配，说明已被其他线程修改，直接回滚当前操作，返回失败。
- 「补偿机制」如果已经出现数据不一致（比如库存超卖），通过定时任务或人工介入修正：
	- 定时任务对比 “订单数” 和 “库存扣减数”，发现超卖后，自动冻结异常订单，通知运营处理；
	- 关键业务（比如资金交易）可记录操作日志，定期对账，发现不一致后触发自动补偿（比如退款、补扣库存）。

##### 3.3.2.1.5 第五层：依赖成熟客户端，避免手写锁的坑
自己手写看门狗、续期重试、锁校验很容易出错，生产环境建议用成熟客户端（比如 Redisson），它已经内置了完善的容错机制：
- Redisson 的看门狗（`lockWatchdogTimeout`）：默认每 30 秒续期一次，续期失败时会自动重试；
- 支持「自动释放锁」：即使线程执行完临界区忘记释放锁，看门狗也会在最大超时后自动释放；
- 支持「分布式锁的可重入、公平锁」等特性，且兼容 Redis 高可用架构。

#### 3.3.2.2 锁一直被占用怎么办
如果当前获取该锁的线程一直在阻塞，守护线程一直给锁自动续期，会导致哪些问题呢？

##### 3.3.2.2.1 影响
获取锁的线程 “一直阻塞”，本质是**临界区代码无法正常结束**（比如死循环、调用第三方接口超时无重试、本地线程死锁）。此时看门狗持续续期，会导致锁被 “永久占用”—— 这是分布式锁的致命风险：
1. 锁泄漏（最直接）
	- 其他线程永远无法获取该锁，依赖该锁的业务流程（比如库存扣减、订单创建）全部阻塞，导致 “业务雪崩”
	- 极端情况：如果锁是 “非可重入锁”，甚至会导致持有锁的线程自己后续也无法重入，造成自我阻塞。
2. 业务数据不一致（间接风险）：如果阻塞的线程持有部分业务资源（比如数据库事务未提交），长期阻塞可能导致事务超时回滚，或其他依赖该事务的操作阻塞，引发数据一致性问题。

##### 3.3.2.2.2 第一层：放弃 “无限续期”，给锁设 “最大持有超时”（核心）
生产环境不建议用 Redisson 默认的 “无限续期模式”，而是**手动指定`leaseTime`（最大持有时间）+ 看门狗兜底**，结合成 “有限续期”。

##### 3.3.2.2.3 第二层：给临界区线程设 “执行超时 + 中断机制”（避免线程活锁）
针对 “线程活着但阻塞”（比如死循环、外部依赖超时），需要从应用层强制终止线程：
- 给临界区代码设超时：用`Future`或`CompletableFuture`包装临界区逻辑，设置超时时间（比如 25 秒，小于 leaseTime=30 秒）；

##### 3.3.2.2.4 第三层：监控告警 + 强制释放（兜底）
即使前面两层失效，也要通过监控快速发现并处理：
- 监控锁持有时间：通过 Prometheus+Grafana 监控 Redisson 锁的持有时间，设置阈值告警（比如锁持有超过 25 秒就告警）；
- 手动强制释放：告警后，运维可通过 Redis 命令（`DEL lockKey`）手动删除锁（注意：需先确认持有锁的线程确实异常，避免误删正常锁）；

##### 3.3.2.2.5 第四层：避免线程阻塞的根源（从源头减少问题）
- 禁止临界区写死循环：所有循环必须有退出条件（比如重试次数上限、超时时间）；
- 避免本地线程死锁：规范锁的获取顺序（比如按资源 ID 升序获取锁），避免应用内多线程竞争本地锁导致死锁；
- 临界区轻量化：分布式锁的临界区代码要尽量短（比如只做 “扣库存” 核心操作，不包含复杂计算、外部接口调用），减少阻塞概率。
- 外部依赖设超时：调用 HTTP 接口、数据库操作时，必须设超时（比如 HTTP 超时 5 秒，数据库查询超时 3 秒），避免因外部依赖卡死导致线程阻塞。

### 3.3.3 误解锁
如下图场景：  
1. Client 1 获取到锁。  
2. Client 1 开始任务，然后发生了 STW 的 GC，时间超过了锁的过期时间。  
3. Client 2 获取到锁，开始了任务。  
4. Client1 的 GC 结束后，会发起解锁请求，释放client2的锁。
![[67f3b7e0fd91110bce4326a0.png]]

解决方法：fencing token机制，每次上锁时写入uuid作为client 的唯一标识，对应“SET key value PX milliseconds NX”命令中的value值，**解锁时带着uuid解锁**。

### 3.3.4 锁释放和续期不具备原子性
在fencing token机制下，使用redis原生命令解锁和续期时，需要分两步：  
1. 查询当前锁是否存在（get命令，读操作）  
2. 如果client id匹配，删除锁（del命令，写操作），或者续期锁（set px nx命令，写操作）  

锁释放和续期不具备原子性，可能在两次操作中间，锁被其他线程占用。
解决方法：使用redis lua实现原子解锁（CAD）和续期（CASEX）

### 3.3.5 一致性：Redis主从架构数据同步复制问题
我们通常使用「Redis Cluster」或者「哨兵模式」这两种方式实现redis的高可用，而这两种方式都是基于「主从架构数据同步复制」实现的，而redis默认的主从复制是异步的。  
Redis所有的写操作都是先在Master上操作，然后同步更新到Slave上，Slave只能读不能写。

当集群发生崩溃，主从发生切换时，会破坏redis锁的安全性，如下场景：
1. 客户端 1 在主库上执行 SET 命令，加锁成功。  
2. 此时，主库异常宕机，SET 命令还未同步到从库上（主从复制是异步的）。 
3. 从库被提升为新主库，这个锁在新的主库上丢失了。

#### 3.3.5.1 WAIT命令能够为Redis实现强一致吗？
`WAIT numreplicas timeout `
- numreplicas：指定副本（slave）的数量。  
- timeout：超时时间，时间单位为毫秒；当设置为0 时，表示无限等待，即用不超时。  
WAIT命令作用：WAIT 命令阻塞当前客户端，直到所有先前的写入命令成功传输，并且由至少指定数量的副本（slave）确认。在主从、sentinel和Redis群集故障转移中， WAIT能够增强（仅仅是增强，但不是保证）数据的安全性。  
  
> [!info] 官方文档：https://redis.io/commands/wait  
> 然而，这只是尽力而为的尝试，因此仍然有可能丢失同步复制到多个副本的写入。  
  
所以WAIT 不能保证 Redis 的强一致性。

#### 3.3.5.2 业务层面要保证最终一致性
因为Redis锁没办法保证一致性，所以要靠业务层面兜底。参见 [[#4.3.2.1.4 第四层：业务层面幂等 + 冲突检测，最终一致性兜底]]。

### 3.3.6 总结
- 分布式锁是为了控制分布式系统之间同步访问共享资源的一种方式
- Redis分布式要实现以下要求：
	- 互斥性：在任意时刻，只有一个客户端持有锁。
		- 靠下面的其他特性保证的
	- 原子性：加锁、续期、释放锁都需要做到原子操作。
		- 用lua脚本实现
	- 防死锁：如果客户端获得锁后自身出现异常，锁能够在一段时间后自动释放，资源不会被锁死。
		- 需要给锁加上超时时间
	- 可续期：当执行耗时较大任务时，无法提前预估加锁时间，在任务执行过程中对锁做续期。  
		- 使用看门狗机制实现
		- 如何解决续期失败问题
			- 续期失败要重试（退避策略），应对网络波动
			- ~~设置兜底超时时间，要大于临界区代码的最长执行时间，通过压测获得~~
			- 临界区要定期或执行数据库操作前，校验自己是否还持有锁
			- 业务兜底：
				- 幂等：业务层面要实现幂等，确保重复执行结果一致；
				- 冲突检测：检测数据是否被其他人修改，例如乐观锁版本号；
				- 定期对账：发现不一致就冻结或补偿数据。
		- 如何解决线程阻塞长时间占有锁的问题：影响主要是锁泄露，导致业务雪崩
			- 锁要有最大超时时间，避免无限续期
			- 临界区代码需要有执行超时和中断机制
			- 临界区内，避免死循环、避免死锁、代码尽可能短、外部依赖（如RPC调用）设超时
			- 有兜底监控和强制释放机制
	- 防误解：加锁解锁必须为同一客户端，避免出现误解锁的情况。例如线程A阻塞，锁过期被线程B抢占，线程A恢复把B的锁解锁
		- 解锁、续期时，都需要带上锁的value值做校验
	* 一致性：锁管理器在主从切换等操作后，锁需要在切换到新的master后保持原状态。
		* 这个靠Redis分布式锁没办法保证，得靠业务做最终一致性兜底。
		* 或者改用RedLock、ZooKeeper

## 3.4 RedLock分析
### 3.4.1 基础知识
针对上面的问题，redis之父antirez设计了Redlock算法，Redlock的算法描述就放在Redis的官网上：  
- https://redis.io/topics/distlock  
在Redlock之前，很多人对于分布式锁的实现都是基于单个Redis节点的。而Redlock是基于多个Redis节点（都是Master）的一种实现。前面基于单Redis节点的算法是Redlock的基础。
注意：redLock会直接连接多个redis主节点，不是通过集群机制连接的。

优缺点：
- 优点：部分节点宕机，依然可以保证锁的可用性。降低了单实例Redis分布式锁主从切换导致不一致的问题概率。
- 缺点：实现复杂，并且依然不能完全解决分布式场景的资源互斥问题。  

#### 3.4.1.1 加解锁
加锁：
- 加锁：官方推荐至少 5 个redis实例，而且都是主库，它们之间没有任何关系，都是一个个孤立的实例  
* 流程如下：  
    1. 客户端先获取「当前时间戳T1」。  
    2. 客户端**依次**向这 5 个 Redis 实例发起加锁请求（SET命令），且每个请求会设置超时时间（毫秒级，要远小于锁的有效时间），如果某一个实例加锁失败（包括网络超时、锁被其它人持有等各种异常情况），就立即向下一个 Redis 实例申请加锁。  
	    - 之所以要依次，是因为并发节省的时间有限，但复杂度大幅提升：需要处理并发请求的异步回调、统一超时控制、锁开始时间校准，反而容易出错。所以 Redlock 的经典实现（包括 Redis 官方文档、Redisson 的 Redlock 实现）都选择「依次请求」—— 用微小的时间代价，换取锁的安全性和实现的简洁性。
    3. 如果客户端从 3 个（大多数）以上 Redis 实例加锁成功，则再次获取「当前时间戳T2」，如果 T2 - T1 ≤ 锁过期时间的 1/3~1/5，此时，认为客户端加锁成功，否则认为加锁失败。  
    4. 如果加锁成功，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。  
    5. 如果加锁失败，向全部节点发起释放锁请求。  

解锁：Redlock算法释放锁的过程比较简单：客户端向所有Redis节点发起释放锁的操作，不管这些节点当时在获取锁的时候成功与否。  
因为当对某一个redis节点加锁时，可能因为网络原因导致加锁“失败”。注意这个“失败”，指的是redis节点实际已经加锁成功了，但是返回的结果因为网络延迟并没有传到加锁的线程，被加锁线程丢弃了，加锁线程误以为没有成功，于是加锁线程去尝试下一个节点了。

#### 3.4.1.2 仍然存在的问题
RedLock仍然很难解决**主从切换问题**：  
- 假设加锁线程在5个实例中对其中3个加锁成功，获得了这把分布式锁，这个时候3个实例中有一个实例被重启了。重启后的实例丢失其中的锁信息，这个时候另一个加锁线程可以对这个实例加锁成功，此时两个线程同时持有分布式锁。锁的安全性被破坏。  
- 要想解决这个问题，得使用**延迟重启**保证锁的安全性。把崩溃节点进行延迟重启，超过崩溃前所有锁的TTL时间之后才加入Redlock节点组。

还有其他不可解问题：
- **时钟漂移问题**：RedLock算法数建立在了时间是可信的模型上的一种分布式锁，所以时间被破坏的情况下它无法实现锁的绝对安全；
	1. 客户端 1 获取节点 A、B、C 上的锁，但由于网络问题，无法访问 D 和 E  
	2. 节点 C 上的时钟向前跳跃，导致锁到期  
	3. 客户端 2 获取节点 C、D、E 上的锁，由于网络问题，无法访问 A 和 B  
	4. 客户端 1 和 2 现在都相信它们持有了锁（冲突）  
- **进程暂停问题**也很难解决，不过其他分布式锁也面临同一问题。
	1. 客户端 1 请求锁定节点 A、B、C、D、E  
	2. 客户端 1 的拿到锁后，进入 GC（时间比较久）  
	3. 所有 Redis 节点上的锁都过期了  
	4. 客户端 2 获取到了 A、B、C、D、E 上的锁  
	5. 客户端 1 GC 结束，认为成功获取锁  
	6. 客户端 2 也认为获取到了锁，发生冲突
- **缺乏分布式共识问题**：这也是RedLock不可靠的根因。
	- RedLock中的每台Redis，充当的仍旧只是存储锁数据的功能，每台Redis之间各自独立，单台Redis缺乏全局的信息，自然也不知道自己的锁数据是否是完整的。
	- 没有分布式共识机制，使得在各种分布式环境的典型场景下（结点故障、网络丢包、网络乱序），没有完整数据但参与决策，从而破坏数据一致性。
- RedLock算法实现、运维比较复杂，并且性能比较差；  

### 3.4.2 作者的论述
Martin Kleppmann 是英国剑桥大学的分布式系统的研究员，和 Redis 之父 Antirez 进行过关于 RedLock是否安全展开过激烈讨论。

Martin 表示，一个分布式系统，更像一个复杂的野兽，存在着你想不到的各种异常情况。这些异常场景主要包括三大块，这也是分布式系统会遇到的三座大山：NPC，下面对二位的讨论内容做大致描述，感兴趣的读者可以阅读原文：  
- Martin关于分布式锁的分析博客：https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html  
- 针对Martin的blog的讨论：https://news.ycombinator.com/item?id=11059738  
- Antirez对此的反驳：http://antirez.com/news/101  
- 针对antirez的blog的讨论：https://news.ycombinator.com/item?id=11065933

NPC问题：
- N：Network Delay，网络延迟
	- 例如某个请求网络延迟，会导致操作延时、后发先至等问题。
- P：Process Pause，进程暂停（GC）
- C：Clock Drift，时钟漂移

#### 3.4.2.1 C：Clock Drift，时钟漂移
Martin：  
1. 当多个 Redis 节点时钟发生问题时，也会导致 Redlock 锁失效。  
2. 客户端 1 获取节点 A、B、C 上的锁，但由于网络问题，无法访问 D 和 E  
3. 节点 C 上的时钟向前跳跃，导致锁到期  
4. 客户端 2 获取节点 C、D、E 上的锁，由于网络问题，无法访问 A 和 B  
5. 客户端 1 和 2 现在都相信它们持有了锁（冲突）  

上面这种情况之所以有可能发生，本质上是因为**Redlock的安全性(safety property)对系统的时钟有比较强的依赖**，一旦系统的时钟变得不准确，算法的安全性也就保证不了了。
Martin在这里其实是要指出分布式算法研究中的一些基础性问题，或者说一些常识问题，即**好的分布式算法应该基于异步模型(asynchronous model)，算法的安全性不应该依赖于任何记时假设(timing assumption)**。在异步模型中：进程可能pause任意长的时间，消息可能在网络中延迟任意长的时间，甚至丢失，系统时钟也可能以任意方式出错。
一个好的分布式算法，**这些因素不应该影响它的安全性(safety property)，只可能影响到它的活性(liveness property)**，也就是说，即使在非常极端的情况下（比如系统时钟严重错误），算法顶多是不能在有限的时间内给出结果而已，而不应该给出错误的结果。这样的算法在现实中是存在的，像比较著名的Paxos，或Raft。但显然按这个标准的话，Redlock的安全性级别是达不到的。

什么情况下会发生时钟变迁？  
- 人为修改了时钟  
- 从NTP服务收到了一个大的时钟更新事件导致时钟漂移  
- 闰秒（是指为保持协调世界时接近于世界时时刻，由国际计量局统一规定在年底或年中或者季末对协调世界时增加或减少1秒的调整，此时一分钟为59秒或者61秒，闰秒曾使许多大型系统崩溃）

Antirez：  
- Redlock 并不需要完全一致的时钟，只需要大体一致就可以了，允许有误差。  
	* 例如要计时 5s，但实际可能记了 4.5s，之后又记了 5.5s，有一定误差，但只要不超过误差范围锁失效时间即可，这种对于时钟的精度的要求并不是很高，而且这也符合现实环境。  
* 对于对方提到的时钟修改问题，Redis 作者反驳到：  
    * 手动修改时钟：不要这么做就好了。否则你直接修改 Raft 日志，那 Raft 也会无法工作…  
    * 时钟跳跃：通过恰当的运维，保证机器时钟不会大幅度跳跃（每次通过微小的调整来完成），实际上这是可以做到的

##### 3.4.2.1.1 linux系统时间和时钟漂移
为什么系统时钟会存在漂移呢？先简单说下系统时间，linux提供了两个系统时间：clock realtime和clock monotonic  
- clock realtime也就是xtime/wall time，这个时间是可以被用户改变的，被NTP改变。redis的判断超时使用的gettimeofday函数取的就是这个时间，redis的过期计算用的也是这个时间。参考https://blog.habets.se/2010/09/gettimeofday-should-never-be-used-to-measure-time.html  
- clock monotonic，直译过来是单调时间，不会被用户改变，但是会被NTP改变。  

最理想的情况是：所有系统的时钟都时时刻刻和NTP服务器保持同步，但这显然是不可能的。  
clock realtime可以被人为修改，在实现分布式锁时，不应该使用clock realtime。不过很可惜，redis使用的就是这个时间，Redis 5.0使用的还是clock realtime。Antirez说过后面会改成clock monotonic的。也就是说，人为修改redis服务器的时间，就能让redis出问题了。

#### 3.4.2.2 fencing token机制
Martin 提出一种被叫作 fencing token 的方案，保证分布式锁的正确性。流程如下：  
1. 客户端在获取锁时，锁服务可以提供一个递增的 token。  
2. 客户端拿着这个 token 去操作共享资源。  
3. 共享资源可以根据 token 拒绝后来者的请求。  
这样一来，无论 NPC 哪种异常情况发生，都可以保证分布式锁的安全性，因为它是建立在异步模型上的。而 Redlock 无法提供类似 fencing token 的方案，所以它无法保证安全性。

Antirez对于Martin提出的 fencing token 机制，也提出了质疑，主要分为 2 个问题：  
1. 第一，这个方案必须要求要操作的共享资源服务器有拒绝旧 token的能力。 
    - 例如，要操作 MySQL，从锁服务拿到一个递增数字的 token，然后客户端要带着这个 token 去改 MySQL 的某一行，这就需要利用 MySQL 的事物隔离性来做。  
    * 但如果操作的不是 MySQL 呢？例如向磁盘上写一个文件，或发起一个 HTTP 请求，那这个方案就无能为力了，这对要操作的资源服务器，提出了更高的要求。  
    * 也就是说，大部分要操作的资源服务器，都是没有这种互斥能力的。再者，既然资源服务器都有了互斥能力，那还要分布式锁干什么？所以，Antirez认为这个方案是站不住脚的。  
2. 退一步讲，即使 Redlock 没有提供 fencing token 的能力，但 Redlock 已经提供了随机值（就是前面讲的 UUID），利用这个随机值，也可以达到与 fencing token 同样的效果。

#### 3.4.2.3 结论
在Martin的这篇文章中，还有一个很有见地的观点，就是对锁的用途的区分。他把锁的用途分为两种：  
- 为了效率(efficiency)，协调各个客户端避免做重复的工作。即使锁偶尔失效了，只是可能把某些操作多做一遍而已，不会产生其它的不良后果。比如重复发送了一封同样的email。  
- 为了正确性(correctness)。在任何情况下都不允许锁失效的情况发生，因为一旦发生，就可能意味着数据不一致(inconsistency)，数据丢失，文件损坏，或者其它严重的问题。  

最后，Martin得出了如下的结论：  
- 如果是为了效率(efficiency)而使用分布式锁，允许锁的偶尔失效，那么使用单Redis节点的锁方案就足够了，简单而且效率高。Redlock则是个过重的实现(heavyweight)。  
- 如果是为了正确性(correctness)在很严肃的场合使用分布式锁，那么不要使用Redlock。它不是建立在异步模型上的一个足够强的算法，它对于系统模型的假设中包含很多危险的成分(对于timing)。那应该使用什么技术呢？Martin认为，应该考虑类似Zookeeper的方案，或者支持事务的数据库。  
* **Redlock 不伦不类**：它对于效率来讲，Redlock 比较重，没必要这么做，而对于正确性来说，Redlock 是不够安全的。

### 3.4.3 RedLock是否鸡肋
RedLock相比Redis分布式锁确实运维复杂、性能较差，又不如ZooKeeper分布式锁一致性强。但效率和正确性不是非此即彼的关系。
高估了RedLock的复杂度：
- 实现复杂度可被工具降低：Redisson 已内置 RedLock 实现（`RedissonRedLock`），开发时只需几行代码调用，无需手写多节点通信、重试、超时控制逻辑；
- 运维复杂度可通过自动化缓解：容器化（Docker+K8s）部署可快速搭建多独立节点，监控可通过 Prometheus+Grafana 统一采集节点状态，故障时自动重启节点；

分布式锁的选型，本质是平衡「**一致性强度、并发性能、运维成本**」三个维度，没有 “最优方案”，只有 “最适合场景”：

| 锁方案                            | 一致性强度     | 并发性能       | 运维成本                 | 核心适用场景                         |
| ------------------------------ | --------- | ---------- | -------------------- | ------------------------------ |
| 单节点 Redis 锁（Lua + 看门狗 + 主从半同步） | 高（工程落地级）  | 极高（万级 QPS） | 低（主从集群）              | 高并发核心业务（库存扣减、订单创建、缓存更新）        |
| RedLock（3 + 独立 Redis 节点）       | 极高（接近强一致） | 中（万级 QPS）  | 中（多独立节点）             | 高一致性 + 中低并发（转账、支付确认、幂等校验）      |
| ZooKeeper 锁（Curator 客户端）       | 强一致（理论级）  | 低（千级 QPS）  | 中高（集群部署 + Leader 选举） | 零容忍锁失效（分布式任务调度、主节点选举、核心配置更新）   |
| 数据库锁（唯一索引 / 行锁）                | 强一致（事务级）  | 极低（百级 QPS） | 低（复用现有数据库）           | 低并发 + 强事务关联（分布式事务二阶段提交、低频数据同步） |

### 3.4.4 总结
- RedLock 需要多台独立的 Redis 主库（推荐5台），降低了单实例Redis分布式锁主从切换导致不一致问题的概率。
	- 加锁：依次(并发实现复杂且收益小)向多个主库发送加锁请求，如果超时或失败就略过。加锁成功数过半，并且加锁总时长<=超时时间的⅓或1/5，则加锁成功；加锁失败向所有实例发送解锁请求
	- 解锁：向所有实例发送解锁请求
- RedLock的缺点
	- 实现和运维复杂
	- 难以彻底解决主从切换问题，得靠延迟重启的运维方式避免
	- RedLock 不能解决时钟漂移、进程阻塞问题
- RedLock的适用场景：
	- RedLock复杂的缺点可以被库和自动化运维解决
	- 它适用于高一致性 + 中低并发，并且能够投入资源在运维上的场景

## 3.5 BLock（byte lock）
### 3.5.1 使用方式
```Go  
// 初始化client  
func initClient(servicePSM, redisPSM string) error  
  
// 创建锁  
func NewLock(dest string) (RedLock, error)   
  
// 非阻塞加锁  
// ttl：ttl时间  
// ttl <= 0，BLock托管锁，只要锁没有被主动释放，BLock会帮助用户维护锁。  
// ttl > 0, 按照指定时间设定锁的租期  
// 返回true表示加锁成功  
func (redLock *RedLock) TryLock(ctx context.Context, ttl time.Duration) (bool, error)   
  
// 阻塞加锁  
// ttl：ttl时间（单位s）  
// ttl < 0，BLock托管锁，直到用户主动释放锁  
// ttl > 0, 按照指定时间设定锁的租期  
// timeout：大于0时生效，阻塞时间，timeout和round都大于0时，优先按照timeout规则阻塞  
// round：大于0时生效，阻塞加锁次数，timeout和round不可同时<=0  
// 返回true表示加锁成功  
func (redLock *RedLock) Lock(ctx context.Context, ttl time.Duration, timeout time.Duration, round int) (bool, error)  
  
// 锁续期  
func (redLock *RedLock) Renew(ctx context.Context, ttl time.Duration) (bool, error)  
  
// 解锁  
func (redLock *RedLock) UnLock(ctx context.Context) error  
  
// 优雅退出  
func GracefulUnlock(ctx context.Context)  
```

**加分项**：  
- 非阻塞锁和阻塞锁
- 锁的超时重试实现  
* 优雅退出

### 3.5.2 原理
#### 3.5.2.1 加锁
1. 创建锁，根据资源名称，生成redis key 和 redis value（uuid）  
2. setnx方式加锁（set key value ex ttl nx），根据返回结果不同，分两种情况，  
    - 返回false，表示加锁失败，如果是非阻塞方式加锁，则直接返回false，如果为阻塞方式加锁，执行第3步骤。  
    - 返回true，表示加锁成功，执行第4步。  
    - 返回超时，尝试再次加锁，如果加锁成功，执行第4步，加锁失败，获取redis中的key，value进行校验，如果uuid与预期一致，则表示加锁成功，否则加锁失败，进入是否为阻塞锁的判断。  
3. 随机等待0-5ms后，判断是否满足重试条件（阻塞时间）执行第2步。  
4. 如果是托管方式加锁成功，则将锁写入watch dog map，以便后续的锁托管工作，如果是非托管方式加锁，直接返回true，结束。

![[67f3bbb60cd4510bce432d7c.png]]

##### 3.5.2.1.1 超时重试
流程中包含了锁的重试机制，超时可能是由于网络原因，可能是由于redis集群自身的原因，因此并不能确定收到超时响应时是否加锁失败，如果redis写入加锁数据成功，但是客户端认为加锁失败，会导致在锁的整个租期内，任何客户端线程都没有获得锁，资源使用率降低。

细心的读者可能会注意到，超时后是尝试重新加锁，而不是校验是否加锁成功，先看下图：
![[67f3bc080cd1d10bce432e6c.png]]
当第一次 lock 请求时候，客户端超时。再尝试 get 请求判断锁是否可以重入的时候，发现锁不存在。在第二次发起重试 lock 请求的时候，第一次的 lock 请求已到达且执行成功，则第二次加锁失败。会存在一个 lease time 内没有任何线程拿到这把锁 (实际server端又已经存在) 的问题。
触发的概率非常低，但是一定会遇到。所以超时重试时，先尝试加锁，加锁失败再验证是否加锁成功。

#### 3.5.2.2 Watch Dog
针对有些场景用户无法确定锁的TTL，BLock提供了看门狗机制，帮助用户维护锁，用户不需要设置锁的TTL，直到用户手动释放锁为止。看门狗机制流程如下：
1. 服务启动时，初始化Cron Job，定期维护托管锁。  
2. 判断isRunning的值，true表示前一轮任务还在执行中，当前任务直接返回，false表示前一轮任务已经结束，当前任务可正常执行。  
3. 根据存放托管锁的lock map，创建快照，这样做的目的是提高效率，不影响主流程中加锁，解锁过程中对lock map的写操作，同时isRunning置为true。  
4. 依次对持有的锁进行续期。  
5. 通过CasEx指令（原子操作），重置锁的ttl为30s。可能返回三种结果：  
	1. 锁已经被其他进程占有，跳至第6步；  
	2. 锁已经被释放，跳至第6步；  
	3. ttl重置成功。  
6. 判断lock map中是否包含该锁，如果不包含，说明该锁已经被释放，返回。如果包含该锁，抛出error，返回。  
7. 当所有的托管锁都被执行完，将isRunning置为false。  
8. 结束。
![[67f3d55c0f78510bce4334f7.png]]

#### 3.5.2.3 锁延期
通过CasEx重置锁的租期。重置锁的ttl。可能返回三种结果：  
1. 锁已经被其他进程占有，重置失败，返回error。  
2. 锁已经被释放，重置失败，返回error。  
3. ttl重置成功。

#### 3.5.2.4 解锁
通过Cad删除锁。可能返回三种结果：（表述意义，不要返回值）  
1. 锁被其他进程持有，删除失败，返回error。  
2. 锁已经被释放，删除失败，返回error。  
3. 删除成功，同时删除watchdog和LongLeaseMap中的记录。
![[67f3d5f1f8d9d10bce4337fb.png]]

#### 3.5.2.5 优雅退出 
在有些分布式场景下，由于框架或服务自身原因，当程序关闭时有些锁没有得到正确释放，导致资源无法被其他正在运行的客户端获取，必须要等待资源的上锁时间到达预设的TTL时间后，其他客户端才有可能获取到该资源。
**为了提高资源利用率，BLock提供了优雅退出的功能，在用户监听到关闭信号时，帮助释放正在持有的长租期锁（TTL > 30s）**。  
需要注意的是，对于安全性要求较高的场景需要慎重使用优雅退出功能，因为优雅退出可能导致用户线程正在运行，而锁被提前释放，其他线程获取到该锁，破坏了锁的互斥性。  
  
**BLock中使用map来维护持有的长租期锁**，加锁成功后把锁put到map中，解锁成功后将锁从map中remove。
为了防止解锁失败的长租期锁数据在map中积攒过多，应对map中已经超时的长租期锁进行定期的删除。
对于TTL时间比较短的锁，比如10s的锁，TTL小于实例的重启间隔，所以没有必要在关机的时候做释放，优雅退出只针对TTL时间大于30s的锁。

##### 3.5.2.5.1 定期删除失效锁
1. BLock初始化后会启动一个定时任务，定期对长租期锁检查  
2. 每次检查开始，会先校验标识，RunningFlag，如果RunningFlag等于true，标识前一轮的校验任务还没有运行完，此轮任务暂不执行，如果RunningFlag等于false，执行第三步  
3. 将RunningFlag置为true，标识此轮校验任务开始，同时生成一份长租期锁的map的快照，生成当前时间点CurrentTime  
4. 对快照map中的锁元素依次进行校验  
	1. 如果锁的租期小于CurrentTime，表示该锁已经过期，检查原map中是否包含该锁，如果包含，则从原map中删除该锁，如果不包含，继续校验后面的锁元素  
	2. 如果锁的租期大于CurrentTime，表示该锁为有效锁，不做操作，继续校验后面的锁元素  
5. 所有锁元素校验完毕后，将RunningFlag置为false，表示此轮校验任务运行结束
![[67f3d7e8f5a8d10bce433f54.png]]

##### 3.5.2.5.2 优雅退出
1. 当调用优雅退出接口时，先将RunningFlag置为true，阻塞长租期锁的定时检查任务。  
2. 对长租期锁的map和watch dog map中的锁依次进行释放。

##### 3.5.2.5.3 优雅退出样例
**基于kiteX框架**：
```Go  
func (s *server) Run() (err error) {  
  
   // ......  
  
   if err = s.waitSignal(errCh); err != nil {  
  
      if s.opt.Logger != nil {  
  
         s.opt.Logger.Errorf( "KITEX: received error and exit: %s" , err.Error())  
  
      }  
  
   }  
  
   for i := range onShutdown {  
  
      onShutdown[i]()  
  
   }  
  
   // stop server after user hooks  
  
  if e := s.Stop(); e != nil && err == nil {  
  
      err = e  
  
      if s.opt.Logger != nil {  
  
         s.opt.Logger.Errorf( "KITEX: stop server error: %s" , e.Error())  
  
      }  
  
   }  
  
   return  
  
}  
  
func (s *server) waitSignal(errCh chan error) error {  
  
   signals := make(chan os.Signal, 1)  
  
   signal.Notify(signals, syscall.SIGINT, syscall.SIGHUP, syscall.SIGTERM)  
  
  
  
   // ......  
  
}  
```
可以看到，waitSignal方法中监听了syscall.SIGINT, syscall.SIGHUP, syscall.SIGTERM三个新号，收到信号后，run方法结束。
所以，在main函数的最后增加优雅退出的逻辑即可:
```Go  
func main() {  
  
   // ......  
  
   err = svr.Run()  
  
  
  
   if err != nil {  
  
      log.Println(err.Error())  
  
   }  
  
     
  
   // 调用BLock优雅退出  
  
   block.GracefulUnlock(ctx)  
  
}  
```

**基于Hertz框架**：
注册钩子函数:  
```Go  
func main() {  
  
    hertz.Init()  
  
      
  
    // 退出前最长等待时间，如果不指定默认是5s  
  
    r := app.Default(config.WithExitWaitTime(30*time.Second))   
  
  
  
    r.OnShutdown = append(r.OnShutdown, func(ctx context.Context) {  
  
        logs.CtxInfo(ctx, "Stop websockets gracefully")  
  
        // 调用BLock优雅退出  
  
        block.GracefulUnlock(ctx)  
  
        <-ctx.Done()  
  
        logs.CtxInfo(ctx, "App shutdown!")  
  
    })  
  
  
  
    r.Spin()  
  
}  
```

# 4 事务
## 4.1 Redis 如何实现事务？
事务的执行过程包含三个步骤，Redis 提供了 `MULTI`、`EXEC` 两个命令来完成这三个步骤。下面我们来分析下。
1. 第一步，客户端要使用一个命令显式地表示一个事务的开启。在 Redis 中，这个命令就是 `MULTI`。
2. 第二步，客户端把事务中本身要执行的具体操作（例如增删改数据）发送给服务器端。这些操作就是 Redis 本身提供的数据读写命令，例如 `GET`、`SET` 等。不过，这些命令虽然被客户端发送到了服务器端，但 Redis 实例只是把这些命令暂存到一个命令队列中，并不会立即执行。
3. 第三步，客户端向服务器端发送提交事务的命令，让数据库实际执行第二步中发送的具体操作。Redis 提供的 `EXEC` 命令就是执行事务提交的。当服务器端收到 `EXEC` 命令后，才会实际执行命令队列中的所有命令。

下面的代码就显示了使用 `MULTI` 和 `EXEC` 执行一个事务的过程：
```bash
#开启事务
127.0.0.1:6379> MULTI
OK
#将a:stock减1，
127.0.0.1:6379> DECR a:stock
QUEUED
#将b:stock减1
127.0.0.1:6379> DECR b:stock
QUEUED
#实际执行事务
127.0.0.1:6379> EXEC
1) (integer) 4
2) (integer) 9
```
我们假设 a:stock、b:stock 两个键的初始值是 5 和 10。

## 4.2 Redis 的事务机制能保证哪些属性？
### 4.2.1 原子性
如果事务正常执行，没有发生任何错误，那么，`MULTI` 和 `EXEC` 配合使用，就可以保证多个操作都完成。但是，如果事务执行发生错误了，原子性还能保证吗？我们需要分三种情况来看。

#### 4.2.1.1 操作命令不存在
**第一种情况是，在执行 `EXEC` 命令前，客户端发送的操作命令本身就有错误**（比如语法错误，使用了不存在的命令），在命令入队时就被 Redis 实例判断出来了。
对于这种情况，在命令入队时，Redis 就会报错并且记录下这个错误。此时，我们还能继续提交命令操作。等到执行了 `EXEC` 命令之后，Redis 就会拒绝执行所有提交的命令操作，返回事务失败的结果。这样一来，事务中的所有命令都不会再被执行了，保证了原子性。

#### 4.2.1.2 命令和数据类型不匹配
我们再来看第二种情况。和第一种情况不同的是，**事务操作入队时，命令和操作的数据类型不匹配，但 Redis 实例没有检查出错误**。但是，在执行完 `EXEC` 命令以后，Redis 实际执行这些事务操作时，就会报错。不过，需要注意的是，虽然 Redis 会对错误命令报错，但还是会把正确的命令执行完。在这种情况下，事务的原子性就无法得到保证了。
```bash
#开启事务
127.0.0.1:6379> MULTI
OK
#发送事务中的第一个操作，LPOP命令操作的数据类型不匹配，此时并不报错
127.0.0.1:6379> LPOP a:stock
QUEUED
#发送事务中的第二个操作
127.0.0.1:6379> DECR b:stock
QUEUED
#实际执行事务，事务第一个操作执行报错
127.0.0.1:6379> EXEC
1) (error) WRONGTYPE Operation against a key holding the wrong kind of value
2) (integer) 8
```

Redis 中并没有提供回滚机制。虽然 Redis 提供了 `DISCARD` 命令，但是，这个命令只能用来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果。我们来看下下面的代码。
```bash
#读取a:stock的值4
127.0.0.1:6379> GET a:stock
"4"
#开启事务
127.0.0.1:6379> MULTI 
OK
#发送事务的第一个操作，对a:stock减1
127.0.0.1:6379> DECR a:stock
QUEUED
#执行DISCARD命令，主动放弃事务
127.0.0.1:6379> DISCARD
OK
#再次读取a:stock的值，值没有被修改
127.0.0.1:6379> GET a:stock
"4"
```

#### 4.2.1.3 Redis实例故障
我们再来看下第三种情况：在执行事务的 `EXEC` 命令时，Redis 实例发生了故障，导致事务执行失败。在这种情况下，如果 Redis 开启了 AOF 日志，那么，只会有部分的事务操作被记录到 AOF 日志中。我们需要使用 redis-check-aof 工具检查 AOF 日志文件，这个工具可以把未完成的事务操作从 AOF 文件中去除。这样一来，我们使用 AOF 恢复实例后，事务操作不会再被执行，从而保证了原子性。当然，如果 AOF 日志并没有开启，那么实例重启后，数据也都没法恢复了，此时，也就谈不上原子性了。

### 4.2.2 一致性
我们按照命令出错和实例故障的发生时机，分成三种情况来看。
- 情况一：命令入队时就报错。在这种情况下，事务本身就会被放弃执行，所以可以保证数据库的一致性。
- 情况二：命令入队时没报错，实际执行时报错。在这种情况下，有错误的命令不会被执行，正确的命令可以正常执行，也不会改变数据库的一致性。ACID中的一致性，从两方面来描述：从数据库层面，事物执行前后能够保证数据符合你设置的约束（如外键约束，唯一性约束，check约束等）。从应用层面，若破坏了应用层的约束，应用层利用事务回滚保证了我们的约束不被破坏，事务提供了一致性保证。这里是保证了数据的一致性，但有可能没有保证业务上的一致性。
- 情况三：EXEC 命令执行时实例发生故障。在这种情况下，实例故障后会进行重启，这就和数据恢复的方式有关了，我们要根据实例是否开启了 RDB 或 AOF 来分情况讨论下。
    - 如果我们没有开启 RDB 或 AOF，那么，实例故障重启后，数据都没有了，数据库是一致的。
    - 如果我们使用了 RDB 快照，因为 **RDB 快照不会在事务执行时执行**，所以，事务命令操作的结果不会被保存到 RDB 快照中，使用 RDB 快照进行恢复时，数据库里的数据也是一致的。
    - 如果我们使用了 AOF 日志，而事务操作还没有被记录到 AOF 日志时，实例就发生了故障，那么，使用 AOF 日志恢复的数据库数据是一致的。如果只有部分操作被记录到了 AOF 日志，我们可以使用 redis-check-aof 清除事务中已经完成的操作，数据库恢复后也是一致的。

所以，总结来说，在命令执行错误或 Redis 发生故障的情况下，Redis 事务机制对一致性属性是有保证的。

### 4.2.3 隔离性——WATCH, Pipeline
事务执行又可以分成命令入队（`EXEC` 命令执行前）和命令实际执行（`EXEC` 命令执行后）两个阶段，所以，我们就针对这两个阶段，分成两种情况来分析：
1. 并发操作在 `EXEC` 命令前执行，此时，隔离性的保证要使用 `WATCH` 机制来实现，否则隔离性无法保证；
2. 并发操作在 `EXEC` 命令后执行，此时，隔离性可以保证。

我们先来看第一种情况。一个事务的 `EXEC` 命令还没有执行时，事务的命令操作是暂存在命令队列中的。此时，如果有其它的并发操作，我们就需要看事务是否使用了 `WATCH` 机制。
`WATCH` 机制的作用是，在事务执行前，监控一个或多个键的值变化情况，当事务调用 `EXEC` 命令执行时，`WATCH` 机制会先检查监控的键是否被其它客户端修改了。如果修改了，就放弃事务执行，避免事务的隔离性被破坏。然后，客户端可以再次执行事务，此时，如果没有并发修改事务数据的操作了，事务就能正常执行，隔离性也得到了保证。

在使用事务时，建议配合 Pipeline 使用。
1. 如果不使用 Pipeline，客户端是先发一个 `MULTI` 命令到服务端，客户端收到 OK，然后客户端再发送一个个操作命令，客户端依次收到 `QUEUED`，最后客户端发送 `EXEC` 执行整个事务（文章例子就是这样演示的），这样消息每次都是一来一回，效率比较低，而且在这多次操作之间，别的客户端可能就把原本准备修改的值给修改了，所以无法保证隔离性。
2. 而使用 Pipeline 是一次性把所有命令打包好全部发送到服务端，服务端全部处理完成后返回。这么做好的好处，一是减少了来回网络 IO 次数，提高操作性能。二是一次性发送所有命令到服务端，服务端在处理过程中，是不会被别的请求打断的（Redis单线程特性，此时别的请求进不来），这本身就保证了隔离性。我们平时使用的 Redis SDK 在使用开启事务时，一般都会默认开启 Pipeline 的，可以留意观察一下。

没有配合 Pipeline 使用，如果想要保证隔离性，需要使用 `WATCH` 命令保证。但如果使用了 Pipeline 一次发送所有命令到服务端，那么就不需要使用 `WATCH` 了，因为服务端本身就保证了隔离性。
那 `WATCH` 还有没有使用的必要？答案是有的。对于一个资源操作为读取、修改、写回这种场景，如果需要保证事物的隔离性，此时就需要用到 `WATCH` 了。

### 4.2.4 持久性
因为 Redis 是内存数据库，所以，数据是否持久化保存完全取决于 Redis 的持久化配置模式。
如果 Redis 没有使用 RDB 或 AOF，那么事务的持久化属性肯定得不到保证。
如果 Redis 使用了 RDB 模式，那么，在一个事务执行后，而下一次的 RDB 快照还未执行前，如果发生了实例宕机，这种情况下，事务修改的数据也是不能保证持久化的。
如果 Redis 采用了 AOF 模式，因为 AOF 模式的三种配置选项 no、everysec 和 always 都会存在数据丢失的情况，所以，事务的持久性属性也还是得不到保证。
所以，不管 Redis 采用什么持久化模式，事务的持久性属性是得不到保证的。

## 4.3 总结
Redis 的事务机制可以保证一致性和隔离性，但是无法保证原子性、持久性。

# 5 引用