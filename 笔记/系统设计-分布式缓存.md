2025-12-02 20:23
Status: #idea
Tags:

# 1 分布式缓存设计
## 1.1 简介
面试官问我，设计一个高可用的分布式缓存系统，支持多机房部署和动态扩容。需要保证数据的一致性和高可用，并且要考虑到多机器的路由系统要怎么做。

先明确核心设计原则：**分片存储（支撑扩容）+ 多副本（支撑高可用）+ 跨机房部署（容灾）+ 客户端路由（高性能）**，整体架构：
1. 客户端路由层（槽位映射+就近访问）
2. 集群管理层（节点注册/心跳/槽位分配）
3. 数据层（分片存储+主从副本+跨机房同步）

## 1.2 核心模块详细设计
### 1.2.1 路由系统设计
路由系统的核心目标：**快速定位数据所在节点 + 支持动态扩容 + 容错**，分三部分设计：

#### 1.2.1.1 分片算法选型：虚拟槽分区（参考 Redis Cluster）
原理：将整个缓存空间划分为固定数量的 “槽位”（如 16384 个），每个槽位对应一段数据（通过哈希函数 `CRC16(key) % 槽位总数` 映射）；集群中每个节点负责一部分连续槽位，槽位与节点的映射关系是路由的核心。

#### 1.2.1.2 路由实现模式：客户端路由
- 架构：客户端直接缓存 “槽位→节点地址” 映射表，无需中间代理（如 Twemproxy/Codis），减少网络开销。
- 核心流程：
    1. 客户端初始化时，从集群配置中心（如 Consul）拉取最新的槽位 - 节点映射表，缓存到本地；
    2. 集群节点变动时，从集群配置中心监听自动更新本地缓存的映射表
    3. 重定向机制（兜底）：槽位从旧节点迁移到新节点，迁移期间槽位处于 “迁移中” 状态：
	    - 客户端请求该槽位时，旧节点会返回 “迁移中” 响应，并携带新节点地址；
	    - Go 客户端收到响应后，自动重试到新节点，同时更新本地映射表，保证路由不中断。

### 1.2.2 多机房部署设计（容灾 + 低延迟）
多机房部署的核心是 “**跨机房副本 + 就近访问 + 脑裂防护**”。

#### 1.2.2.1 副本分布策略
- 3 副本部署：每个槽位的主副本（写节点）和 2 个从副本（读节点）分布在不同机房，例如：
    - 机房 A：主副本（处理写请求）；
    - 机房 B：从副本 1（同步主数据，处理机房 B 读请求 + 容灾备份）；
    - 机房 C：从副本 2（同步主数据，处理机房 C 读请求 + 容灾备份）；
- 原则：避免单机房故障导致整个分片不可用，同时保证每个机房有读节点，降低跨机房读延迟。

#### 1.2.2.2 数据同步策略
- 写流程：客户端→机房 A 主节点（写入成功）→异步同步到机房 B/C 从节点（追求性能）；
- 一致性增强：关键场景下启用 “半同步复制”（主节点等待至少 1 个从节点同步完成后，再向客户端返回成功），避免主节点宕机导致数据丢失；
- 跨机房网络优化：
	- 同步数据时采用批量压缩（如 gzip），减少网络带宽占用；
	- 监控跨机房网络延迟，若延迟过高，自动切换同步模式为 “定时批量同步”。

#### 1.2.2.3 脑裂防护（核心容灾点）
- 问题：多机房网络分区时，不同机房的节点可能各自选举主节点，导致数据不一致；
- 解决方案：基于 Raft 协议的 “Quorum 机制”（选举和写操作需超过半数副本同意）：
    - 3 副本场景下，需至少 2 个副本在线且同意，才能选举主节点或执行写操作；
    - 若机房 A 与 B/C 断开网络，机房 A 的主节点因无法获得半数副本同意，自动降级为从节点；机房 B/C 的从节点选举新主，保证集群一致性。

### 1.2.3 动态扩容设计（无感知 + 平滑迁移）
动态扩容的核心是 “**槽位迁移而非数据重哈希**”，流程如下：

#### 1.2.3.1 扩容触发条件
- 自动触发：监控节点内存使用率（如超过 70%）、CPU 使用率（如超过 80%）；
- 手动触发：通过集群管理 API（Go 实现 HTTP 接口）手动添加节点。

#### 1.2.3.2 扩容核心流程（无感知）
1. **新增节点**：新节点加入集群，向 Consul 注册节点信息（IP、端口、机房、负载等）；
2. **槽位分配**：集群管理模块（基于 Consul 的分布式锁避免并发冲突））从现有节点中拆分部分槽位（如从每个旧节点拆分 1/4 槽位），分配给新节点；
3. **数据迁移**：
    - 源节点将槽位数据批量迁移到目标节点（新节点），每个槽位分批次（如每批 1000 条）迁移，避免阻塞；
    - 源节点（旧节点）将该槽位的请求通过重定向机制转发到新节点；
    - 目标节点接收数据并写入，完成后向源节点发送 “迁移完成” 确认；
4. **路由更新**：集群管理模块将新的槽位 - 节点映射同步到 Consul，客户端通过监听从而自动更新本地缓存的映射表。

#### 1.2.3.3 缩容流程
与扩容反向：将待下线节点的槽位迁移到其他节点，迁移完成后，该节点从 Etcd 注销，客户端自动停止路由请求，实现无感知下线。

### 1.2.4 数据一致性保障
分布式缓存的一致性需 “**结合业务场景权衡**”，优先保证 “最终一致性”，关键场景增强为 “强一致性”：

##### 1.2.4.1.1 写一致性
- 写请求仅路由到主节点，主节点写入成功后，异步同步到从节点（默认最终一致性）；
- 强一致性需求（如支付缓存）：启用半同步复制，主节点等待至少 1 个从节点同步完成（复制偏移量一致），再返回客户端成功，牺牲部分性能换取一致性。

##### 1.2.4.1.2 读一致性
- 普通读：路由到同机房从节点（最终一致性，低延迟）；
- 强一致读：客户端携带 “一致性标记”，路由到主节点（如用户刚更新数据后，需立即读取最新值）；
- 避免脏读：从节点同步时记录 “复制偏移量”，客户端读从节点时，若从节点偏移量低于主节点最新偏移量，自动重试或路由到主节点。

##### 1.2.4.1.3 故障恢复一致性
- 主节点故障时，通过 Raft 协议选举 “复制偏移量最大” 的从节点作为新主，保证新主数据最新；
- 故障节点恢复后，自动作为从节点同步新主的数据，同步完成后再加入集群提供服务，避免数据不一致。

### 1.2.5 高可用设计（故障自动恢复）
高可用的核心是 “**故障检测 + 自动切换**”，避免单点故障：

#### 1.2.5.1 节点故障检测
- 心跳机制：集群内节点每 1 秒向 Consul 发送心跳（携带节点状态、负载、复制偏移量）；
- 故障判定：Consul 监控节点心跳，若超过 5 秒未收到心跳，标记为 “故障节点”，并触发集群重新平衡。

#### 1.2.5.2 主节点故障恢复
- 触发条件：主节点故障（心跳超时）或网络分区导致主节点失联；
- 选举流程：Raft 协议触发从节点选举，选择 “复制偏移量最大 + 在线” 的从节点作为新主；
- 切换流程：选举完成后，更新槽位 - 主节点映射，客户端自动路由到新主，整个过程耗时≤3 秒（可通过调整 Raft 参数优化）。

## 1.3 总结


---
# 2 引用