2025-04-12 15:27
Status: #idea
Tags: [[分布式]] [[分布式-基础理论]]


# 1 什么是共识
分布式系统中充满了各种潜在的错误场景，**网络数据包可能丢失、顺序紊乱、重复发送或者延迟，节点还可能宕机**。“在充满不确定性的环境中，就某个决策达成共识”是软件工程领域最具挑战性的问题之一。

在汉语中，“共识”和“一致”意思相似，但在计算机领域，它们具有截然不同的含义。
- **共识**（Consensus）：指所有节点就某项操作（如选主、原子事务提交、日志复制、分布式锁管理等）达成一致的实现过程。
- **一致性**（Consistency）：描述多个节点的数据是否保持一致，关注数据最终达到稳定状态的结果。

在分布式系统中，节点故障是不可避免的，但部分节点故障不应该影响系统整体状态。通过增加节点数量，依据“少数服从多数”原则，只要多数节点（至少 N/2+1N）达成一致，其状态即可代表整个系统。这种**依赖多数节点实现容错的机制称为 Quorum 机制**。
> [!note] Quorum 机制
> - 3 节点集群：Quorum 为 2，允许 1 个节点故障。
> - 4 节点集群：Quorum 为 ⌈4/2⌉+1=3⌈4/2⌉+1=3，允许 1 个节点故障。
> - 5 节点集群：Quorum 为 ⌈5/2⌉+1=3⌈5/2⌉+1=3，允许 2 个节点故障。
> 
> 你注意到了吗？3 节点和 4 节点集群的故障容忍能力一样。因此，通常情况下，针对容错的分布式系统无需使用 4 个节点。

基于 Quorum 的机制，通过“少数服从多数”协商机制达成一致的决策，从而对外表现为一致的运行结果。这一过程被称为节点间的“协商共识”。一旦解决共识问题，便可提供一套屏蔽内部复杂性的抽象机制，为应用层提供一致性保证，满足多种需求。
- **主节点选举**：在主从复制数据库中，所有节点需要就“谁来当主节点”达成一致。如果由于网络问题导致节点间无法通信，很容易引发争议。若争议未解决，可能会出现多个节点同时认为自己是主节点的情况，这就是分布式系统中最棘手的问题之一 —— “脑裂”。
- **原子事务提交**：对于支持跨节点或跨分区事务的数据库，可能会发生部分节点事务成功、部分节点事务失败的情况。为维护事务的原子性（即 ACID 特性），所有节点必须就事务的最终结果达成一致。
- **分布式锁管理**：当多个请求尝试访问共享资源时，共识机制可确保所有节点一致认定“谁成功获取了锁”。即使发生网络故障或节点异常，也能避免锁争议，从而防止并发冲突或数据不一致。
- **日志复制**：日志复制指将主节点的操作日志同步到从节点。在这一过程中，所有节点必须确保日志条目的顺序一致，即日志条目必须以相同顺序写入。


> [!note] 总结
> 分布式系统的共识，就是 Quorum 节点（大多数节点，保证了容错性）就某项操作达成一致的实现过程。例如选主（谁当上了主）、分布式锁管理（谁拿到了锁）。
> 
> 分布式系统中充满了各种潜在的错误场景：消息延迟、丢失、乱序、重复，节点宕机，网络分区等。因此达成共识是一件很有挑战性的事情。

# 2 日志与复制状态机
如果统计分布式系统有多少块基石，“日志”一定是其中之一。
这里“日志”并不是常见的通过 log4j 或 syslog 输出的文本。而是 MySQL 中的 binlog（Binary Log）、MongoDB 中的 Oplog（Operations Log）、Redis 中的 AOF（Append Only File）、PostgreSQL 中的 WAL（Write-Ahead Log）...。它们虽然名称不同，但共同特点是**只能追加、完全有序的记录序列**。

有序的日志记录了“何时发生了什么”，这一点可以通过以下两种数据复制模型来理解。
- **主备模型**（Primary-backup）：又称“状态转移”模型，主节点（Master）负责执行如“+1”、“-2”的操作，将操作结果（如“1”、“3”、“6”）记录到日志中，备节点（Slave）根据日志直接同步结果。
- **复制状态机模型**（State-Machine Replication）：又称“操作转移”模型，日志记录的不是最终结果，而是具体的操作指令，如“+1”、“-2”。指令按照顺序被依次复制到各个节点（Peer）。如果每个节点按顺序执行这些指令，各个节点最终将达到一致的状态。
无论哪一种模型，它们都揭示了：“**顺序是节点之间保持一致性的关键因素**”。如果打乱了操作的顺序，就会得到不同的运算结果。

**共识算法通过消息，将日志广播至所有节点，它们就日志什么位置，记录什么达成共识。换句话说，所有的节点中，都有着相同顺序的日志序列**。
节点内的进程（图中的 State Machine）按顺序执行日志序列，操作具有全局顺序。因此，所有节点最终将达到一致的状态。多个这样的进程结合有序日志，就构成了 Apache Kafka、Zookeeper、etcd、CockroachDB 等分布式系统中的关键组件。


> [!note] 总结
> 日志是只能追加、完全有序的记录序列（例如 MySQL binlog）。
> 
> 共识算法通过消息将日志广播至所有节点，它们就日志什么位置，记录什么达成共识。换句话说，所有的节点中，都有着相同顺序的日志序列。
> 
> 节点按顺序执行日志序列，所有节点最终将达到一致的状态

# 3 Paxos 算法详解
## 3.1 简介
Paxos 算法是 Leslie Lamport（[莱斯利·兰伯特](https://zh.wikipedia.org/wiki/%E8%8E%B1%E6%96%AF%E5%88%A9%C2%B7%E5%85%B0%E4%BC%AF%E7%89%B9)）在 **1990** 年提出了一种分布式系统 **共识** 算法。这也是第一个被证明完备的共识算法（前提是不存在拜占庭将军问题，也就是没有恶意节点）。
**共识算法的作用是让分布式系统中的多个节点之间对某个提案（Proposal）达成一致的看法**。提案的含义在分布式系统中十分宽泛，像哪一个节点是 Leader 节点、多个事件发生的顺序等等都可以是一个提案。
兰伯特当时提出的 Paxos 算法主要包含 2 个部分:
- **Basic Paxos 算法**：描述的是多节点之间如何就某个值(提案 Value)达成共识。
- **Multi-Paxos 思想**：描述的是执行多个 Basic Paxos 实例，就一系列值达成共识。Multi-Paxos 说白了就是执行多次 Basic Paxos ，核心还是 Basic Paxos 。

由于 Paxos 算法在国际上被公认的非常难以理解和实现，因此不断有人尝试简化这一算法。到了 2013 年才诞生了一个比 Paxos 算法更易理解和实现的共识算法—[Raft 算法](https://javaguide.cn/distributed-system/theorem&algorithm&protocol/raft-algorithm.html) 。更具体点来说，Raft 是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现。
针对没有恶意节点的情况，除了 Raft 算法之外，当前最常用的一些共识算法比如 **ZAB 协议**、 **Fast Paxos** 算法都是基于 Paxos 算法改进的。

针对存在恶意节点的情况，一般使用的是 **工作量证明（POW，Proof-of-Work）**、 **权益证明（PoS，Proof-of-Stake ）** 等共识算法。这类共识算法最典型的应用就是区块链，就比如说前段时间以太坊官方宣布其共识机制正在从工作量证明(PoW)转变为权益证明(PoS)。
区块链系统使用的共识算法需要解决的核心问题是 **拜占庭将军问题** ，这和我们日常接触到的 ZooKeeper、Etcd、Consul 等分布式中间件不太一样。

下面我们来对 Paxos 算法的定义做一个总结：
- Paxos 算法是兰伯特在 1990 年提出了一种分布式系统共识算法。
- 兰伯特当时提出的 Paxos 算法主要包含 2 个部分: Basic Paxos 算法和 Multi-Paxos 思想。
- Raft 算法、ZAB 协议、 Fast Paxos 算法都是基于 Paxos 算法改进而来。

## 3.2 Basic Paxos 算法
### 3.2.1 问题和背景
假如我们设计一个由三个节点 A、B、C 组成分布式集群，提供只读的 KV 存储服务。所有的节点必须要先对只读变量的值（提案）达成共识，然后所有的节点再一起创建这个只读变量。
当有多个客户端访问这个系统，试图创建同一个只读变量时，集群中所有的节点该如何达成共识，实现各个节点中的 x 值的一致呢？
![[Pasted image 20250410223011.png|图3.2.2.1 x值一致性|500]]

实现多个节点 x 值一致的复杂度主要来源于以下两个因素的共同影响：
1. 系统内部各个节点的**通信是不可靠的**，总会发生诸如**机器宕机**或**网络异常**（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。
2. 客户端**写入是并发的**，如果是串行的修改数据，仅单纯使用少数服从多数原则，就足以保证数据被正确读写。而并发访问就变成了“**分布式环境下多个节点并发操作共享数据**”的问题。

我们把上面的背景问题总结转化，其实就是下面 2 个核心需求：
1. **安全性** Safety：
	- 一个变量只会被确定一个值；
	- 一个变量只有值被确定之后，才能被学习。
2. **活性** Liveness：
	- 提案最终会被接受；
	- 一个提案被接受之后，最终会被所有的 Learner 学习到；
	- 必须在有限时间内做出决议（不能有太多轮投票）。

### 3.2.2 基础概念
Basic Paxos 中存在 3 个重要的角色：
- **提议者（Proposer）**：也可以叫做协调者（coordinator），提议者负责接受客户端的请求并发起提案。提案信息通常包括提案编号 (Proposal ID) 和提议的值 (Value)。
- **接受者（Acceptor）**：也可以叫做投票员（voter），负责对提议者的提案进行投票，同时需要记住自己的投票历史；
- **学习者（Learner）**：如果有超过半数接受者就某个提议达成了共识，那么学习者就需要接受这个提议，并就该提议作出运算，然后将运算结果返回给客户端。

为了减少实现该算法所需的节点数，一个节点可以身兼多个角色。并且，一个提案被选定需要被半数以上的 Acceptor 接受。这样的话，Basic Paxos 算法还具备容错性，在少于一半的节点出现故障时，集群仍能正常工作。

### 3.2.3 解决思路推导
Basic Paxos 问题背景相信已经讲清楚了，那怎么解决？
#### 3.2.3.1 只有一个Acceptor
简单的方案如同图 3.2.3.1 所示，多个提议节点、单个决策节点，决策节点接受第一个发给它的值，作为被选中的值。但如果决策节点故障，整个系统就会不可用。
![[Pasted image 20250410223805.png|图3.2.3.1]]

#### 3.2.3.2 多个Acceptor
##### 3.2.3.2.1 Acceptor约束
为了克服单点故障问题，借鉴多数派的机制，思路是写入一半以上的节点，如果集群中有 N 个节点，客户端需要写入 W >= N/2 + 1 个节点。使用多数派的机制后最多可容忍 (N-1)/2 个节点故障。
但是问题还是存在：**每个Acceptor节点该接受几个提案呢**？如果我们希望即使只有一个Proposer提出了一个value，该value也最终被选定。那么，就得到下面的约束：
==P1：一个Acceptor必须接受它收到的第一个提案。==

但是，这又会引出另一个问题：如果每个Proposer分别提出不同的value，发给不同的Acceptor。根据P1，Acceptor分别接受自己收到的value，就导致不同的value被选定。出现了不一致。如下图：
![[Pasted image 20250411092515.png|图 3.2.3.2]]
刚刚是因为『一个提案只要被一个Acceptor接受，则该提案的value就被选定了』才导致了出现上面不一致的问题。因此，我们需要加一个规定：
==规定：一个提案被选定需要被半数以上的Acceptor接受==
这个规定又暗示了：『一个Acceptor必须能够接受不止一个提案！』不然可能导致最终没有value被选定。比如上图的情况。v1、v2、v3都没有被选定，因为它们都只被一个Acceptor的接受。

还要考虑到，如果两个节点同时发起提案，就会导致提案冲突。如下图，S1 向 S1、S2、S3 发起提案（red）。同时，S5 也向 S3、S4、S5 发起提案（blue）。它们的提案 Quorum 都达成了，也就是说一个提案有两个值被批准，这显然破坏了一致性原则。
![[image-12.png]]
你会发现提案冲突发生在 S3，S3 是两个 Quorum 的交集点，它的时间线上有两个不同的值被批准。
我们知道，设计程序的一个基本常识是，如果多个线程同时操作某个共享变量，一定要加上互斥锁，不然会出现各种意外情况。不难发现，**S3 问题的本质是“在分布式环境下并发操作共享变量的问题”**。
由于分布式环境中随时可能发生通信故障，我们不能粗暴“套用”进程加锁机制来解决 S3 的问题。我们可以借鉴“乐观锁”的思路，加上“版本号”。首先，S1 发起提案，S3 收到 S1 提案时，应该意识到 S5 发起的提案（blue）的 Quorum 已经达成，S1 提案（red）已经失效。根据先到先得原则，S1 应该更新自己的提案值（red 替换为 blue），这个操作相当于对提案编号（乐观锁中的 version）“锁定”，防止之后出现多个冲突的提案编号。

重新设计提案，给每个提案加上一个提案编号，表示提案被提出的顺序。令『**提案=提案编号+value**』。虽然允许多个提案被选定，但必须保证所有被选定的提案都具有相同的value值。否则又会出现不一致。于是有了下面的约束：
==P2：如果某个value为v的提案被选定了，那么每个编号更高的被选定提案的value必须也是v。==
一个提案只有被Acceptor接受才可能被选定，因此我们可以把P2约束改写成对Acceptor接受的提案的约束P2a。
==P2a：如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v。==

但是，考虑如下的情况：假设总的有5个Acceptor。Proposer2提出[M1,V1]的提案，Acceptor2~5（半数以上）均接受了该提案，于是对于Acceptor2~5和Proposer2来讲，它们都认为V1被选定。
Acceptor1刚刚从宕机状态恢复过来（之前Acceptor1没有收到过任何提案），此时Proposer1向Acceptor1发送了[M2,V2]的提案（V2≠V1且M2>M1），对于Acceptor1来讲，这是它收到的第一个提案。根据P1（一个Acceptor必须接受它收到的第一个提案。）,Acceptor1必须接受该提案（Acceptor1 不知道V2有没有被半数以上节点选定）。
![[Pasted image 20250411094327.png|图 P2a]]

##### 3.2.3.2.2 Proposer约束
P2a是对Acceptor接受的提案约束，但其实提案是Proposer提出来的，所有我们可以对Proposer提出的提案进行约束。得到P2b：
==P2b：如果某个value为v的提案被选定了，那么之后任何Proposer提出的编号更高的提案的value必须也是v。==
如何确保在某个value为v的提案被选定后，Proposer提出的编号更高的提案的value都是v呢？只要满足P2c即可：
==P2c：对于任意的N和V，如果提案[N, V]被提出，那么存在一个半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个：==
- ==S中每个Acceptor都没有接受过编号小于N的提案。==
- ==S中Acceptor接受过的最大编号的提案的value为V。==

基于此，就需要一个**两阶段（2-phase）协议，对于已经选定的值，后面的提案要放弃自己的提议，提出已经被选中的值**。例如，图P2a 中的 Proposer1 发起提案之前，先广播给 Acceptor2-4 这 3 个节点，询问是否已经有接受的提案，如果已有，则撤销自己的提案，并且改成 [M2, V2]。
先广播的这个请求，又叫Prepare请求。

### 3.2.4 算法描述
对于 Acceptor，我们给它定义几个变量：
1. PrepareN：自己收到的最大 Prepare 请求的编号
2. AcceptN：自己最终选定的编号
3. AcceptV：自己最终选定的值

Paxos算法分为**两个阶段**。具体如下：
1. 阶段一：准备阶段
	1. Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare(N)请求。
	2. Acceptor接受到Prepare请求后
		1. 如果N<=PrepareN，就不响应or响应error
		2. 否则N>PrepareN，令PrepareN=N，响应(Pok,AcceptN,AcceptV)或(Pok,null,null)
2. 阶段二：批准阶段
	1. 如果 Proposer 收到超过半数的 Pok，就发出Accept(N,V)请求。如果响应中有提案，则 V=响应中最大的AcceptN对应的AcceptV；否则 V=自己定的值
		1. 否则Pok数未过半，重新获取N（N递增，不重复），发起Prepare请求（回到第一阶段）
	2. Acceptor接受到 Accept 请求后
		1. 如果 N>=PrepareN，接受提案，令 AcceptN=N，AcceptV=V。回复Aok
		2. 否则 N<PrepareN，不接受。不响应or响应error
	3. 如果 Proposer 收到超过半数的 Aok，则确定 V 被选定，并广播给所有 learner；否则重新发起Prepare请求（回到第一阶段）

## 3.3 Multi Paxos 思想
Basic Paxos 算法的仅能就单个值达成共识，达成共识至少需要两次网络往返，高并发情况下还可能导致活锁：
![[image-7.png]]
因此Basic Paxos几乎只是用来做理论研究，并不直接应用在实际工程中。

实际应用中几乎都需要连续确定多个值，而且希望能有更高的效率。Multi-Paxos正是为解决此问题而提出。Multi-Paxos基于Basic Paxos做了两点改进：
1. 针对每一个要确定的值，运行一次Paxos算法实例（Instance），形成决议。每一个Paxos实例使用唯一的Instance ID标识。
2. 在所有Proposers中选举一个Leader，由Leader唯一地提交Proposal给Acceptors进行表决。这样没有Proposer竞争，解决了活锁问题。在系统中仅有一个Leader进行Value提交的情况下，Prepare阶段就可以跳过，从而将两阶段变为一阶段，提高效率。

可执行一次Basic Paxos实例来选举出一个Leader。在Leader宕机之后服务临时不可用，需要重新选举Leader继续服务。
Multi-Paxos允许有多个自认为是Leader的节点并发提交Proposal而不影响其安全性，这样的场景即退化为Basic Paxos。

由于兰伯特提到的 Multi-Paxos 思想缺少代码实现的必要细节(比如怎么选举领导者)，所以在理解和实现上比较困难。
Chubby和Boxwood均使用Multi-Paxos。ZooKeeper使用的Zab也是Multi-Paxos的变形。
Raft 算法就是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现，实际项目中可以优先考虑 Raft 算法。

## 3.4 总结
Paxos 是一个分布式共识算法，主要包含 2 个部分: Basic Paxos 算法和 Multi-Paxos 思想。
Basic Paxos 算法描述的是多节点之间如何就某个提案达成共识。Multi-Paxos 思想在 Basic Paxos 算法的基础上，就一系列提案达成共识，并提升效率。

Basic Paxos 算法有三个角色：提议者（Proposer）、接受者（Acceptor）、学习者（Learner）。提案包含两个部分：编号N和值V。
Basic Paxos 算法是一个二阶段算法：
1. 准备阶段：Proposer向半数以上的Acceptor发Prepare请求。Acceptor如果判断N>收到过的最大编号，就返回OK和接收过的提案（如果有的话）
2. 接受阶段：Proposer收到半数以上Preapre请求的ok响应，就会向半数以上的Acceptor发Accept请求（V=响应中最大编号的V or 自己选的值）
	1. Acceptor如果判断N>=收到过的最大编号，就接受提案返回ok。
	2. Proposer 收到超过半数的Accept请求的ok响应，则确定 V 被选定，并广播给所有 learner

问题：
1. 为什么要两个阶段？因为可能有其他Proposer已经提出了提案并被大多数Acceptor接受，为了避免同时提出多个提案，需要有个查询确认的过程。
2. 为什么需要编号？Acceptor可能会收到多个Proposer的提案，所以它需要用编号排序提案的顺序，决定要接受哪个提案。类似于乐观锁中版本号的概念。

Multi-Paxos 思想在所有Proposers中选举一个Leader，由Leader唯一地提交提案。这样只需要一阶段提交就行，提升了效率；并且没有Proposer竞争，解决了活锁问题。

但 Multi-Paxos 思想缺少代码实现的必要细节(比如怎么选举领导者)，所以在理解和实现上比较困难。而Raft 算法就是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现。

Basic Paxos算法推导正确性的一些约束：
1. 要有多个 Acceptor 节点。这样避免了单点故障。
2. 一个Acceptor必须接受它收到的第一个提案。这样即使只有一个Proposer提出了一个提案，该提案也最终被选定。
3. 一个提案被选定需要被半数以上的Acceptor接受。这样避免了多个提案都被选定。
	- 这又暗示了，一个Acceptor必须能够接受不止一个提案，不然可能导致最终没有value被选定。
	- 例如三个Proposer分别向三个不同的Acceptor发送提案，每个Acceptor都只接受了一个，导致出现平票问题。
4. 如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v。
	- 这样保证了允许多个提案被接受：因为所有被选定的提案都具有相同的value值，就不会出现不一致。
5. 如果某个value为v的提案被选定了，那么之后任何Proposer提出的编号更高的提案的value必须也是v。这样就保证上面一条约束能得到满足。
6. 对于任意的N和V，如果提案[N, V]被提出，那么存在一个半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个。这样就保证上面一条约束能得到满足。
	- S中每个Acceptor都没有接受过编号小于N的提案。
	- S中Acceptor接受过的最大编号的提案的value为V。

# 4 Raft 算法详解
## 4.1 背景
> [!tip] 词义
> Raft 是 Re{liable|plicated|dundant} And Fault-Tolerant，即可靠、复制、冗余和容错，组合起来的单词。同时，Raft 在英文有“筏”的含义，隐喻一艘帮助你逃离 Paxos 小岛的救生筏。

Paxos 算法理解起来非常晦涩。此外，论文虽然提到了 Multi Paxos，但缺少实现细节。虽然所有的共识系统都是从 Paxos 算法开始的，但工程师们实现过程中有很多难以逾越的难题，往往不得已开发出与 Paxos 完全不一样的算法，这导致 Lamport 的证明并没有太大价值。所以，很长的一段时间内，实际上并没有一个被大众广泛认同的 Paxos 算法。

2013 年，斯坦福大学的学者 Diego Ongaro 和 John Ousterhout 发表了论文 《[In Search of an Understandable Consensus Algorithm](https://www.thebyte.com.cn/consensus/raft.html#footnote1)》，提出了 Raft 算法。Raft 论文开篇描述了 Raft 的证明和 Paxos 等价，详细阐述了算法如何实现。也就是说，Raft 天生就是 Paxos 算法的工程化。

不同于Paxos算法直接从分布式一致性问题出发推导出来，**Raft算法则是从多副本状态机的角度提出，用于管理多副本状态机的日志复制**。
Raft实现了和Paxos相同的功能，它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等。
同时，Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。

## 4.2 基础概念
### 4.2.1 节点类型
Raft 提出了领导者角色，通过选举机制“分享”提案权利：
- **领导者**（Leader）：负责处理所有客户端请求，将请求转换为“日志”复制到其他节点，不断地向所有节点广播心跳消息：“你们的领导还在，不要发起新的选举”。
- **跟随者**（Follower）：接收、处理领导者的消息，并向领导者反馈日志的写入情况。当领导者心跳超时时，他会主动站起来，推荐自己成为候选人。
- **候选人**（Candidate）：候选人属于过渡角色，他向所有的节点广播投票消息，如果他赢得多数选票，那么他将晋升为领导者。

在正常的情况下，只有一个服务器是 Leader，剩下的服务器是 Follower。Follower 是被动的，它们不会发送任何请求，只是响应来自 Leader 和 Candidate 的请求。

### 4.2.2 任期
联想到现实世界中的领导人都有一段不等的任期。自然，Raft 算法中也对应的概念 —— **“任期”（term）**。Raft 中的任期是一个递增的数字，贯穿于 Raft 的选举、日志复制和一致性维护过程中。
- **选举过程**：任期确保了领导者的唯一性。在一次任期内，只有获得多数选票的节点才能成为领导者。
- **日志一致性**：任期号会附加到每条日志条目中，帮助集群判断日志的最新程度。
- **冲突检测**：通过比较任期号，节点可以快速判断自己是否落后，并切换到跟随者状态。

每个节点都会存储当前的 term 号，当服务器之间进行通信时会交换当前的 term 号：
- 如果有服务器发现自己的 term 号比其他人小，那么他会更新到较大的 term 值。
- 如果一个 Candidate 或者 Leader 发现自己的 term 过期了，他会立即退回成 Follower。
- 如果一台服务器收到的请求的 term 号是过期的，那么它会拒绝此次请求。

### 4.2.3 日志
- `entry`：每一个事件成为 entry，只有 Leader 可以创建 entry。entry 的内容为`<term,index,cmd>`，其中 cmd 表示客户端请求的具体操作内容，也就是可以应用到状态机的操作。
- `log`：由 entry 构成的数组，每一个 entry 都有一个表明自己在 log 中的 index。只有 Leader 才可以改变其他节点的 log。entry 总是先被 Leader 添加到自己的 log 数组中，然后再发起共识请求，获得同意后才会被 Leader 提交给状态机。Follower 只能从 Leader 获取新日志和当前的 commitIndex，然后把对应的 entry 应用到自己的状态机中。

## 4.3 领导者选举
### 4.3.1 心跳机制
raft 使用心跳机制来触发 Leader 的选举。
如果一台服务器能够收到来自 Leader 或者 Candidate 的有效信息，那么它会一直保持为 Follower 状态，并且刷新自己的 electionElapsed，重新计时。
Leader 会向所有的 Follower 周期性发送心跳来保证自己的 Leader 地位。如果一个 Follower 在一个周期内没有收到心跳信息，就叫做**选举超时**，然后它就会认为此时没有可用的 Leader，并且**等待一段随机的时间后（避免活锁）**发起一次Leader选举。

### 4.3.2 RequestVote RPC 请求
为了开始新的选举，Follower 会**自增自己的 term 号**并且转换状态为 Candidate 并**给自己投一票**。然后他会向所有节点发起 **RequestVote RPC** 请求，消息示例如下：
```json
{
  "term": 5, // 候选者的当前任期号，用于通知接收方当前选举属于哪个任期。
  "candidateId": 3, // 候选者的节点 ID，标识请求投票的节点。
  "lastLogIndex": 12, // 候选者日志的最后一条日志的索引，用于比较日志的完整性。
  "lastLogTerm": 4//候选者日志的最后一条日志的任期号，用于进一步比较日志的新旧程度。
}
```
其他节点收到投票消息后，根据下面的条件判断是否投票：
- Candidate 的日志至少与投票者的日志一样新：如果最后一条log entry的term更大，则term大的更新；如果term一样大，则log index更大的更新。
- 当前节点尚未在本任期投票。
RequestVote 响应的示例如下：
```json
{
  "term": 5, //接收方的当前任期号，用于告知候选者最新的任期号。如果候选者发现该值比自己大，会转为跟随者。
  "voteGranted": true//是否投票给候选者，true 表示同意，false 表示拒绝。
}
```

### 4.3.3 选举结果
Candidate 的状态会持续到以下情况发生：
- 赢得选举
- 其他节点赢得选举
- 一轮选举结束，无人胜出
赢得选举的条件是：一个 Candidate 在一个任期内收到了来自集群内的多数选票`（N/2+1）`，就可以成为 Leader。

在 Candidate 等待选票的时候，它可能收到其他节点声明自己是 Leader 的心跳，此时有两种情况：
- 该 Leader 的 term 号大于等于自己的 term 号，说明对方已经成为 Leader，则自己回退为 Follower。
- 该 Leader 的 term 号小于自己的 term 号，那么会拒绝该请求并让该节点更新 term。

下图概述了 Raft 集群 Leader 选举过程：
![[image-8.png]]

### 4.3.4 如何选择节点的数量？
Raft 日志复制过程需要等待多数节点确认。节点越多，等待的延迟也相应增加。所以说，以 Raft 构建的分布式系统并不是节点越多越好。如 etcd，推荐使用 3 个节点，对高可用性要求较高，且能容忍稍高的性能开销，可增加至 5 个节点，如果超出 5 个节点，可能得不偿失。

## 4.4 日志同步
### 4.4.1 AppendEntries RPC 请求
Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 **AppendEntries RPC** 复制日志条目：
```json
{
  "term": 5, // 领导者的任期号
  "leaderId": "leader-123",
  "prevLogIndex": 8, // 前一日志条目的索引
  "prevLogTerm": 4, // 前一日志条目的任期
  "entries": [
    { "index": 9, "term": 5, "command": "set x=4" }, // 要复制的日志条目
  ],
  "leaderCommit": 7// Leader 的“已提交”状态的日志条目索引号
}
```
当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。
某些Followers可能没有成功的复制日志，Leader会无限的重试 AppendEntries RPC直到所有的Followers最终存储了所有的日志条目。

### 4.4.2 Raft日志同步保证
Raft日志同步保证如下两点：
- 如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。
- 如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的。

第一条特性源于Leader在一个term内在给定的一个log index最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变。
第二条特性源于 AppendEntries 的一个简单的一致性检查。当发送一个 AppendEntries RPC 时，Leader会把新日志条目紧接着之前的条目的log index和term都包含在里面。如果Follower没有在它的日志中找到log index和term都相同的日志，它就会拒绝新的日志条目。

### 4.4.3 日志不一致的处理
一般情况下，Leader和Followers的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，Leader崩溃可能会导致日志不一致：旧的Leader可能没有完全复制完日志中的所有条目。
![[image-9.png]]
一个Follower可能会丢失掉Leader上的一些条目，也有可能包含一些Leader没有的条目，也有可能两者都会发生（我猜脑裂的情况下可能会出现？）。丢失的或者多出来的条目可能会持续多个任期。

Leader通过强制Followers复制它的日志来处理日志的不一致，Followers上的不一致的日志会被Leader的日志覆盖。
具体来说，follower 收到日志复制请求后，它会通过 prevLogIndex 和 prevLogTerm 检查本地日志的连续性。如果日志缺失或存在冲突，follower-2 返回失败响应，指明与领导者日志不一致的部分：
```json
{
  "success": false,
  "term": 4,
  "conflictIndex": 4, // 表示发生缺失的日志索引，Follower 的日志中最大索引为 3，所以缺失的索引是 4。
  "conflictTerm": 3//缺失日志的“上一个有效日志条目”的任期号
}
```
当领导者收到失败响应，根据 conflictIndex 和 conflictTerm 找到与跟随者日志的最大匹配索引。随后，领导者从该索引开始重新向跟随者发送日志条目。
如果还是失败，再根据响应确定日志一致位点，然后向后逐条覆盖Followers在该位置之后的条目。

## 4.5 安全性
### 4.5.1 选举限制
Leader 需要保证自己存储全部已经提交的日志条目。这样才可以使日志条目只有一个流向：从 Leader 流向 Follower，Leader 永远不会覆盖已经存在的日志条目。
每个 Candidate 发送 RequestVoteRPC 时，都会带上最后一个 entry 的信息。所有节点收到投票信息时，会对该 entry 进行比较，如果发现自己的更新，则拒绝投票给该 Candidate。
判断日志新旧的方式：如果两个日志的 term 不同，term 大的更新；如果 term 相同，更长的 index 更新。

### 4.5.2 日志提交限制
Leader只能提交当前term的已经复制到大多数服务器上的日志，旧term日志的提交要等到提交当前term的日志来**间接提交**（log index 小于 commit index的日志会被间接提交）。

之所以要这样，是因为可能会出现已提交的日志又被覆盖的情况：
![[image-10.png]]
1. 在阶段a，term为2，S1是Leader，且S1写入日志（term, index）为(2, 2)，并且日志被同步写入了S2；
2. 在阶段b，S1离线，触发一次新的选主。此时S5被选为新的Leader，系统term为3，且写入了日志（term, index）为（3， 2）;
3. 在阶段c，S5尚未将日志推送到Followers就离线了，进而触发了一次新的选主。而之前离线的S1经过重新上线后被选中变成Leader，此时系统term为4。
	S1将日志（2， 2）同步到了S3（注意这个日志是旧term的日志），而此时由于该日志已经被同步到了多数节点（S1, S2, S3），因此，此时日志（2，2）可以被提交了。
4. 在阶段d，S1又下线了，触发一次选主，而S5被选为新的Leader。然后S5会将自己的日志更新到Followers，于是S2、S3中已经被提交的日志（2，2）被截断了，出现了数据不一致的问题。

因此，我们要增加日志提交的限制：即使日志（2，2）已经被大多数节点（S1、S2、S3）确认了，但是它不能被提交，因为它是来自之前term（2）的日志；直到S1在当前term（4）产生的日志（4， 4）被大多数Followers确认，S1方可提交日志（4，4）这条日志。当然，根据Raft定义，（4，4）之前的所有日志也会被提交。
此时即使S1再下线，重新选主时S5不可能成为Leader，因为它没有包含大多数节点已经拥有的日志（4，4）。

### 4.5.3 节点崩溃
如果 Leader 崩溃，集群中的节点在 electionTimeout 时间内没有收到 Leader 的心跳信息就会触发新一轮的选主，**在选主期间整个集群对外是不可用的**。
如果 Follower 和 Candidate 崩溃，处理方式会简单很多。之后发送给它的 RequestVoteRPC 和 AppendEntriesRPC 会失败。由于 raft 的所有请求都是幂等的，所以失败的话会无限的重试。如果崩溃恢复后，就可以收到新的请求，然后选择追加或者拒绝 entry。

### 4.5.4 时间与可用性
raft 的要求之一就是安全性不依赖于时间：系统不能仅仅因为一些事件发生的比预想的快一些或者慢一些就产生错误。为了保证上述要求，最好能满足以下的时间条件：
`broadcastTime << electionTimeout << MTBF`
- `broadcastTime`：向其他节点并发传递消息的平均响应时间；
- `electionTimeout`：选举超时时间；
- `MTBF(mean time between failures)`：单台机器的平均健康时间；

`broadcastTime`应该比`electionTimeout`小一个数量级，为的是使`Leader`能够持续发送心跳信息（heartbeat）来阻止`Follower`开始选举。
`electionTimeout`也要比`MTBF`小几个数量级，为的是使得系统稳定运行。当`Leader`崩溃时，大约会在整个`electionTimeout`的时间内不可用；我们希望这种情况仅占全部时间的很小一部分。

由于`broadcastTime`和`MTBF`是由系统决定的属性，因此需要决定`electionTimeout`的时间。
一般来说，broadcastTime 一般为 `0.5～20ms`，electionTimeout 可以设置为 `10～500ms`，MTBF 一般为一两个月。

## 4.6 日志压缩
在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响可用性。Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃。
每个副本独立的对自己的系统状态进行snapshot，并且只能对已经提交的日志记录进行snapshot。
Snapshot中包含以下内容：
- 日志元数据。最后一条已提交的 log entry的 log index和term。这两个值在snapshot之后的第一条log entry的AppendEntries RPC的完整性检查的时候会被用上。
- 系统当前状态。

当Leader要发给某个日志落后太多的Follower的log entry被丢弃，Leader会将snapshot发给Follower；或者当新加进一台机器时，也会发送snapshot给它。发送snapshot使用InstalledSnapshot RPC。

做snapshot既不要做的太频繁，否则消耗磁盘带宽；也不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响可用性。推荐当日志达到某个固定的大小做一次snapshot。
做一次snapshot可能耗时过长，会影响正常日志同步。可以通过使用**copy-on-write技术（类似Redis）**避免snapshot过程影响正常日志同步。

## 4.7 成员变更
### 4.7.1 问题背景
在前面的内容中，我们假设集群节点数固定，即集群的 Quorum 也保持不变。然而，在生产环境中，集群通常需要进行节点变更，例如因故障移除节点或扩容增加节点等。对于旨在实现容错能力的算法来说，显然不能通过“关闭集群、更新配置并重启系统”的方式来实现。

在讨论如何实现成员动态变更之前，我们需要先搞明白 Raft 集群中“配置”（configuration）的概念：配置说明集群由哪些节点组成。例如，一个集群有三个节点（Server 1、Server 2、Server 3），该集群的配置就是 [Server1、Server2、Server3]。
如果把“配置”当成 Raft 中的“特殊日志”。这样一来，成员动态变更需求就可以转化为“配置日志”的一致性问题。但需要注意的是，各个节点中的日志“应用”（apply）到状态机是异步的，不可能同时操作。这种情况下，apply “配置日志”很容易导致“脑裂”问题。

举个具体例子，假设有一个由三个节点 [Server1、Server2 和 Server3] 组成的 Raft 集群，当前的配置为 Cold。现在，我们计划增加两个节点 [Server1、Server2、Server3、Server4、Server5]，新的配置为 Cnew。
由于日志提交是异步处理的，假设 Server1 和 Server2 比较迟钝，仍在使用老配置 Cold，而 Server3、Server4、Server5 的状态机已经应用了新配置 Cnew：
- 假设 Server5 触发选举并赢得 Server3、Server4、Server5 的投票（满足 Cnew 配置下的 Quorum 3 要求），成为领导者；
- 同时，假设 Server1 也触发选举并赢得 Server1、Server2 的投票（满足 Cold 配置下的 Quorum 2 要求），成为领导者。

一个集群存在两个领导者也就是“脑裂”，同一个日志索引可能会对应不同的日志条目，最终导致集群数据不一致。
上述问题的根本原因在于，成员变更过程中形成了两个没有交集的 Quorum，即 [Server1, Server2] 和 [Server3, Server4, Server5] 各自为营。

### 4.7.2 联合共识算法
为了解决这一问题，Raft提出了两阶段的成员变更方法。集群先从旧成员配置Cold切换到一个过渡成员配置，称为联合共识（joint consensus），联合共识是旧成员配置Cold和新成员配置Cnew的组合Cold U Cnew，一旦联合共识Cold U Cnew被提交，系统再切换到新成员配置Cnew。

Raft两阶段成员变更过程如下：
1. Leader收到成员变更请求从Cold切成Cold,new；
2. Leader在本地生成一个新的log entry，其内容是Cold∪Cnew，代表当前时刻新旧成员配置共存，写入本地日志，同时将该log entry复制至Cold∪Cnew中的所有副本。**在此之后新的日志同步需要保证得到Cold和Cnew两个多数派的确认**；
3. Follower收到Cold∪Cnew的log entry后更新本地日志，并且此时就以该配置作为自己的成员配置；
4. 如果Cold和Cnew中的两个多数派确认了Cold U Cnew这条日志，Leader就提交这条log entry并切换到Cnew；
5. 接下来Leader生成一条新的log entry，其内容是新成员配置Cnew，同样将该log entry写入本地日志，同时复制到Follower上；
6. Follower收到新成员配置Cnew后，将其写入日志，并且从此刻起，就以该配置作为自己的成员配置
7. Leader收到Cnew的多数派确认后，表示成员变更成功，后续的日志只要得到Cnew多数派确认即可。Leader给客户端回复成员变更执行成功。

异常分析：
1. Cold,new日志在提交之前，在这个阶段，Cold,new中的所有节点有可能处于Cold的配置下，也有可能处于Cold,new的配置下，如果这个时候原Leader宕机了，无论是发起新一轮投票的节点当前的配置是Cold还是Cold,new，都需要Cold的节点同意投票，所以不会出现两个Leader。也就是old节点不可能同时follow两个leader。
2. Cold,new提交之后，Cnew下发之前，此时所有Cold,new的配置已经在Cold和Cnew的大多数节点上，如果集群中的节点超时，那么肯定只有有Cold,new配置的节点才能成为Leader，所以不会出现两个Leader
3. Cnew下发以后，Cnew提交之前，此时集群中的节点可能有三种，Cold的节点（可能一直没有收到请求）， Cold,new的节点，Cnew的节点，其中Cold的节点因为没有最新的日志的，集群中的大多数节点是不会给他投票的，剩下的持有Cnew和Cold,new的节点，无论是谁发起选举，都需要Cnew同意，那么也是不会出现两个Leader
4. Cnew提交之后，这个时候集群处于Cnew配置下运行，只有Cnew的节点才可以成为Leader，这个时候就可以开始下一轮的成员变更了。

两阶段成员变更比较通用且容易理解，但是实现比较复杂，同时两阶段的变更协议也会在一定程度上影响变更过程中的服务可用性，因此我们期望增强成员变更的限制，以简化操作流程。

### 4.7.3 单成员变更
Diego Ongaro 后来又提出一种更为简化的方案 — “单成员变更”（Single Server Changes）。该方案思想的核心是，既然同时提交多个成员变更可能引发问题，那么每次只提交一个成员变更，需要添加多个成员，就执行多次单成员变更操作。这样不就没有问题了么！
单成员变更方案很容易穷举所有情况，如下图所示，穷举奇/偶数集群下节点添加/删除情况。**如果每次只操作一个节点，Cold 的 Quorum 和 Cnew 的 Quorum 一定存在交集。交集节点只会进行一次投票，要么投票给 Cold，要么投票给 Cnew**。因此，不可能出现两个符合条件的 Quorum，也就不会出现两个领导者。
![[image-11.png]]
以(b)为例，Cold 为 [Server1、Server2、Server3]，该配置的 Quorum 为 2，Cnew 为 [Server1、Server2、Server3、Server4]，该配置的 Quorum 为 3。假设 Server1、Server2 比较迟钝，还在用 Cold ，其他节点的状态机已经应用 Cnew：
- 假设 Server1 触发选举，赢得 Server1，Server2 的投票，满足 Cold Quorum 要求，当选领导者；
- 假设 Server3 也触发选举，赢得 Server3，Server4 的投票，但不满足Cnew 的 Quorum 要求，选举失效。

变更的流程如下：
1. 向Leader提交一个成员变更请求，请求的内容为服务节点的是添加还是移除，以及服务节点的地址信息
2. Leader在收到请求以后，回向日志中追加一条ConfChange的日志，其中包含了Cnew，后续这些日志会随着AppendEntries的RPC同步所有的Follower节点中
3. 当ConfChange的日志被添加到日志中是立即生效，所以Cnew的节点可以进行投票（注意：不是等到提交以后才生效）
4. 当ConfChange的日志被复制到Cnew的大多数时，那么就可以对日志进行提交了

## 4.8 其他思考
再思考一个问题，Raft 算法属于“强领导者”（Strong Leader）模型，领导者负责所有写入操作，它的写瓶颈就是 Raft 集群的写瓶颈。那么，该如何突破 Raft 集群的写瓶颈呢？
一种方法是使用哈希算法将数据划分成多个独立部分（分片）。例如，将一个 100TB 规模数据的系统分成 10 部分，每部分只需处理 10TB。这种根据规则（范围或哈希）将数据分散处理的策略，被称为“分片机制”（Sharding）。
分片机制广泛应用于 Prometheus、Elasticsearch 、ClickHouse 等大数据系统。理论上，只要机器数量足够，分片机制就能支持任意规模的数据。

## 4.9 总结
不同于Paxos算法直接从分布式一致性问题出发推导出来，**Raft算法则是从多副本状态机的角度提出，用于管理多副本状态机的日志复制**。
Raft 它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等。

Raft算法基础概念：
1. 节点类型：领导者（Leader）、 跟随者（Follower）、候选人（Candidate）
2. 任期：递增的数字，当服务器之间进行通信时会交换当前的 term 号。term 贯穿于 Raft 的选举、日志复制和一致性维护过程中。
3. 日志：参见日志的总结

Raft算法核心思想：
1. Leader选举：raft 使用心跳机制来触发 Leader 的选举。
	1. Leader 会向所有的 Follower 周期性发送心跳来保证自己的 Leader 地位。
	2. 如果一个 Follower 在一个周期内没有收到心跳信息，就叫做**选举超时**，然后它就会认为此时没有可用的 Leader。
	3. Follower**等待一段随机的时间后（避免活锁）**转变成Candidate，自增自己的 term 号、给自己投一票并发起一次Leader选举。
	4. 一个 Candidate 在一个任期内收到了来自集群内的多数选票`（N/2+1）`，就可以成为 Leader。
2. 日志同步：
	1. Leader把客户端请求作为日志条目加入到它的日志中，然后并行的向其他节点同步日志条目。如果一个日志条目被同步到大多数服务器上，就被认为可以提交了。
	2. Raft日志同步保证：1）不同节点日志中相同索引和任期号的日志条目 A 是一致的（leader保证）；2）并且 A 之前的日志条目也都是一样的（日志同步请求的一致性检查保证）
	3. 当Leader和Follower日志不一致时，Leader通过强制Followers复制它的日志来处理日志的不一致
3. 安全性：
	1. 选举限制：每个 Candidate 发送投票请求时，都会带上最后一个 entry 的信息。其他节点接受到投票消息时，如果自己的日志更新就会拒绝投票
	2. 日志提交限制：Leader只能提交当前term的已经复制到大多数服务器上的日志，旧term日志的提交要等到提交当前term的日志来间接提交。因为可能会出现已提交的日志又被覆盖的情况。
	3. 节点崩溃：Leader 崩溃，在选主期间整个集群对外是不可用的
	4. 时间与可用性：最好满足这样的时间条件：`broadcastTime << electionTimeout << MTBF`
4. 日志压缩：把日志压缩为 Snapshot（最后一条日志条目的下标和任期，系统当前状态），用来减少日志回放的耗时（用在新节点和很老的follower）
5. 成员变更：单成员变更实现比联合共识算法简单。如果每次只操作一个节点，Cold 的 Quorum 和 Cnew 的 Quorum 一定存在交集。交集节点只会进行一次投票，要么投票给 Cold，要么投票给 Cnew，因此不会有脑裂问题。

---
# 5 引用
[分布式共识](https://www.thebyte.com.cn/consensus/summary.html)
[Paxos算法](https://www.cnblogs.com/linbingdong/p/6253479.html)
[Raft算法-知乎](https://zhuanlan.zhihu.com/p/32052223)
[Raft算法](https://blog.csdn.net/zhousenshan/article/details/137998763)