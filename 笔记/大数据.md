2025-12-06 17:23
Status: #idea
Tags:

# 1 Hive
## 1.1 hive SQL 和 mysql 有什么区别？它比mysql有什么优势？
sql 层面的不同：
- MySQL：支持标准 SQL，侧重事务型功能（索引、存储过程、触发器），靠索引提升单条记录查询效率。
- Hive：HiveSQL 是类 SQL，扩展了大数据适配功能（比如自定义函数 UDF）。
	- hive sql很适合做覆写一个表的整个分区或追加操作，但是像mysql那样修改某一条记录则很低效。
	- 但是它能处理多表关联、复杂聚合这类 MySQL 扛不住的计算。

hive难以细粒度的修改数据，不是它的缺陷，而是它和mysql的应用场景不同。

核心定位与场景（最根本）：
- MySQL 是**联机事务处理（OLTP）** 数据库，它面向实时处理场景
- Hive不是数据库，而是**基于 Hadoop 的 SQL 解析引擎和定义数据的存储格式**，属于联机分析处理（OLAP）。面向大数据离线分析场景，适合低频次、TB/PB 级海量数据的批量分析和计算
	- **离线分析场景本就不需要频繁改单条数据**。

底层存储与计算（差异的根源）：
- MySQL：**数据存在本地文件系统** / 共享存储（如 SAN），**计算依赖单机** / 小集群，能力受单节点 CPU / 内存限制；用 InnoDB 等引擎做**行式存储**，行式存储适合随机读写单条记录，所以能支持细粒度更新。
- Hive：本身不存数据，数据全靠 **HDFS（Hadoop 分布式文件系统）** 存储 ——HDFS 是分布式、高容错的，能**横向扩容到成百上千节点，天生适配海量数据**。计算则把 Hive SQL 翻译成 Spark/MapReduce 分布式任务，分散到集群节点并行执行。Hive 默认用 ORC/Parquet **列式存储**，只读取分析需要的列（比如算成本只读价格、数量列，不用读全表），批量分析效率远高于行式存储。

数据操作与事务：
- MySQL：支持细粒度增删改查、完整 ACID 事务、行锁 / 表锁，这是为了满足交易场景的一致性（比如改一笔订单的价格）。
- Hive：早期不支持细粒度更新和事务，不是 “缺陷”，而是**适配 HDFS 特性** ——HDFS 文件写入后难以修改（追加可行，修改成本极高），所以核心操作是 “覆盖 / 追加分区”（比如按日期分区计算每日成本，直接写入当天分区）。
	- Hive 3.0 后虽支持 ACID，但仅用于缓慢变化维度等小众场景，并非核心设计目标，因为**离线分析场景本就不需要频繁改单条数据**（成本算价只需要批量计算全量数据，不需要改某一条物料的价格）。

扩展性与性能：
- MySQL：垂直扩展为主（升级服务器配置），水平扩展（分库分表）复杂度高，能处理 GB 级数据，毫秒级响应。
- Hive：水平扩展简单（加集群节点即可），能线性提升存储 / 计算能力，处理 TB/PB 级数据，高吞吐但延迟高（符合离线分析的节奏）。

## 1.2 Hive的底层结构是什么呢？
### 1.2.1 Hive 的存储底层：先明确 “Hive 不存数据，只映射数据”
Hive 本身不是数据库，没有自己的存储引擎 —— 它的 “存储” 分为两层：
1. **元数据层**：表结构、分区信息、数据存储路径、文件格式等（存在 Metastore，通常用 MySQL 托管）；
2. **实际数据层**：所有数据都以文件形式存在 HDFS 上（分布式存储，数据分散在集群多个节点），Hive 只是通过 SQL 解析，映射到 HDFS 的文件。

而 Hive 的核心优化，就体现在 “HDFS 上的文件格式”—— 日常用的 ORC/Parquet 是**列式存储**，这是和 MySQL 行式存储的核心差异，也是 Hive 能高效处理批量分析的关键。

### 1.2.2 核心重点：Hive 列式存储（ORC/Parquet）的底层工作机制
#### 1.2.2.1 列式存储的 “数据组织方式”（核心：按列分块，而非按行）
假设成本算价表有 4 列：`order_id（订单ID）、price（单价）、quantity（数量）、date（日期）`，共 100 万行数据：
- **MySQL 行式存储**：把每一行的所有字段存在一起（比如一行是「1001, 50.0, 2, 20251001」），100 万行就是 100 万个 “行数据块”，读的时候要么读整行，要么通过 B + 树找某一行；
- **Hive ORC 列式存储**：（Parquet 逻辑类似，只是分块 / 压缩细节不同）把所有列的数据单独聚合，拆成 “列块 + 元数据” 的分层结构，最终存在 HDFS 的一个 / 多个ORC文件里，每个ORC文件包含一些行，它的结构如下（从大到小）：
	- 文件头（File Header）：存列数、压缩方式、文件创建时间等全局元数据
	- 多个Stripe（数据块，默认250MB/个）：ORC的核心数据单元，每个Stripe又分别包含ORC的一些行。Stripe里面对每列分别作为列块存储，列块内部会做 “编码 + 压缩”。分布式计算按Stripe并行处理。
		- Index Data（列索引）：每列的轻量索引，存该 Stripe 内每列的 min/max、取值范围
		- Row Data（行数据）：按列存储的实际数据（所有order_id放一个块，所有price放一个块，以此类推）
		- Stripe Footer：存该Stripe的列偏移量、统计信息等
	- 文件尾（File Footer）：存文件内所有列的统计信息（比如`payer_id`的 min/max、`goods`的取值范围、null 数等）、数据偏移量等

所以说是叫列式存储，其实是先按行划分文件，文件内部再按行划分Stripe，Stripe内部最后按列划分。

注意，列式存储不是 “把列完全孤立存储”，ORC 文件的每个 Stripe 内会存储 N 行数据（比如 100 万行），这些行在 Stripe 内有**连续的行号（0、1、2...999999）** —— 每一列的列块数据，都是按这个行号顺序存储的。因此，通过行号能把不同的列关联起来。
ORC/Parquet 会在 Stripe 的「Index Data」里记录「行号→列数据偏移量」的映射关系，哪怕列数据做了压缩 / 编码，读取时也能通过偏移量快速定位到同一行的列值，不会错位。

#### 1.2.2.2 为什么列式存储批量分析更快？
比如：`select sum (price*quantity) from order where date='20251001' and payer_id=200003 and goods='ECS'`。
Hive读流程：
1. **元数据查询**：Hive 先查 Metastore，拿到表的存储路径（HDFS 路径）、文件格式（ORC）、分区信息（比如按 date 分区）；
2. **谓词下推**（核心优化）：
	1. **分区级下推**：最粗粒度，直接排除 99% 无效数据
	2. **ORC 文件级下推**（过滤整个无效文件）：分区下会有多个 ORC 文件，Hive 不会直接读文件内容，而是先读每个 ORC 文件的「文件尾（File Footer）」—— 这里存了该文件所有列的全局统计信息（比如`payer_id`的 min/max、`goods`的取值范围、null 数等）：
		- 比如某 ORC 文件的`payer_id`全局统计是`min=200000，max=200002`，而你要的是`payer_id=200003` → 直接跳过这个文件，不用读里面任何数据；
		- 再比如某文件的`goods`全局统计只有`RDS、SLB、OSS`，没有`ECS` → 也直接跳过这个文件；
		- 只有文件尾的统计显示：`payer_id`包含 200003、`goods`包含 ECS → 才会处理这个文件。
	3. **Stripe 级下推**（过滤无效数据块）：每个 Stripe 都有「Index Data（列索引）」—— 存该 Stripe 内每列的 min/max、取值范围：
		- 比如某 Stripe 的`payer_id`索引是`min=200003，max=200003`，但`goods`索引只有`RDS` → 跳过这个 Stripe；
		- 再比如某 Stripe 的`payer_id=200003`且`goods=ECS` → 才会读取这个 Stripe 的实际数据；
		- 这一步又过滤掉了文件内的无效数据块，只留下需要处理的 Stripe。
	4. **行级下推**（最终过滤）：读取符合条件的 Stripe 后，Hive 会在列数据块内对每行做最终过滤：
		- 比如某行的`payer_id=200003`但`goods='RDS'` → 直接过滤掉这一行，不读取它的`price/quantity`；
		- 只有同时满足`payer_id=200003`且`goods='ECS'`的行 → 才会读取对应的`price`和`quantity`。
3. **列裁剪**（核心优化）：只读取需要的列；
4. **分布式读取**：HDFS 把不同 Stripe 分布在集群不同节点，Spark/Hive 同时读取多个 Stripe 的 price/quantity 列块，解压后计算 sum (price\*quantity)。

如果用 MySQL 做这个计算，需要：
1. 遍历 B + 树找到所有 202510 的行（随机 IO，慢）；
2. 每找到一行，必须读取整行数据（包括 order_id 等无关列，IO 浪费）；
3. 单节点计算（无法分布式），100 万行可能还能撑，1 亿行直接卡死。

## 1.3 总结
- 注意，我在公司写的是 Spark SQL
- Hive和MySQL的差异
	- Hive不是类似于MySQL的存储引擎，它只负责定义数据的格式，而实际的存储交给分布式文件系统解决，SQL的解析和计算工作又交给Spark解决
	- MySQL支持事务和单条记录的操作，而Hive则难以做到修改单条记录，它更适合做表分区整体的覆盖和追加操作。不过这不是它的缺点，而是它和MySQL的应用场景不同
	- MySQL它是联机事务处理数据库，面向实时场景。而Hive面向的是大数据离线分析场景，适合低频次、TB级海量数据的批量分析和计算
	- Hive和MySQL差异的根源在于底层的存储原理不同：MySQL是本地文件系统，计算依赖单机，底层结构是行式存储；而Hive是分布式文件系统，可以很容易做水平扩展以容纳海量数据，它的计算也是分布式计算且容易扩展，底层结构是列式存储，适合做列裁剪优化
- Hive的底层结构
	- Hive 分为元数据和数据文件。元数据定义了表结构、数据文件的结构、存储路径等信息，一般存储在MySQL等数据库中；而数据文件存储实际的行列数据，存储在分布式文件系统中
	- Hive 表的每个分区，都会按照行切分成多个文件。每个文件内部，又包含文件头、数据块、文件尾三个部分。
	- 文件头存列数、压缩方式等全局元数据；文件尾存每一列的统计信息（例如一列的范围）；每个数据块存一部分行。
	- 数据块内部分为索引、列数据。它的索引也是列的统计信息，例如范围。列数据把一行的每一列单独存储，并且会使用一些压缩方式。一列中的每一行都会记录行号和列偏移量的映射，这样通过行号能把不同的列关联起来。
- Hive的查询过程：
	- 先查元数据，获得文件的存储位置等信息。然后又下面三个优化
	- 谓词下推
		- 分区级过滤
		- 文件级过滤
		- 数据块级过滤
		- 数据块内部筛选
	- 列裁剪：只读指定的列
	- 分布式读取：按照数据块并发处理

# 2 Spark
## 2.1 核心架构：Driver + Executor
Spark 是分布式内存计算引擎，采用**主从架构**，所有计算都基于这个架构展开：
- **Driver（驱动节点）**：是 Spark 应用的 “大脑”，核心是`SparkSession`（Spark 2.0+，替代老的 SparkContext），负责：
	- 解析你的 SQL / 代码，生成执行计划；
	- 向集群（YARN/K8s/Standalone）申请资源（Executor）；
		- 其中Spark on YARN，提两种模式——Client 模式 Driver 在客户端，适合调试；Cluster 模式 Driver 在 YARN 容器里，适合生产，避免客户端挂掉导致任务中断。
	- 拆分任务（Task）并调度到 Executor；
	- 监控任务执行、容错（比如 Task 失败重试）。
- **Executor（执行节点）**：是工作节点上的进程，每个 Executor 有多个 Core（CPU 核心）和专属内存，负责：
	- 执行 Driver 分配的 Task（**每个 Task 处理一个数据分区**）；
	- 存储计算中间结果（优先内存，内存不足落磁盘）；
	- 向 Driver 汇报任务状态。

## 2.2 核心抽象：RDD
Spark 所有计算的底层都是 RDD（弹性分布式数据集），这是理解 Spark 的关键：
- **定义**：分布式、不可变、可分区的数据集，是 Spark 对 “分布式数据 + 计算” 的抽象；
- **核心特性**（面试必提）：
	- 惰性求值：只有遇到 Action（如 count、saveAsTable）才触发真正计算，Transformation（如 map、groupBy）只是记录操作；
	- 不可变：创建后不能修改，只能通过转换生成新 RDD，保证分布式计算一致性；
	- 弹性（容错 + 资源弹性）：
		- 容错：通过 “血统（Lineage）” 记录 RDD 的生成路径（它的所有转换操作），某个分区丢了，只需重算该分区（而非全量）；
		- 资源弹性：可动态扩缩容 Executor，适配计算负载；
	- 可分区：数据分片存储在不同节点，是并行计算的基础。

## 2.3 执行流程
1. **提交 SQL**：Driver 的 SparkSession 解析 SQL，做**语法校验 + 逻辑优化**（例如谓词下推、列裁剪）
2. 生成**物理执行计划**：将优化后的逻辑计划转成可执行的物理计划（比如选择 HashJoin 还是 SortMergeJoin，避免低效的 Shuffle）；
3. **DAG 调度**：拆分成多个 Stage（阶段）——Stage 的划分依据是 “是否有宽依赖（Shuffle）”：
	- 窄依赖（如 filter、map）：父 RDD 一个分区对应子 RDD 一个分区，无数据混洗，归为一个 Stage；
	- 宽依赖（如 groupBy、join）：父 RDD 一个分区对应子 RDD 多个分区，需要 Shuffle，是 Stage 的边界；
	- Spark 优化：内存 Shuffle（避免落盘）、Shuffle 文件合并（减少 IO）、动态分区（避免数据倾斜）。
4. **Task 调度**：Driver 将每个 Stage 的 Task（每个 Task 处理一个 RDD 分区）分发给 Executor 的 Core 执行；
5. **执行与容错**：Executor 执行 Task，中间结果优先存内存；若 Task 失败，Driver 重试（默认 3 次）；若 Executor 挂了，Driver 重新分配资源。

其他优化：
- 如果计算过程中多次用到同一基础数据（比如商品基础价），可通过`cache()`/`persist()`将 RDD 存内存 / 磁盘，避免重复计算，这是 Spark 比 Hive MR 快的核心原因之一。

## 2.4 数据倾斜
### 2.4.1 定义
数据倾斜是分布式计算中最常见的性能问题，指的是**数据被分配到不同节点（分区）时，分布极不均匀**：大部分数据集中在少数几个分区，导致这些分区的 Task 执行时间远超其他 Task（比如 99% 的 Task 1 分钟完成，1% 的 Task 跑 1 小时），最终拖慢整个作业的执行速度，甚至出现 OOM（内存溢出）。
例如用`商品ID`做 join 时，某个商品 ID 对应的数据量极大，会导致 join 的 Task 倾斜。或者执行`group by 产品ID`，如果某个爆款产品的订单量占了总数据的 80%，那么处理这个产品 ID 的 Task 就会出现倾斜。

### 2.4.2 核心原因
数据倾斜的本质是**键（Key）的哈希分布不均**或**处理逻辑不合理**，具体分为以下 4 类。

#### 2.4.2.1 键值（Key）分布天然不均
这是最常见的情况，属于业务数据本身的特性：
- **热点 Key**：比如电商场景中，爆款商品的 ID、大促期间的日期 Key，这些 Key 对应的数据量会远大于其他 Key。
- **长尾 Key**：少数 Key 对应的数据量极大，其余 Key 数据量很小。

#### 2.4.2.2 特殊值（Null 值 / 空字符串）处理不当
在 join、group by 时，若关联键 / 分组键包含大量**Null 值**或空字符串，这些值的哈希值是固定的，会全部落到同一个分区，导致倾斜。

#### 2.4.2.3 数据类型不一致导致的隐性倾斜
比如两张表做 join 时，一张表的`商品ID`是**字符串类型**（如 "123"），另一张表的`商品ID`是**数值类型**（如 123）：
- Spark/Hive 会对不同类型的 Key 计算不同的哈希值，导致原本应该匹配的 Key 被分到不同分区，而其中一种类型的 Key 可能集中在少数分区，形成隐性倾斜。

#### 2.4.2.4 不合理的分区策略或配置
- **分区数太少**：比如整个作业只设置了 10 个分区，即使数据分布均匀，也可能因为分区数不足导致单个分区数据量过大；
- **自定义分区函数有问题**：比如手动编写的分区函数逻辑缺陷，导致数据集中到少数分区；
- **Spark/Hive 的 Shuffle 分区数默认值过小**（Spark 默认 200，Hive 默认 1），无法应对大数据量的分布需求。

### 2.4.3 解决办法
#### 2.4.3.1 聚合操作
聚合操作的倾斜主要是热点 Key 导致的，最常用的解决办法是**两阶段聚合（局部聚合 + 全局聚合，也叫加盐聚合）**，其次是单独处理热点 Key。

##### 2.4.3.1.1 两阶段聚合
将原本的热点 Key 拆分成多个子 Key（加盐），先做局部聚合，再去掉盐值做全局聚合，把一次大聚合拆成两次小聚合，分散数据压力。
流程：
1. **加盐**：给热点 Key 添加随机前缀（比如 0~9 的随机数），将一个热点 Key 拆分成多个子 Key；
2. **局部聚合**：按 “随机前缀 + 原 Key” 分组聚合，计算每个子 Key 的局部结果；
3. **去盐**：去掉随机前缀，按原 Key 分组聚合，合并局部结果得到最终结果。

代码：
```sql
-- 原SQL（存在倾斜：产品ID=10086的数据量极大）
SELECT product_id, SUM(cost) AS total_cost
FROM cost_table
GROUP BY product_id;

-- 优化后的两阶段聚合SQL
WITH step1 AS (
    -- 步骤1：加盐，给product_id添加0~9的随机前缀
    SELECT 
        CONCAT(CAST(RAND() * 10 AS INT), '_', product_id) AS salt_product_id,
        product_id,
        cost
    FROM cost_table
),
step2 AS (
    -- 步骤2：局部聚合，按加盐后的Key分组
    SELECT 
        salt_product_id,
        product_id,
        SUM(cost) AS partial_cost
    FROM step1
    GROUP BY salt_product_id, product_id
),
step3 AS (
    -- 步骤3：去盐，按原product_id全局聚合
    SELECT 
        product_id,
        SUM(partial_cost) AS total_cost
    FROM step2
    GROUP BY product_id
)
SELECT * FROM step3;
```

##### 2.4.3.1.2 单独处理热点 Key
将热点 Key 从原数据中抽出来，单独计算其聚合结果，再与其他 Key 的聚合结果合并，避免热点 Key 影响整体作业。
流程：
1. **筛选热点 Key**：通过`group by Key LIMIT 10`找到数据量最大的热点 Key（比如产品 ID=10086）；
2. **单独计算热点 Key**：过滤出热点 Key 的数据，单独做聚合（可增加分区数）；
3. **计算非热点 Key**：过滤掉热点 Key 的数据，正常做聚合；
4. **合并结果**：将热点 Key 和非热点 Key 的聚合结果 union 起来。

使用场景：热点 Key 数量极少（比如 1~2 个）的情况，处理起来比两阶段聚合更简单。

#### 2.4.3.2 关联操作
join 操作的倾斜分**大小表 join**和**大表 join 大表**两种情况，解决办法不同。

##### 2.4.3.2.1 广播小表
将小表的数据全部广播到所有 Executor 的内存中，大表直接在本地与小表数据做 join，**完全避免 Shuffle**，从根源上解决倾斜。
Spark 会自动触发 Broadcast Join（默认小表阈值为 10MB，可通过`spark.sql.autoBroadcastJoinThreshold`调整，比如设为 100MB），也可以手动指定。

代码：
```sql
-- 原SQL（商品表是小表，成本表是大表，join时可能倾斜）
SELECT c.*, p.product_name
FROM cost_table c
JOIN product_table p ON c.product_id = p.product_id;

-- 手动指定广播小表（强制触发Broadcast Join）
SELECT c.*, p.product_name
FROM cost_table c
JOIN /*+ BROADCAST(p) */ product_table p ON c.product_id = p.product_id;
```

**适用场景**：**小表 join 大表**（小表数据量在百 MB 级别内），这是解决 join 倾斜的首选方案。

##### 2.4.3.2.2 大表 join 大表的热点 Key 处理
如果两张都是大表，且存在热点 Key，需要给热点 Key 加盐，将大表拆分成多个小表做 join，再合并结果。并且要根据大表的业务类型（维度表 / 事务表），调整加盐的策略。
例如 cost_table 和 order_table 按照 product_id join，通常分为以下两种情况，处理方式不同：
1. **场景 1（最常见）**：一张是**维度属性表**（比如`cost_table`是产品成本维度表，每个`product_id`只有**1 条记录**，存储的是该产品的单位成本 / 标准成本），另一张是**事务表**（`order_table`是订单事务表，每个`product_id`有**多条订单记录**）。
2. **场景 2（较少见）**：两张都是**事务表**（`cost_table`是成本事务表，`order_table`是订单事务表，同一个`product_id`在两张表中都有多条记录）。

###### 2.4.3.2.2.1 场景 1：维度表 + 事务表
- 对**事务表（订单表）**：给热点 Key 加**随机前缀**（0~9），将 100 万条 10086 记录分散到 10 个前缀（每个前缀 10 万条）；
- 对**维度表（成本表）**：给热点 Key 加**全量前缀**（0~9 各生成 1 条），让 1 条 10086 记录变成 10 条（前缀 0~9），覆盖订单表的所有随机前缀；
- 这样，订单表的`3_10086`能匹配成本表的`3_10086`，`5_10086`能匹配成本表的`5_10086`，**所有记录都能正确匹配，结果条数和原逻辑一致**。

```sql
-- 优化后的加盐join：维度表+事务表（成本算价主流场景）
WITH 
-- 第一步：处理事务表（订单表）：热点Key加随机前缀（0~9），非热点Key加固定前缀0
order_table_salt AS (
    SELECT 
        order_id,
        product_id,
        order_amount,
        order_time,
        -- 热点Key（10086）加随机前缀，非热点Key加固定前缀0
        CASE 
            WHEN product_id = 10086 THEN CONCAT(CAST(RAND() * 10 AS INT), '_', product_id)
            ELSE CONCAT('0_', product_id) 
        END AS salt_product_id
    FROM order_table
),
-- 第二步：处理维度表（成本表）：热点Key加全量前缀（0~9），非热点Key加固定前缀0
-- 关键：用explode生成0~9的前缀，让1条10086记录变成10条
cost_table_salt AS (
    SELECT 
        product_id,
        cost,
        cost_time,
        -- 加盐后的关联键
        CONCAT(salt_num, '_', product_id) AS salt_product_id
    FROM cost_table
    -- 生成0~9的数字序列，用于全量前缀
    LATERAL VIEW EXPLODE(ARRAY(0,1,2,3,4,5,6,7,8,9)) tmp AS salt_num
    -- 只对热点Key做全量前缀，非热点Key只保留0前缀（避免数据膨胀）
    WHERE (product_id = 10086) OR (salt_num = 0)
),
-- 第三步：用加盐后的关联键join，结果和原逻辑完全一致
join_result AS (
    SELECT 
        o.order_id,
        o.product_id,
        c.cost,
        o.order_amount,
        o.order_time
    FROM order_table_salt o
    JOIN cost_table_salt c ON o.salt_product_id = c.salt_product_id
)
SELECT * FROM join_result;
```

###### 2.4.3.2.2.2 场景 2：两张都是事务表——分治法
将**热点 Key 和非热点 Key 分开处理**，再合并结果：
1. **热点 Key 单独处理**：把两张表中`product_id=10086`的数据抽出来，通过**增加 Shuffle 分区数**让`m×n`条结果分散到多个 Task；
2. **非热点 Key 正常处理**：把两张表中`product_id≠10086`的数据正常 join；
3. **合并结果**：将热点 Key 和非热点 Key 的 join 结果`UNION ALL`。

```sql
-- 第一步：设置Spark Shuffle分区数（临时调整，针对热点Key的大计算量）
-- 注意：Spark SQL中可以通过SET命令调整，也可以在代码中用repartition指定分区数
SET spark.sql.shuffle.partitions = 500; -- 默认为200，热点Key处理时调大（如500/1000）

-- 场景2：两张都是事务表的分治处理（成本算价）
WITH 
-- -------------- 第一部分：处理热点Key（product_id=10086）--------------
-- 1.1 筛选成本表的热点Key数据，并重分区（增加分区数分散压力）
cost_hot AS (
    SELECT * FROM cost_table WHERE product_id = 10086
    DISTRIBUTE BY product_id, rand() -- 按product_id+随机数分区，分散数据
),
-- 1.2 筛选订单表的热点Key数据，并重分区
order_hot AS (
    SELECT * FROM order_table WHERE product_id = 10086
    DISTRIBUTE BY product_id, rand() -- 同上，保证分区分散
),
-- 1.3 热点Key数据join（此时分区数足够，m×n条结果分散到多个Task）
join_hot AS (
    SELECT 
        o.order_id,
        o.product_id,
        c.cost,
        o.order_amount,
        o.order_time,
        c.cost_time
    FROM order_hot o
    JOIN cost_hot c ON o.product_id = c.product_id
),

-- -------------- 第二部分：处理非热点Key（product_id≠10086）--------------
-- 2.1 筛选成本表的非热点Key数据
cost_normal AS (
    SELECT * FROM cost_table WHERE product_id != 10086
),
-- 2.2 筛选订单表的非热点Key数据
order_normal AS (
    SELECT * FROM order_table WHERE product_id != 10086
),
-- 2.3 非热点Key数据正常join（无需额外处理，数据分布均匀）
join_normal AS (
    SELECT 
        o.order_id,
        o.product_id,
        c.cost,
        o.order_amount,
        o.order_time,
        c.cost_time
    FROM order_normal o
    JOIN cost_normal c ON o.product_id = c.product_id
),

-- -------------- 第三部分：合并热点Key和非热点Key的结果--------------
final_result AS (
    SELECT * FROM join_hot
    UNION ALL
    SELECT * FROM join_normal
)

-- 最终结果：与原逻辑完全一致，且解决了数据倾斜
SELECT * FROM final_result;
```

###### 2.4.3.2.2.3 场景 2：两张都是事务表——两步加盐法
“两步加盐法” 是 **「一张表随机加盐（分散压力） + 另一张表全量加盐（扩容覆盖所有盐值）」**，具体逻辑：
1. **第一步：确定盐值范围**（比如 0~9，记为盐值`salt`）；
2. **第二步：主表（选数据量更大的表，比如订单表）随机加盐**：给热点 Key 的每条记录加一个 0~9 的随机盐值，将数据分散到 10 个分组；
3. **第三步：从表（成本表）全量加盐（扩容）**：给热点 Key 的每条记录**同时添加 0~9 的所有盐值**（即 1 条记录变成 10 条记录，覆盖所有盐值）；
4. **第四步：盐值匹配 join**：两张表通过`product_id + salt`关联，此时主表的每条随机盐值记录，都能在从表中找到对应盐值的记录，且从表的全量扩容保证了**每条主表记录能匹配从表的所有记录**，最终结果条数仍是`m×n`。

```sql
-- 场景2：两张都是事务表的正确两步加盐法（扩容+加盐，成本算价）
WITH 
-- -------------- 第一步：定义盐值范围（0~9，可调整为更大范围如0~19）--------------
salt_range AS (
    SELECT explode(array(0,1,2,3,4,5,6,7,8,9)) AS salt -- 生成0~9的盐值
),

-- -------------- 第二步：处理主表（订单表）：热点Key随机加盐，非热点Key加固定盐值0 --------------
order_table_salt AS (
    SELECT 
        order_id,
        product_id,
        order_amount,
        order_time,
        -- 热点Key：随机选0~9的盐值；非热点Key：固定盐值0
        CASE 
            WHEN product_id = 10086 THEN cast(rand() * 10 as int)
            ELSE 0 
        END AS salt
    FROM order_table
),

-- -------------- 第三步：处理从表（成本表）：热点Key全量加盐（扩容），非热点Key加固定盐值0 --------------
cost_table_salt AS (
    SELECT 
        c.cost_id,
        c.product_id,
        c.cost,
        c.cost_time,
        -- 热点Key：取salt_range的所有盐值（0~9）；非热点Key：只取盐值0
        CASE 
            WHEN c.product_id = 10086 THEN s.salt
            ELSE 0 
        END AS salt
    FROM cost_table c
    -- 笛卡尔积：热点Key的每条记录与所有盐值匹配（扩容10倍），非热点Key只与盐值0匹配（无扩容）
    CROSS JOIN salt_range s
    WHERE (c.product_id = 10086) OR (s.salt = 0) -- 限制非热点Key只保留盐值0，避免无意义扩容
),

-- -------------- 第四步：通过product_id + salt join，保证全量匹配且分散压力 --------------
final_result AS (
    SELECT 
        o.order_id,
        o.product_id,
        c.cost,
        o.order_amount,
        o.order_time,
        c.cost_time
    FROM order_table_salt o
    JOIN cost_table_salt c 
        ON o.product_id = c.product_id 
        AND o.salt = c.salt -- 盐值匹配，分散到不同Task
    -- 强制按salt分区，让每个盐值的任务在独立的Task中执行
    DISTRIBUTE BY o.salt, o.product_id
)

-- 最终结果：条数与原逻辑一致（m×n），且数据倾斜问题解决
SELECT * FROM final_result;
```

##### 2.4.3.2.3 处理 Null 值 / 空字符串
将 Null 值的 Key 替换成随机值，让其均匀分布到各个分区，避免集中。

代码：
```sql
-- 原SQL（product_id为Null的记录导致倾斜）
SELECT c.*, p.product_name
FROM cost_table c
JOIN product_table p ON c.product_id = p.product_id;

-- 优化：将Null值替换为随机数，避免集中
SELECT c.*, p.product_name
FROM (
    SELECT 
        CASE WHEN product_id IS NULL THEN CONCAT('null_', RAND()) ELSE product_id END AS product_id,
        cost,
        order_id
    FROM cost_table
) c
LEFT JOIN product_table p ON c.product_id = p.product_id;
```

#### 2.4.3.3 通用预防 / 优化手段
1. **合理设置 Shuffle 分区数**：
    - Spark：调整`spark.sql.shuffle.partitions`（默认 200，可根据数据量调到 500~2000）；
    - Hive：调整`hive.exec.reducers.bytes.per.reducer`（默认 1GB，减少该值可增加 Reducer 数）；
2. **数据预处理**：统一数据类型（比如将商品 ID 全部转为字符串）、清洗 Null 值 / 空字符串；

## 2.5 总结
- Spark是一个分布式计算引擎，它采用**主从架构**
	- Driver负责解析SQL生成执行架构，从资源管理器获取资源，分配Task给Executor，监控任务的执行和容错（如重试）
	- Executor负责执行Driver分配的Task，并向Driver汇报Task状态
- Spark所有计算的底层是RDD（弹性分布式数据集），它有以下特点
	- 不可变：RDD不能修改，只能转换成新的RDD
	- 可分区：数据分片存储在不同节点，是并行计算的基础。
	- 容错：RDD 会记录生成它的操作，它的某个分区生成失败，只需要重算这个分区
	- 惰性求值：只有遇到 Action（如 count）才触发真正计算，Transformation（如 groupBy）只是记录操作
- Spark的执行流程：
	- 解析SQL，生成逻辑执行计划
	- 生成物理执行计划，比如Join选择Sort
	- 生成DAG，根据shuffle划分Task
	- 把Task分配给Executor执行
	- 

---
# 3 引用