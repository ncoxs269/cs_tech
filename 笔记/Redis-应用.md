2025-05-01 11:23
Status: #idea
Tags: [[Redis]]


# 1 旁路缓存
我们也把 Redis 称为**旁路缓存**，也就是说，读取缓存、读取数据库和更新缓存的操作都需要在应用程序中来完成。
Redis 缓存的两种类型：**只读缓存和读写缓存**。只读缓存能加速读请求，而读写缓存可以同时加速读写请求。而且，读写缓存又有两种数据写回策略。

## 1.1 只读缓存
应用要读取数据的话，会先调用 Redis `GET` 接口，查询数据是否存在。
而所有的数据写请求，会直接发往后端的数据库，在数据库中增删改。对于删改的数据来说，如果 Redis 已经缓存了相应的数据，应用需要把这些缓存的数据删除，Redis 中就没有这些数据了。
当应用再次读取这些数据时，会发生缓存缺失，应用会把这些数据从数据库中读出来，并写到缓存中。这样一来，这些数据后续再被读取时，就可以直接从缓存中获取了，能起到加速访问的效果。

只读缓存直接在数据库中更新数据的好处是，所有最新的数据都在数据库中，而数据库是提供数据可靠性保障的，这些数据不会有丢失的风险。当我们需要缓存图片、短视频这些用户只读的数据时，就可以使用只读缓存这个类型了。

## 1.2 读写缓存
对于读写缓存来说，除了读请求会发送到缓存进行处理（直接在缓存中查询数据是否存在)，所有的写请求也会发送到缓存，在缓存中直接对数据进行增删改操作。

同步直写是指，写请求发给缓存的同时，也会发给后端数据库进行处理，等到缓存和数据库都写完数据，才给客户端返回。这样，即使缓存宕机或发生故障，最新的数据仍然保存在数据库中，这就提供了数据可靠性保证。
~~而异步写回策略，则是优先考虑了响应延迟。此时，所有写请求都先在缓存中处理。等到这些增改的数据要被从缓存中淘汰出来时，缓存将它们写回后端数据库。这样一来，处理这些数据的操作是在缓存中进行的，很快就能完成。只不过，如果发生了掉电，而它们还没有被写回数据库，就会有丢失的风险了。~~

读写缓存相对于只读模式，当数据被修改后的下一次查询不需要从数据库读。

# 2 缓存和数据库的一致性
## 2.1 只读缓存
### 2.1.1 先删除缓存，再更新数据库
#### 2.1.1.1 问题一：并发读写数据不一致
假设线程 A 删除缓存值后，还没有来得及更新数据库，线程 B 就开始读取数据了，那么这个时候，线程 B 会发现缓存缺失，就只能去数据库读取：
- 线程 B 读取到了旧值；
- 线程 B 是在缓存缺失的情况下读取的数据库，所以，它还会把旧值写入缓存，这可能会导致其他线程从缓存中读到旧值。

**延迟双删：在线程 A 更新完数据库值以后，我们可以让它先 sleep 一小段时间，再进行一次缓存删除操作**。
之所以要加上 sleep 的这段时间，就是为了让线程 B 能够先从数据库读取数据，再把缺失的数据写入缓存，然后，线程 A 再进行删除。所以，线程 A sleep 的时间，就需要大于线程 B 读取数据再写入缓存的时间。
这个时间怎么确定呢？建议你在业务程序运行的时候，统计下线程读数据和写缓存的操作时间，以此为基础来进行估算。

严格来说，在极端情况下，延迟双删还是会存在数据不一致情况。比如:A线程删除了缓存，准备更新数据库之前，B线程进来没有缓存命中，去数据库查找到了旧值，但是B在准备更新缓存时被阻塞了；这时候A更新了数据库，sleep了，然后删除缓存，这时候B拿着旧数据再来更新缓存，就导致了数据不一致问题。

#### 2.1.1.2 问题二：回滚
如果缓存删除成功，数据库更新失败，那么不好回滚缓存。虽然不会导致数据不一致。

### 2.1.2 情况二：先更新数据库值，再删除缓存值
如果线程 A 更新了数据库中的值，但还没来得及删除缓存值，线程 B 就开始读取数据了，就会读到缓存的旧值。不过可以把两个删除放到一个事务里面，这样保证数据的一致性。
如果数据库更新成功，缓存删除失败，可以回滚数据库。

#### 2.1.2.1 概率很低的缓存不一致问题
1. 请求A读缓存未命中
2. 请求A读数据库中的值20
3. 请求B将数据库更新为30
4. 请求B删除缓存
5. 请求A将缓存更新为20

缓存的写入远远快于数据库的写入，所以在实际中很难出现请求B已经更新了数据库并且清空了缓存，请求A才更新完缓存的情况。

## 2.2 读写缓存
### 2.2.1 先更新缓存，再更新数据库
如果更新缓存成功，但数据库更新失败，此时缓存中是最新值，数据库中是旧值，后续读请求会直接命中缓存，但得到的是最新值，短期对业务影响不大。
但是，一旦缓存过期或者满容后被淘汰，读请求就会从数据库中重新加载旧值到缓存中，之后的读请求会从缓存中得到旧值，对业务产生影响。

如果存在并发读写，也会产生不一致：
1. 先更新缓存，再更新数据库，写+读并发：线程A先更新缓存成功，之后线程B读取数据，此时线程B命中缓存，读取到最新值后返回，之后线程A更新数据库成功。这种场景下，虽然线程A还未更新完数据库，数据库会与缓存存在短暂不一致，但在这之前进来的读请求都能直接命中缓存，获取到最新值，所以对业务没影响。
2. 先更新缓存，再更新数据库，写+写并发：线程A和线程B同时更新同一条数据，**更新缓存的顺序是先A后B，但是更新数据库的顺序是先B后A**，这也会导致数据库和缓存的不一致。

### 2.2.2 先更新数据库，再更新缓存
如果更新数据库成功，但缓存更新失败，此时数据库中是最新值，但缓存中是旧值，后续的读请求会直接命中缓存，得到的是旧值。

如果存在并发读写，也会产生不一致：
1. 先更新数据库，再更新缓存，写+读并发：线程A先更新数据库，之后线程B读取数据，此时线程B会命中缓存，读取到旧值，之后线程A更新缓存成功，后续的读请求会命中缓存得到最新值。这种场景下，线程A未更新完缓存之前，在这期间的读请求会短暂读到旧值，对业务短暂影响。
2. 1. 先更新数据库，再更新缓存，写+写并发：线程A和线程B同时更新同一条数据，更新数据库的顺序是先A后B，但更新缓存时顺序是先B后A，这会导致数据库和缓存的不一致。

### 2.2.3 总结
也就是说，在读写缓存模式下，写+读并发对业务的影响较小，而**写+写并发时，会造成数据库和缓存的不一致**。对于写请求，需要配合分布式锁使用。

读写缓存模式由于会同时更新数据库和缓存，优点是，缓存中一直会有数据，如果更新操作后会立即再次访问，可以直接命中缓存，能够降低读请求对于数据库的压力（没有了只读缓存的删除缓存导致缓存缺失和再加载的过程）。
缺点是，如果更新后的数据，之后很少再被访问到，会导致缓存中保留的不是最热的数据，缓存利用率不高（只读缓存中保留的都是热数据），所以读写缓存比较适合用于读写相当的业务场景。

# 3 缓存的其他问题
## 3.1 缓存雪崩
缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理。应用将大量请求发送到数据库层，导致数据库层的压力激增。

第一个原因是有大量数据同时过期，处理方法如下：
1. 首先，**我们可以避免给大量的数据设置相同的过期时间**。如果业务层的确要求有些数据同时失效，你可以在用 `EXPIRE` 命令给每个数据设置过期时间时，给这些数据的过期时间增加一个较小的随机数（例如，随机增加 1~3 分钟）。
2. 我们还可以通过服务降级，来应对缓存雪崩。

第二个原因是Redis 缓存实例发生故障宕机了，处理方法如下：
1. 第一个建议，是在业务系统中实现服务熔断或请求限流机制。
2. 我给你的第二个建议就是事前预防。通过主从节点的方式构建 Redis 缓存高可靠集群。

缓存雪崩还有一个场景，是一致性hash环的集群特性导致的。集群中某个主从节点挂掉了，请求分散到其他集群，但是量极大，把其他集群也都冲垮了。 解决办法，如果场景是热的极热 冷的极冷，不建议使用 一致性hash环的集群玩法，直接使用逻辑分组，挂掉的就暂时挂掉，后续人工恢复，总比打垮整个系统的好。

## 3.2 缓存击穿
缓存击穿是指，针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。缓存击穿的情况，经常发生在热点数据过期失效时。

为了避免缓存击穿给数据库带来的激增压力，我们的解决方法也比较直接，对于访问特别频繁的热点数据，我们就不设置过期时间了。**或者当访问数据时，可以刷新过期时间**。

## 3.3 缓存穿透
缓存穿透是指要访问的数据既不在 Redis 缓存中，也不在数据库中，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。这样一来，缓存也就成了“摆设”，如果应用持续有大量请求访问数据，就会同时给缓存和数据库带来巨大压力。

缓存穿透会发生在什么时候呢？一般来说，有两种情况。
- 业务层误操作：缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据；
- 恶意攻击：专门访问数据库中没有的数据。

解决方法：
1. **第一种方案是，缓存空值或缺省值**。一旦发生缓存穿透，我们就可以针对查询的数据，在 Redis 中缓存一个空值或是和业务层协商确定的缺省值（例如，库存的缺省值可以设为 0）。紧接着，应用发送的后续请求再进行查询时，就可以直接从 Redis 中读取空值或缺省值，返回给业务应用了，避免了把大量请求发送给数据库处理，保持了数据库的正常运行。
2. **第二种方法是，使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力**。
	布隆过滤器由一个初值都为 0 的 bit 数组和 N 个哈希函数组成，可以用来快速判断某个数据是否存在。当我们想标记某个数据存在时（例如，数据已被写入数据库），布隆过滤器会通过三个操作完成标记：
	- 首先，使用 N 个哈希函数，分别计算这个数据的哈希值，得到 N 个哈希值。
    - 然后，我们把这 N 个哈希值对 bit 数组的长度取模，得到每个哈希值在数组中的对应位置。
    - 最后，我们把对应位置的 bit 位设置为 1，这就完成了在布隆过滤器中标记数据的操作。
    注意Redis实现的布隆过滤器bigkey问题：Redis布隆过滤器是使用String类型实现的，存储的方式是一个bigkey，建议使用时单独部署一个实例，专门存放布隆过滤器的数据，不要和业务数据混用，否则在集群环境下，数据迁移时会导致Redis阻塞问题。
    还有使用布隆过滤器来应对缓存穿透，那当应用刚启动的时候，布隆过滤器全是0，所以需要预加载，把数据库的数据都来过滤器这边设置一遍。
3. **最后一种方案是，在请求入口的前端进行请求检测**。缓存穿透的一个原因是有大量的恶意请求访问不存在的数据。

# 4 Redis 分布式锁
## 4.1 基础概念
**分布式锁，是控制分布式系统之间同步访问共享资源的一种方式**。如果不同的系统或是同一个系统的不同主机之间共享了一个或一组资源，那么访问这些资源的时候，往往需要互斥来防止彼此干扰来保证一致性，在这种情况下，便需要使用到分布式锁。

## 4.2 必备要素
- 互斥性：在任意时刻，只有一个客户端持有锁。  
- 原子性：加锁、续期、释放锁都需要做到原子操作  
- 防死锁：分布式锁本质上是一个基于租约的租借锁，如果客户端获得锁后自身出现异常，锁能够在一段时间后自动释放，资源不会被锁死。  
- 可续期：当执行耗时较大任务时，无法提前预估加锁时间，可能在任务执行过程中对锁做续期。  
- 防误解：加锁解锁必须为同一客户端，避免出现误解锁的情况。  
* 一致性：硬件故障或网络异常等外部问题，以及慢查询、自身缺陷等内部因素都可能导致存储介质发生高可用切换，此时，如果业务对互斥性的要求非常高，锁需要在切换到新的master后保持原状态。

## 4.3 Redis集群分布式锁常见问题
### 4.3.1 死锁问题
基于SETNX命令实现最简单的分布式锁，多个client申请加锁时，如果A线程加锁成功，其他client的SETNX命令都会失败，必须等待A线程主动释放锁，但是如果A线程执行异常，没有主动释放锁，就会造成死锁，其他client将永远无法获取锁。  
解决方法：设置TTL，使用命令 SET key value PX milliseconds NX，保证原子性。

### 4.3.2 业务执行时间超出预期TTL
对于任务执行时间超出预期，或者任务执行时间难以评估的情况，不能确保一定在租期内结束任务并释放锁，当释放锁之前已经达到租期，该锁会被其他线程获取，使锁的互斥性失效。  
解决方法：使用watch dog机制，定期对锁续期

### 4.3.3 误解锁
如下图场景：  
1. Client 1 获取到锁。  
2. Client 1 开始任务，然后发生了 STW 的 GC，时间超过了锁的过期时间。  
3. Client 2 获取到锁，开始了任务。  
4. Client1 的 GC 结束后，会发起解锁请求，释放client2的锁。
![[67f3b7e0fd91110bce4326a0.png]]

解决方法：fencing token机制，每次上锁时写入uuid作为client 的唯一标识，对应“SET key value PX milliseconds NX”命令中的value值，**解锁时带着uuid解锁**。

### 4.3.4 锁释放和续期不具备原子性和强一致性  
在fencing token机制下，使用redis原生命令解锁和续期时，需要分两步：  
1. 查询当前锁是否存在（get命令，读操作）  
2. 如果client id匹配，删除锁（del命令，写操作），或者续期锁（set px nx命令，写操作）  

这样会存在两个问题：  
- 锁释放和续期不具备原子性，可能在两次操作中间，锁被其他线程占用。  
- redis集群往往是读写分离的，写入命令操作master节点，读取命令操作slave节点，由于redis集群不保证强一致性，slave节点数据可能延迟于master，最终导致解锁或续期失败。  
  
解决方法：使用redis lua实现原子解锁（CAD）和续期（CASEX）

### 4.3.5 Redis主从架构数据同步复制问题
我们通常使用「Redis Cluster」或者「哨兵模式」这两种方式实现redis的高可用，而这两种方式都是基于「主从架构数据同步复制」实现的，而redis默认的主从复制是异步的。  
Redis所有的写操作都是先在Master上操作，然后同步更新到Slave上，Slave只能读不能写。

当集群发生崩溃，主从发生切换时，会破坏redis锁的安全性，如下场景：
1. 客户端 1 在主库上执行 SET 命令，加锁成功。  
2. 此时，主库异常宕机，SET 命令还未同步到从库上（主从复制是异步的）。 
3. 从库被提升为新主库，这个锁在新的主库上丢失了。

#### 4.3.5.1 WAIT命令能够为Redis实现强一致吗？
`WAIT numreplicas timeout `   
- numreplicas：指定副本（slave）的数量。  
- timeout：超时时间，时间单位为毫秒；当设置为0 时，表示无限等待，即用不超时。  
WAIT命令作用：WAIT 命令阻塞当前客户端，直到所有先前的写入命令成功传输，并且由至少指定数量的副本（slave）确认。在主从、sentinel和Redis群集故障转移中， WAIT能够增强（仅仅是增强，但不是保证）数据的安全性。  
  
> [!info] 官方文档：https://redis.io/commands/wait  
> 然而，这只是尽力而为的尝试，因此仍然有可能丢失同步复制到多个副本的写入。  
  
所以WAIT 不能保证 Redis 的强一致性。

## 4.4 RedLock分析
### 4.4.1 基础知识
针对上面的问题，redis之父antirez设计了Redlock算法，Redlock的算法描述就放在Redis的官网上：  
- https://redis.io/topics/distlock  
在Redlock之前，很多人对于分布式锁的实现都是基于单个Redis节点的。而Redlock是基于多个Redis节点（都是Master）的一种实现。前面基于单Redis节点的算法是Redlock的基础。
注意：redLock会直接连接多个redis主节点，不是通过集群机制连接的。

优缺点：
- 优点：部分节点宕机，依然可以保证锁的可用性。  
- 缺点：实现复杂，依然不能完全解决分布式场景的资源互斥问题。  

#### 4.4.1.1 加解锁
加锁：
- 加锁：官方推荐至少 5 个redis实例，而且都是主库，它们之间没有任何关系，都是一个个孤立的实例  
* 流程如下：  
    1. 客户端先获取「当前时间戳T1」。  
    2. 客户端依次向这 5 个 Redis 实例发起加锁请求（SET命令），且每个请求会设置超时时间（毫秒级，要远小于锁的有效时间），如果某一个实例加锁失败（包括网络超时、锁被其它人持有等各种异常情况），就立即向下一个 Redis 实例申请加锁。  
    3. 如果客户端从 3 个（大多数）以上 Redis 实例加锁成功，则再次获取「当前时间戳T2」，如果 T2 - T1 < 锁的过期时间，此时，认为客户端加锁成功，否则认为加锁失败。  
    4. 如果加锁成功，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。  
    5. 如果加锁失败，向全部节点发起释放锁请求。  

解锁：Redlock算法释放锁的过程比较简单：客户端向所有Redis节点发起释放锁的操作，不管这些节点当时在获取锁的时候成功与否。  
因为当对某一个redis节点加锁时，可能因为网络原因导致加锁“失败”。注意这个“失败”，指的是redis节点实际已经加锁成功了，但是返回的结果因为网络延迟并没有传到加锁的线程，被加锁线程丢弃了，加锁线程误以为没有成功，于是加锁线程去尝试下一个节点了。

#### 4.4.1.2 仍然存在的问题
但是RedLock仍然不能解决崩溃问题：  
- 假设加锁线程在5个实例中对其中3个加锁成功，获得了这把分布式锁，这个时候3个实例中有一个实例被重启了。重启后的实例将丢失其中的锁信息，这个时候另一个加锁线程可以对这个实例加锁成功，此时两个线程同时持有分布式锁。锁的安全性被破坏。  
- 如果我们配置了AOF持久化，只能减少它发生的概率而无法保证锁的绝对安全。断电的场景下，如果redis被配置了默认每秒同步数据到硬盘，重启之后lockKey可能会丢失，理论上，如果我们想要保证任何实例重启的情况下锁都是安全的，需要在持久化配置中设置fsync=always，但此时redis的性能将大大打折扣。  
- 如果不配置redis持久化，那么只能使用**延迟重启保证锁的安全性**。把崩溃节点进行延迟重启，超过崩溃前所有锁的TTL时间之后才加入Redlock节点组。

还有其他问题：
- RedLock算法数建立在了 Time 是可信的模型上的一种分布式锁，所以时间被破坏的情况下它无法实现锁的绝对安全；  
- RedLock算法实现比较复杂，并且性能比较差；  
- RedLock需要恰当的运维保障它的正确性，故障-崩溃之后需要一套延迟重启的机制
- 进程暂停问题也很难解决，不过其他分布式锁也面临同一问题。

RedLock中的每台Redis，充当的仍旧只是存储锁数据的功能，每台Redis之间各自独立，单台Redis缺乏全局的信息，自然也不知道自己的锁数据是否是完整的。在单台Redis数据的不完整的前提下，没有分布式共识机制，使得在各种分布式环境的典型场景下（结点故障、网络丢包、网络乱序），没有完整数据但参与决策，从而破坏数据一致性。

### 4.4.2 作者的论述
Martin Kleppmann 是英国剑桥大学的分布式系统的研究员，和 Redis 之父 Antirez 进行过关于 RedLock是否安全展开过激烈讨论。

Martin 表示，一个分布式系统，更像一个复杂的野兽，存在着你想不到的各种异常情况。这些异常场景主要包括三大块，这也是分布式系统会遇到的三座大山：NPC，下面对二位的讨论内容做大致描述，感兴趣的读者可以阅读原文：  
- Martin关于分布式锁的分析博客：https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html  
- 针对Martin的blog的讨论：https://news.ycombinator.com/item?id=11059738  
- Antirez对此的反驳：http://antirez.com/news/101  
- 针对antirez的blog的讨论：https://news.ycombinator.com/item?id=11065933

#### 4.4.2.1 N：Network Delay，网络延迟

#### 4.4.2.2 P：Process Pause，进程暂停（GC）
Martin：  
1. 客户端 1 请求锁定节点 A、B、C、D、E  
2. 客户端 1 的拿到锁后，进入 GC（时间比较久）  
3. 所有 Redis 节点上的锁都过期了  
4. 客户端 2 获取到了 A、B、C、D、E 上的锁  
5. 客户端 1 GC 结束，认为成功获取锁  
6. 客户端 2 也认为获取到了锁，发生冲突

Antirez：  
- Antirez反驳到，这个假设其实是有问题的，Redlock 是可以保证锁安全的。这是怎么回事呢？ 回到Redlock 加锁的流程，重点是 1-3，在步骤 3，加锁成功后为什么要重新获取「当前时间戳T2」？还用 T2 - T1 的时间，与锁的过期时间做比较？  
* Antirez强调：如果在 1-3 发生了网络延迟、进程 GC 等耗时长的异常情况，那在第 3 步 T2 - T1，是可以检测出来的，如果超出了锁设置的过期时间，那这时就认为加锁会失败，之后释放所有节点的锁就好了！  
* 如果对方认为，发生网络延迟、进程 GC 是在步骤 3 之后，也就是客户端确认拿到了锁，去操作共享资源的途中发生了问题，导致锁失效，那这不只是 Redlock 的问题，任何其它锁服务例如Zookeeper，都有类似的问题，这不在讨论范畴内。

#### 4.4.2.3 C：Clock Drift，时钟漂移
Martin：  
1. 当多个 Redis 节点时钟发生问题时，也会导致 Redlock 锁失效。  
2. 客户端 1 获取节点 A、B、C 上的锁，但由于网络问题，无法访问 D 和 E  
3. 节点 C 上的时钟向前跳跃，导致锁到期  
4. 客户端 2 获取节点 C、D、E 上的锁，由于网络问题，无法访问 A 和 B  
5. 客户端 1 和 2 现在都相信它们持有了锁（冲突）  

上面这种情况之所以有可能发生，本质上是因为**Redlock的安全性(safety property)对系统的时钟有比较强的依赖**，一旦系统的时钟变得不准确，算法的安全性也就保证不了了。
Martin在这里其实是要指出分布式算法研究中的一些基础性问题，或者说一些常识问题，即**好的分布式算法应该基于异步模型(asynchronous model)，算法的安全性不应该依赖于任何记时假设(timing assumption)**。在异步模型中：进程可能pause任意长的时间，消息可能在网络中延迟任意长的时间，甚至丢失，系统时钟也可能以任意方式出错。
一个好的分布式算法，**这些因素不应该影响它的安全性(safety property)，只可能影响到它的活性(liveness property)**，也就是说，即使在非常极端的情况下（比如系统时钟严重错误），算法顶多是不能在有限的时间内给出结果而已，而不应该给出错误的结果。这样的算法在现实中是存在的，像比较著名的Paxos，或Raft。但显然按这个标准的话，Redlock的安全性级别是达不到的。

什么情况下会发生时钟变迁？  
- 人为修改了时钟  
- 从NTP服务收到了一个大的时钟更新事件导致时钟漂移  
- 闰秒（是指为保持协调世界时接近于世界时时刻，由国际计量局统一规定在年底或年中或者季末对协调世界时增加或减少1秒的调整，此时一分钟为59秒或者61秒，闰秒曾使许多大型系统崩溃）

Antirez：  
- Redlock 并不需要完全一致的时钟，只需要大体一致就可以了，允许有误差。  
* 例如要计时 5s，但实际可能记了 4.5s，之后又记了 5.5s，有一定误差，但只要不超过误差范围锁失效时间即可，这种对于时钟的精度的要求并不是很高，而且这也符合现实环境。  
* 对于对方提到的时钟修改问题，Redis 作者反驳到：  
    * 手动修改时钟：不要这么做就好了。否则你直接修改 Raft 日志，那 Raft 也会无法工作…  
    * 时钟跳跃：通过恰当的运维，保证机器时钟不会大幅度跳跃（每次通过微小的调整来完成），实际上这是可以做到的

##### 4.4.2.3.1 linux系统时间和时钟漂移
为什么系统时钟会存在漂移呢？先简单说下系统时间，linux提供了两个系统时间：clock realtime和clock monotonic  
- clock realtime也就是xtime/wall time，这个时间是可以被用户改变的，被NTP改变。redis的判断超时使用的gettimeofday函数取的就是这个时间，redis的过期计算用的也是这个时间。参考https://blog.habets.se/2010/09/gettimeofday-should-never-be-used-to-measure-time.html  
- clock monotonic，直译过来是单调时间，不会被用户改变，但是会被NTP改变。  
最理想的情况是：所有系统的时钟都时时刻刻和NTP服务器保持同步，但这显然是不可能的。  
clock realtime可以被人为修改，在实现分布式锁时，不应该使用clock realtime。不过很可惜，redis使用的就是这个时间，Redis 5.0使用的还是clock realtime。Antirez说过后面会改成clock monotonic的。也就是说，人为修改redis服务器的时间，就能让redis出问题了。

#### 4.4.2.4 fencing token机制
Martin 提出一种被叫作 fencing token 的方案，保证分布式锁的正确性。流程如下：  
1. 客户端在获取锁时，锁服务可以提供一个递增的 token。  
2. 客户端拿着这个 token 去操作共享资源。  
3. 共享资源可以根据 token 拒绝后来者的请求。  
这样一来，无论 NPC 哪种异常情况发生，都可以保证分布式锁的安全性，因为它是建立在异步模型上的。而 Redlock 无法提供类似 fencing token 的方案，所以它无法保证安全性。

Antirez对于Martin提出的 fencing token 机制，也提出了质疑，主要分为 2 个问题：  
1. 第一，这个方案必须要求要操作的共享资源服务器有拒绝旧 token的能力。 
    - 例如，要操作 MySQL，从锁服务拿到一个递增数字的 token，然后客户端要带着这个 token 去改 MySQL 的某一行，这就需要利用 MySQL 的事物隔离性来做。  
    * 但如果操作的不是 MySQL 呢？例如向磁盘上写一个文件，或发起一个 HTTP 请求，那这个方案就无能为力了，这对要操作的资源服务器，提出了更高的要求。  
    * 也就是说，大部分要操作的资源服务器，都是没有这种互斥能力的。再者，既然资源服务器都有了互斥能力，那还要分布式锁干什么？所以，Antirez认为这个方案是站不住脚的。  
1. 退一步讲，即使 Redlock 没有提供 fencing token 的能力，但 Redlock 已经提供了随机值（就是前面讲的 UUID），利用这个随机值，也可以达到与 fencing token 同样的效果。

#### 4.4.2.5 结论
在Martin的这篇文章中，还有一个很有见地的观点，就是对锁的用途的区分。他把锁的用途分为两种：  
- 为了效率(efficiency)，协调各个客户端避免做重复的工作。即使锁偶尔失效了，只是可能把某些操作多做一遍而已，不会产生其它的不良后果。比如重复发送了一封同样的email。  
- 为了正确性(correctness)。在任何情况下都不允许锁失效的情况发生，因为一旦发生，就可能意味着数据不一致(inconsistency)，数据丢失，文件损坏，或者其它严重的问题。  

最后，Martin得出了如下的结论：  
- 如果是为了效率(efficiency)而使用分布式锁，允许锁的偶尔失效，那么使用单Redis节点的锁方案就足够了，简单而且效率高。Redlock则是个过重的实现(heavyweight)。  
- 如果是为了正确性(correctness)在很严肃的场合使用分布式锁，那么不要使用Redlock。它不是建立在异步模型上的一个足够强的算法，它对于系统模型的假设中包含很多危险的成分(对于timing)。那应该使用什么技术呢？Martin认为，应该考虑类似Zookeeper的方案，或者支持事务的数据库。  
* **Redlock 不伦不类**：它对于效率来讲，Redlock 比较重，没必要这么做，而对于正确性来说，Redlock 是不够安全的。

## 4.5 BLock（byte lock）
### 4.5.1 使用方式
```Go  
// 初始化client  
func initClient(servicePSM, redisPSM string) error  
  
// 创建锁  
func NewLock(dest string) (RedLock, error)   
  
// 非阻塞加锁  
// ttl：ttl时间  
// ttl <= 0，BLock托管锁，只要锁没有被主动释放，BLock会帮助用户维护锁。  
// ttl > 0, 按照指定时间设定锁的租期  
// 返回true表示加锁成功  
func (redLock *RedLock) TryLock(ctx context.Context, ttl time.Duration) (bool, error)   
  
// 阻塞加锁  
// ttl：ttl时间（单位s）  
// ttl < 0，BLock托管锁，直到用户主动释放锁  
// ttl > 0, 按照指定时间设定锁的租期  
// timeout：大于0时生效，阻塞时间，timeout和round都大于0时，优先按照timeout规则阻塞  
// round：大于0时生效，阻塞加锁次数，timeout和round不可同时<=0  
// 返回true表示加锁成功  
func (redLock *RedLock) Lock(ctx context.Context, ttl time.Duration, timeout time.Duration, round int) (bool, error)  
  
// 锁续期  
func (redLock *RedLock) Renew(ctx context.Context, ttl time.Duration) (bool, error)  
  
// 解锁  
func (redLock *RedLock) UnLock(ctx context.Context) error  
  
// 优雅退出  
func GracefulUnlock(ctx context.Context)  
```

**加分项**：  
- 非阻塞锁和阻塞锁
- 锁的超时重试实现  
* 优雅退出

### 4.5.2 原理
#### 4.5.2.1 加锁
1. 创建锁，根据资源名称，生成redis key 和 redis value（uuid）  
2. setnx方式加锁（set key value ex ttl nx），根据返回结果不同，分两种情况，  
    - 返回false，表示加锁失败，如果是非阻塞方式加锁，则直接返回false，如果为阻塞方式加锁，执行第3步骤。  
    - 返回true，表示加锁成功，执行第4步。  
    - 返回超时，尝试再次加锁，如果加锁成功，执行第4步，加锁失败，获取redis中的key，value进行校验，如果uuid与预期一致，则表示加锁成功，否则加锁失败，进入是否为阻塞锁的判断。  
3. 随机等待0-5ms后，判断是否满足重试条件（阻塞时间）执行第2步。  
4. 如果是托管方式加锁成功，则将锁写入watch dog map，以便后续的锁托管工作，如果是非托管方式加锁，直接返回true，结束。

![[67f3bbb60cd4510bce432d7c.png]]

##### 4.5.2.1.1 超时重试
流程中包含了锁的重试机制，超时可能是由于网络原因，可能是由于redis集群自身的原因，因此并不能确定收到超时响应时是否加锁失败，如果redis写入加锁数据成功，但是客户端认为加锁失败，会导致在锁的整个租期内，任何客户端线程都没有获得锁，资源使用率降低。

细心的读者可能会注意到，超时后是尝试重新加锁，而不是校验是否加锁成功，先看下图：
![[67f3bc080cd1d10bce432e6c.png]]
当第一次 lock 请求时候，客户端超时。再尝试 get 请求判断锁是否可以重入的时候，发现锁不存在。在第二次发起重试 lock 请求的时候，第一次的 lock 请求已到达且执行成功，则第二次加锁失败。会存在一个 lease time 内没有任何线程拿到这把锁 (实际server端又已经存在) 的问题。
触发的概率非常低，但是一定会遇到。所以超时重试时，先尝试加锁，加锁失败再验证是否加锁成功。

#### 4.5.2.2 Watch Dog
针对有些场景用户无法确定锁的TTL，BLock提供了看门狗机制，帮助用户维护锁，用户不需要设置锁的TTL，直到用户手动释放锁为止。看门狗机制流程如下：
1. 服务启动时，初始化Cron Job，定期维护托管锁。  
2. 判断isRunning的值，true表示前一轮任务还在执行中，当前任务直接返回，false表示前一轮任务已经结束，当前任务可正常执行。  
3. 根据存放托管锁的lock map，创建快照，这样做的目的是提高效率，不影响主流程中加锁，解锁过程中对lock map的写操作，同时isRunning置为true。  
4. 依次对持有的锁进行续期。  
5. 通过CasEx指令（原子操作），重置锁的ttl为30s。可能返回三种结果：  
	1. 锁已经被其他进程占有，跳至第6步；  
	2. 锁已经被释放，跳至第6步；  
	3. ttl重置成功。  
6. 判断lock map中是否包含该锁，如果不包含，说明该锁已经被释放，返回。如果包含该锁，抛出error，返回。  
7. 当所有的托管锁都被执行完，将isRunning置为false。  
8. 结束。
![[67f3d55c0f78510bce4334f7.png]]

#### 4.5.2.3 锁延期
通过CasEx重置锁的租期。重置锁的ttl。可能返回三种结果：  
1. 锁已经被其他进程占有，重置失败，返回error。  
2. 锁已经被释放，重置失败，返回error。  
3. ttl重置成功。

#### 4.5.2.4 解锁
通过Cad删除锁。可能返回三种结果：（表述意义，不要返回值）  
1. 锁被其他进程持有，删除失败，返回error。  
2. 锁已经被释放，删除失败，返回error。  
3. 删除成功，同时删除watchdog和LongLeaseMap中的记录。
![[67f3d5f1f8d9d10bce4337fb.png]]

#### 4.5.2.5 优雅退出 
在有些分布式场景下，由于框架或服务自身原因，当程序关闭时有些锁没有得到正确释放，导致资源无法被其他正在运行的客户端获取，必须要等待资源的上锁时间到达预设的TTL时间后，其他客户端才有可能获取到该资源。
**为了提高资源利用率，BLock提供了优雅退出的功能，在用户监听到关闭信号时，帮助释放正在持有的长租期锁（TTL > 30s）**。  
需要注意的是，对于安全性要求较高的场景需要慎重使用优雅退出功能，因为优雅退出可能导致用户线程正在运行，而锁被提前释放，其他线程获取到该锁，破坏了锁的互斥性。  
  
**BLock中使用map来维护持有的长租期锁**，加锁成功后把锁put到map中，解锁成功后将锁从map中remove。
为了防止解锁失败的长租期锁数据在map中积攒过多，应对map中已经超时的长租期锁进行定期的删除。
对于TTL时间比较短的锁，比如10s的锁，TTL小于实例的重启间隔，所以没有必要在关机的时候做释放，优雅退出只针对TTL时间大于30s的锁。

##### 4.5.2.5.1 定期删除失效锁
1. BLock初始化后会启动一个定时任务，定期对长租期锁检查  
2. 每次检查开始，会先校验标识，RunningFlag，如果RunningFlag等于true，标识前一轮的校验任务还没有运行完，此轮任务暂不执行，如果RunningFlag等于false，执行第三步  
3. 将RunningFlag置为true，标识此轮校验任务开始，同时生成一份长租期锁的map的快照，生成当前时间点CurrentTime  
4. 对快照map中的锁元素依次进行校验  
	1. 如果锁的租期小于CurrentTime，表示该锁已经过期，检查原map中是否包含该锁，如果包含，则从原map中删除该锁，如果不包含，继续校验后面的锁元素  
	2. 如果锁的租期大于CurrentTime，表示该锁为有效锁，不做操作，继续校验后面的锁元素  
5. 所有锁元素校验完毕后，将RunningFlag置为false，表示此轮校验任务运行结束
![[67f3d7e8f5a8d10bce433f54.png]]

##### 4.5.2.5.2 优雅退出
1. 当调用优雅退出接口时，先将RunningFlag置为true，阻塞长租期锁的定时检查任务。  
2. 对长租期锁的map和watch dog map中的锁依次进行释放。

##### 4.5.2.5.3 优雅退出样例
**基于kiteX框架**：
```Go  
func (s *server) Run() (err error) {  
  
   // ......  
  
   if err = s.waitSignal(errCh); err != nil {  
  
      if s.opt.Logger != nil {  
  
         s.opt.Logger.Errorf( "KITEX: received error and exit: %s" , err.Error())  
  
      }  
  
   }  
  
   for i := range onShutdown {  
  
      onShutdown[i]()  
  
   }  
  
   // stop server after user hooks  
  
  if e := s.Stop(); e != nil && err == nil {  
  
      err = e  
  
      if s.opt.Logger != nil {  
  
         s.opt.Logger.Errorf( "KITEX: stop server error: %s" , e.Error())  
  
      }  
  
   }  
  
   return  
  
}  
  
func (s *server) waitSignal(errCh chan error) error {  
  
   signals := make(chan os.Signal, 1)  
  
   signal.Notify(signals, syscall.SIGINT, syscall.SIGHUP, syscall.SIGTERM)  
  
  
  
   // ......  
  
}  
```
可以看到，waitSignal方法中监听了syscall.SIGINT, syscall.SIGHUP, syscall.SIGTERM三个新号，收到信号后，run方法结束。
所以，在main函数的最后增加优雅退出的逻辑即可:
```Go  
func main() {  
  
   // ......  
  
   err = svr.Run()  
  
  
  
   if err != nil {  
  
      log.Println(err.Error())  
  
   }  
  
     
  
   // 调用BLock优雅退出  
  
   block.GracefulUnlock(ctx)  
  
}  
```

**基于Hertz框架**：
注册钩子函数:  
```Go  
func main() {  
  
    hertz.Init()  
  
      
  
    // 退出前最长等待时间，如果不指定默认是5s  
  
    r := app.Default(config.WithExitWaitTime(30*time.Second))   
  
  
  
    r.OnShutdown = append(r.OnShutdown, func(ctx context.Context) {  
  
        logs.CtxInfo(ctx, "Stop websockets gracefully")  
  
        // 调用BLock优雅退出  
  
        block.GracefulUnlock(ctx)  
  
        <-ctx.Done()  
  
        logs.CtxInfo(ctx, "App shutdown!")  
  
    })  
  
  
  
    r.Spin()  
  
}  
```

# 5 事务
## 5.1 Redis 如何实现事务？
事务的执行过程包含三个步骤，Redis 提供了 `MULTI`、`EXEC` 两个命令来完成这三个步骤。下面我们来分析下。
1. 第一步，客户端要使用一个命令显式地表示一个事务的开启。在 Redis 中，这个命令就是 `MULTI`。
2. 第二步，客户端把事务中本身要执行的具体操作（例如增删改数据）发送给服务器端。这些操作就是 Redis 本身提供的数据读写命令，例如 `GET`、`SET` 等。不过，这些命令虽然被客户端发送到了服务器端，但 Redis 实例只是把这些命令暂存到一个命令队列中，并不会立即执行。
3. 第三步，客户端向服务器端发送提交事务的命令，让数据库实际执行第二步中发送的具体操作。Redis 提供的 `EXEC` 命令就是执行事务提交的。当服务器端收到 `EXEC` 命令后，才会实际执行命令队列中的所有命令。

下面的代码就显示了使用 `MULTI` 和 `EXEC` 执行一个事务的过程：
```bash
#开启事务
127.0.0.1:6379> MULTI
OK
#将a:stock减1，
127.0.0.1:6379> DECR a:stock
QUEUED
#将b:stock减1
127.0.0.1:6379> DECR b:stock
QUEUED
#实际执行事务
127.0.0.1:6379> EXEC
1) (integer) 4
2) (integer) 9
```
我们假设 a:stock、b:stock 两个键的初始值是 5 和 10。

## 5.2 Redis 的事务机制能保证哪些属性？
### 5.2.1 原子性
如果事务正常执行，没有发生任何错误，那么，`MULTI` 和 `EXEC` 配合使用，就可以保证多个操作都完成。但是，如果事务执行发生错误了，原子性还能保证吗？我们需要分三种情况来看。

#### 5.2.1.1 操作命令不存在
**第一种情况是，在执行 `EXEC` 命令前，客户端发送的操作命令本身就有错误**（比如语法错误，使用了不存在的命令），在命令入队时就被 Redis 实例判断出来了。
对于这种情况，在命令入队时，Redis 就会报错并且记录下这个错误。此时，我们还能继续提交命令操作。等到执行了 `EXEC` 命令之后，Redis 就会拒绝执行所有提交的命令操作，返回事务失败的结果。这样一来，事务中的所有命令都不会再被执行了，保证了原子性。

#### 5.2.1.2 命令和数据类型不匹配
我们再来看第二种情况。和第一种情况不同的是，**事务操作入队时，命令和操作的数据类型不匹配，但 Redis 实例没有检查出错误**。但是，在执行完 `EXEC` 命令以后，Redis 实际执行这些事务操作时，就会报错。不过，需要注意的是，虽然 Redis 会对错误命令报错，但还是会把正确的命令执行完。在这种情况下，事务的原子性就无法得到保证了。
```bash
#开启事务
127.0.0.1:6379> MULTI
OK
#发送事务中的第一个操作，LPOP命令操作的数据类型不匹配，此时并不报错
127.0.0.1:6379> LPOP a:stock
QUEUED
#发送事务中的第二个操作
127.0.0.1:6379> DECR b:stock
QUEUED
#实际执行事务，事务第一个操作执行报错
127.0.0.1:6379> EXEC
1) (error) WRONGTYPE Operation against a key holding the wrong kind of value
2) (integer) 8
```

Redis 中并没有提供回滚机制。虽然 Redis 提供了 `DISCARD` 命令，但是，这个命令只能用来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果。我们来看下下面的代码。
```bash
#读取a:stock的值4
127.0.0.1:6379> GET a:stock
"4"
#开启事务
127.0.0.1:6379> MULTI 
OK
#发送事务的第一个操作，对a:stock减1
127.0.0.1:6379> DECR a:stock
QUEUED
#执行DISCARD命令，主动放弃事务
127.0.0.1:6379> DISCARD
OK
#再次读取a:stock的值，值没有被修改
127.0.0.1:6379> GET a:stock
"4"
```

#### 5.2.1.3 Redis实例故障
我们再来看下第三种情况：在执行事务的 `EXEC` 命令时，Redis 实例发生了故障，导致事务执行失败。在这种情况下，如果 Redis 开启了 AOF 日志，那么，只会有部分的事务操作被记录到 AOF 日志中。我们需要使用 redis-check-aof 工具检查 AOF 日志文件，这个工具可以把未完成的事务操作从 AOF 文件中去除。这样一来，我们使用 AOF 恢复实例后，事务操作不会再被执行，从而保证了原子性。当然，如果 AOF 日志并没有开启，那么实例重启后，数据也都没法恢复了，此时，也就谈不上原子性了。

### 5.2.2 一致性
我们按照命令出错和实例故障的发生时机，分成三种情况来看。
- 情况一：命令入队时就报错。在这种情况下，事务本身就会被放弃执行，所以可以保证数据库的一致性。
- 情况二：命令入队时没报错，实际执行时报错。在这种情况下，有错误的命令不会被执行，正确的命令可以正常执行，也不会改变数据库的一致性。ACID中的一致性，从两方面来描述：从数据库层面，事物执行前后能够保证数据符合你设置的约束（如外键约束，唯一性约束，check约束等）。从应用层面，若破坏了应用层的约束，应用层利用事务回滚保证了我们的约束不被破坏，事务提供了一致性保证。这里是保证了数据的一致性，但有可能没有保证业务上的一致性。
- 情况三：EXEC 命令执行时实例发生故障。在这种情况下，实例故障后会进行重启，这就和数据恢复的方式有关了，我们要根据实例是否开启了 RDB 或 AOF 来分情况讨论下。
    - 如果我们没有开启 RDB 或 AOF，那么，实例故障重启后，数据都没有了，数据库是一致的。
    - 如果我们使用了 RDB 快照，因为 **RDB 快照不会在事务执行时执行**，所以，事务命令操作的结果不会被保存到 RDB 快照中，使用 RDB 快照进行恢复时，数据库里的数据也是一致的。
    - 如果我们使用了 AOF 日志，而事务操作还没有被记录到 AOF 日志时，实例就发生了故障，那么，使用 AOF 日志恢复的数据库数据是一致的。如果只有部分操作被记录到了 AOF 日志，我们可以使用 redis-check-aof 清除事务中已经完成的操作，数据库恢复后也是一致的。

所以，总结来说，在命令执行错误或 Redis 发生故障的情况下，Redis 事务机制对一致性属性是有保证的。

### 5.2.3 隔离性——WATCH, Pipeline
事务执行又可以分成命令入队（`EXEC` 命令执行前）和命令实际执行（`EXEC` 命令执行后）两个阶段，所以，我们就针对这两个阶段，分成两种情况来分析：
1. 并发操作在 `EXEC` 命令前执行，此时，隔离性的保证要使用 `WATCH` 机制来实现，否则隔离性无法保证；
2. 并发操作在 `EXEC` 命令后执行，此时，隔离性可以保证。

我们先来看第一种情况。一个事务的 `EXEC` 命令还没有执行时，事务的命令操作是暂存在命令队列中的。此时，如果有其它的并发操作，我们就需要看事务是否使用了 `WATCH` 机制。
`WATCH` 机制的作用是，在事务执行前，监控一个或多个键的值变化情况，当事务调用 `EXEC` 命令执行时，`WATCH` 机制会先检查监控的键是否被其它客户端修改了。如果修改了，就放弃事务执行，避免事务的隔离性被破坏。然后，客户端可以再次执行事务，此时，如果没有并发修改事务数据的操作了，事务就能正常执行，隔离性也得到了保证。

在使用事务时，建议配合 Pipeline 使用。
1. 如果不使用 Pipeline，客户端是先发一个 `MULTI` 命令到服务端，客户端收到 OK，然后客户端再发送一个个操作命令，客户端依次收到 `QUEUED`，最后客户端发送 `EXEC` 执行整个事务（文章例子就是这样演示的），这样消息每次都是一来一回，效率比较低，而且在这多次操作之间，别的客户端可能就把原本准备修改的值给修改了，所以无法保证隔离性。
2. 而使用 Pipeline 是一次性把所有命令打包好全部发送到服务端，服务端全部处理完成后返回。这么做好的好处，一是减少了来回网络 IO 次数，提高操作性能。二是一次性发送所有命令到服务端，服务端在处理过程中，是不会被别的请求打断的（Redis单线程特性，此时别的请求进不来），这本身就保证了隔离性。我们平时使用的 Redis SDK 在使用开启事务时，一般都会默认开启 Pipeline 的，可以留意观察一下。

没有配合 Pipeline 使用，如果想要保证隔离性，需要使用 `WATCH` 命令保证。但如果使用了 Pipeline 一次发送所有命令到服务端，那么就不需要使用 `WATCH` 了，因为服务端本身就保证了隔离性。
那 `WATCH` 还有没有使用的必要？答案是有的。对于一个资源操作为读取、修改、写回这种场景，如果需要保证事物的隔离性，此时就需要用到 `WATCH` 了。

### 5.2.4 持久性
因为 Redis 是内存数据库，所以，数据是否持久化保存完全取决于 Redis 的持久化配置模式。
如果 Redis 没有使用 RDB 或 AOF，那么事务的持久化属性肯定得不到保证。
如果 Redis 使用了 RDB 模式，那么，在一个事务执行后，而下一次的 RDB 快照还未执行前，如果发生了实例宕机，这种情况下，事务修改的数据也是不能保证持久化的。
如果 Redis 采用了 AOF 模式，因为 AOF 模式的三种配置选项 no、everysec 和 always 都会存在数据丢失的情况，所以，事务的持久性属性也还是得不到保证。
所以，不管 Redis 采用什么持久化模式，事务的持久性属性是得不到保证的。

## 5.3 总结
Redis 的事务机制可以保证一致性和隔离性，但是无法保证原子性、持久性。

# 6 引用