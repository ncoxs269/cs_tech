2025-11-27 15:41
Status: #idea
Tags: [[Go]]

# 1 pprof 使用指南
## 1.1 简介
pprof 是 Go 内置的**性能分析工具**，支持对 CPU、内存、协程、阻塞、锁竞争等维度的性能数据进行采集、分析和可视化，是排查 Go 程序性能问题（如协程泄漏、CPU 占用高、内存溢出）的核心工具。

 pprof 基于 Google 的 `pprof` 可视化工具，Go 内置 `net/http/pprof` 包（用于暴露性能数据）和 `runtime/pprof` 包（用于程序内手动采集）。它支持两种工作模式：
- **采样模式**（默认）：对目标指标（如 CPU、内存）进行周期性采样（非精确统计，但开销低，适合生产环境）；
- **精确模式**：通过手动埋点统计（如阻塞时间、锁竞争，开销略高，但数据精确）；

支持的核心指标（对应 `/debug/pprof` 端点）：

|端点|作用|常用场景|
|---|---|---|
|`/goroutine?debug=2`|查看所有协程的详细栈信息（含状态）|协程泄漏、协程阻塞排查|
|`/profile?seconds=N`|采集 N 秒内的 CPU 采样数据|CPU 占用高、耗时函数排查|
|`/heap`|采集内存分配 / 使用数据（堆内存）|内存泄漏、内存占用高排查|
|`/block`|采集 goroutine 阻塞等待数据（如 channel、锁）|阻塞性能瓶颈排查|
|`/mutex`|采集锁竞争数据|锁冲突导致的性能下降排查|
|`/allocs`|采集所有内存分配历史（包括已释放的）|内存分配频率过高排查|
|`/threadcreate`|采集系统线程创建数据|线程泄漏排查|

## 1.2 实操教程：从「配置→采集→分析」全流程
### 1.2.1 第一步：开启 pprof 数据暴露（两种方式）
#### 1.2.1.1 方式 1：HTTP 方式（推荐生产环境，无需修改业务代码）
通过 `net/http/pprof` 包自动注册 `/debug/pprof` 端点，只需在程序中导入包并启动 HTTP 服务：
```go
package main

import (
    "log"
    "net/http"
    _ "net/http/pprof" // 自动注册端点，无需额外代码
    "time"
)

// 模拟一个耗时函数（用于后续 CPU 分析）
func busyWork() {
    sum := 0
    for i := 0; i < 100000000; i++ {
        sum += i
    }
}

func main() {
    // 启动 HTTP 服务，暴露 6060 端口（默认端点 /debug/pprof）
    go func() {
        log.Printf("pprof server start at :6060")
        log.Fatal(http.ListenAndServe(":6060", nil))
    }()

    // 模拟业务逻辑：持续执行耗时操作
    for {
        busyWork()
        time.Sleep(100 * time.Millisecond)
    }
}
```

#### 1.2.1.2 方式 2：程序内手动采集（适合无 HTTP 服务的场景，如 CLI 工具）
通过 `runtime/pprof` 包手动创建性能数据文件，后续用 `go tool pprof` 分析：
```go
package main

import (
    "os"
    "runtime/pprof"
    "time"
)

func busyWork() {
    sum := 0
    for i := 0; i < 100000000; i++ {
        sum += i
    }
}

func main() {
    // 1. 采集 CPU 数据（持续 10 秒）
    cpuFile, err := os.Create("cpu.pprof")
    if err != nil {
        log.Fatal("create cpu.pprof failed:", err)
    }
    defer cpuFile.Close()
    pprof.StartCPUProfile(cpuFile)
    defer pprof.StopCPUProfile()

    // 2. 采集内存数据（程序结束时写入）
    memFile, err := os.Create("mem.pprof")
    if err != nil {
        log.Fatal("create mem.pprof failed:", err)
    }
    defer memFile.Close()
    defer pprof.WriteHeapProfile(memFile)

    // 模拟业务逻辑
    for i := 0; i < 10; i++ {
        busyWork()
        time.Sleep(100 * time.Millisecond)
    }
}
```
运行程序后，会在当前目录生成 `cpu.pprof` 和 `mem.pprof` 文件。

### 1.2.2 第二步：核心用法：用 `go tool pprof` 分析数据
`go tool pprof` 是命令行工具，支持两种数据来源：
- 远程 HTTP 端点（`go tool pprof http://localhost:6060/debug/pprof/XXX`）；
- 本地文件（`go tool pprof cpu.pprof`）。

以下以「远程 HTTP 方式」为例，讲解常用场景的分析流程。

#### 1.2.2.1 场景 1：排查协程泄漏
核心是查看协程的栈信息和状态，步骤：
1. **实时查看协程总数和状态**：
	- 在浏览器输入 `http://localhost:6060/debug/pprof/goroutine?debug=1`，会展示总的协程数和每个协程的核心函数调用栈
	- 在浏览器输入 `http://localhost:6060/debug/pprof/goroutine?debug=2`，会展示每个协程的ID和状态（running、sleep等），还有完整调用栈。调用栈最下面还会显示 `created by`，表示它是被哪个协程创建的
2. **交互模式分析协程**：
	```bash
	go tool pprof http://localhost:6060/debug/pprof/goroutine
	```
	交互模式下常用命令：

| 命令         | 作用                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `top`      | 按协程数量排序，显示前 10 个函数（创建协程最多的函数）                                                                                                                                                                                                                                                                                                                                                                                                                         |
| `list 函数名` | 查看该函数的代码，标注创建协程的位置                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| `web`      | 生成协程调用链火焰图（需安装 Graphviz），注意不要在 GoLand 的终端中调用，可能不行<br>图片展示的是Goroutine 的函数调用关系 + 数量分布联合图：<br>- 矩形节点代表一个函数<br>- 节点内的数字格式是「`协程数 (占总协程的百分比)`」（总协程数看你图顶部的`Total: 5 goroutines`）<br>- 箭头代表函数调用关系：箭头从「调用者函数」指向「被调用者函数」<br>- 箭头旁的数字代表「通过这条调用路径运行的协程数量」<br>- 节点颜色（浅红）通常是「协程数量较多的函数」<br><br>这个 SVG 图需要和`debug=2`的协程栈信息配合看，效率最高：<br>1. 先看「占比高的节点」：去`debug=2`的栈信息里根据函数名找这些协程的状态<br>2. 追踪「调用路径」：从图的顶部函数往下看，能找到协程的「创建源头」（比如哪个业务入口函数启动了这些协程）—— 这能帮你定位 “泄漏协程是从哪段代码来的”。 |
| `traces`   | 生成协程调用轨迹图（可视化阻塞关系）                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| `help`     | 帮助                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| `exit`     | 退出                                                                                                                                                                                                                                                                                                                                                                                                                                                    |

#### 1.2.2.2 场景 2：排查 CPU 占用高（耗时函数）
1. **采集 CPU 数据**（默认采集 30 秒，可通过 `seconds=N` 指定时长）：
```bash
# 采集 10 秒内的 CPU 数据，进入交互模式
go tool pprof http://localhost:6060/debug/pprof/profile?seconds=10
```
2. **交互模式分析 CPU**：
	- `top`：按 CPU 耗时占比排序，显示前 N 个函数（`flat` 是函数自身耗时，`cum` 是函数 + 子函数总耗时）；
	- `list busyWork`：查看 `busyWork` 函数的代码，标注每行的 CPU 耗时占比；
	- `web`：生成 CPU 火焰图（横向越长，耗时越高），直观定位耗时函数。

#### 1.2.2.3 场景 3：排查内存泄漏（堆内存占用高）
1. **采集内存数据**：
	```bash
	# 进入堆内存分析交互模式（默认采集当前堆内存使用情况）
	go tool pprof http://localhost:6060/debug/pprof/heap
	```
2. **交互模式分析内存**：
	- `top`：按内存占用排序，显示前 N 个函数。默认展示的内存指标是`inuse_space`，当前使用的内存。可以通过 `sample_index` 切换关键指标：
	    - `inuse_space`：当前堆内存使用量（未释放）——排查「内存占用高 / 内存泄漏」
	    - `inuse_objects`：当前存活的对象数量——排查「对象创建过多」
	    - `alloc_space`：累计分配的堆内存（包括已释放的）——排查「内存分配频率过高」
	    - `alloc_objects`：累计分配的对象数量——排查「频繁创建短生命周期对象」
	    - 也可以在启动 pprof 时，通过 `-sample_index=inuse_objects` 参数直接指定要查看的指标，无需进入交互模式后切换
	- `list 函数名`：查看函数中内存分配的代码行（如 `make`、`new` 操作）；
	- `web`：生成内存分配火焰图，定位内存分配频繁的函数。

#### 1.2.2.4 场景 4：排查阻塞问题（channel / 锁等待）
1. **采集阻塞数据**：
```bash
go tool pprof http://localhost:6060/debug/pprof/block
```
2. **交互模式分析**：
    - `top`：按阻塞时长排序，显示前 N 个函数；
    - `list 函数名`：查看阻塞发生的代码行（如 `<-ch`、`wg.Wait()`）；
    - `traces`：可视化阻塞链，查看哪个协程导致了阻塞。
3. **前提条件**：`debug=2` 是直接读取协程的实时状态（不依赖采样），block profile 是基于「采样机制」采集阻塞事件
	- **默认采样率为 0**：即不采集任何阻塞事件，需要通过 `runtime.SetBlockProfileRate(n)` 手动开启（`n=1` 表示采集所有阻塞事件，`n=10` 表示每 10 次阻塞采样 1 次）；
	- **阻塞时间阈值**：默认只记录阻塞时间超过 `10ms` 的事件（可通过 `runtime.SetBlockProfileRate` 间接控制，但核心是先开启采样）。

### 1.2.3 第三步：可视化工具（火焰图 / 调用图）
pprof 的命令行模式不够直观，推荐用「可视化工具」快速定位问题，核心是安装 **Graphviz**（绘图工具）。

在 pprof 交互模式下，输入以下命令自动生成图表（会打开默认浏览器）：
- `web`：生成调用链火焰图（适合 CPU / 内存 / 协程）；
- `svg`：生成 SVG 格式图表（可保存到文件：`svg > cpu.svg`）；
- `png`：生成 PNG 格式图表（`png > mem.png`）。

## 1.3 进阶技巧（生产环境必备）
### 1.3.1 生产环境安全配置
- **限制端口访问**：pprof 端点会暴露程序敏感信息（栈、函数名），生产环境需通过防火墙限制访问 IP（如只允许内网 IP）；
- **添加认证**：给 HTTP 服务添加 Basic Auth 认证，避免未授权访问：
```go
// 示例：给 pprof 服务添加 Basic Auth
func main() {
    mux := http.NewServeMux()
    // 注册 pprof 端点（默认是 http.DefaultServeMux，这里手动注册到自定义 mux）
    mux.Handle("/debug/pprof/", http.HandlerFunc(pprof.Index))
    mux.Handle("/debug/pprof/cmdline", http.HandlerFunc(pprof.Cmdline))
    mux.Handle("/debug/pprof/profile", http.HandlerFunc(pprof.Profile))
    mux.Handle("/debug/pprof/symbol", http.HandlerFunc(pprof.Symbol))
    mux.Handle("/debug/pprof/trace", http.HandlerFunc(pprof.Trace))

    // 添加 Basic Auth 中间件
    authMux := basicAuth(mux, "admin", "123456")

    log.Fatal(http.ListenAndServe(":6060", authMux))
}

// basicAuth 中间件实现
func basicAuth(h http.Handler, username, password string) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        user, pass, ok := r.BasicAuth()
        if !ok || user != username || pass != password {
            w.Header().Set("WWW-Authenticate", `Basic realm="pprof"`)
            w.WriteHeader(http.StatusUnauthorized)
            return
        }
        h.ServeHTTP(w, r)
    })
}
```

### 1.3.2 远程采集生产环境数据（无直接访问权限）
若生产环境不允许直接访问 6060 端口，可通过 `kubectl port-forward`（K8s 环境）或 `ssh 端口转发` 打通通道：
```bash
# 示例 1：K8s 环境，将 pod 的 6060 端口转发到本地 6060
kubectl port-forward pod/your-pod-name 6060:6060

# 示例 2：SSH 端口转发，将远程服务器的 6060 端口转发到本地 6060
ssh -L 6060:localhost:6060 user@remote-server-ip
```
之后直接访问 `http://localhost:6060/debug/pprof` 即可采集数据。

### 1.3.3 持续性能监控（Prometheus+Grafana）
pprof 适合「问题排查」，若需「长期监控」，可将 pprof 指标导出到 Prometheus，用 Grafana 可视化：
- 使用 `prometheus/client_golang` 库，采集 `runtime_num_goroutine`、`go_gc_duration_seconds` 等指标；
- 导入 Grafana 官方 Go 性能监控面板（ID：10823），可直观查看协程数、CPU、内存、GC 等趋势。

### 1.3.4 工具辅助
- **pprof 可视化 Web UI**：Go 1.19+ 支持 `go tool pprof -http=:8080 数据来源`，直接启动 Web 界面，无需交互命令：
```bash
# 启动 Web UI 分析协程数据（访问 http://localhost:8080 即可）
go tool pprof -http=:8080 http://localhost:6060/debug/pprof/goroutine
```
- **go-torch**：生成更美观的火焰图（支持 CPU / 内存 / 协程），github：[https://github.com/uber/go-torch](https://github.com/uber/go-torch)。

# 2 问题排查第一步
- QPS是否升高？
- 之前有发版吗？
- 请求数据是否很大？
- 用户做了一些很重（例如查询大范围数据）的操作吗？

# 3 协程泄漏问题排查
协程泄漏的本质是「goroutine 无法进入退出状态」，常见原因是：**阻塞在无退出条件的操作上**（如 channel、sync 原语、IO）或**无限循环无终止逻辑**。排查需按「**确认现象→收集数据→定位阻塞点→分析根因**」的顺序推进。

## 3.1 第一步：确认现象与数据收集
### 3.1.1 确认现象：先验证是否真泄漏
- **关联系统状态**：观察 CPU、内存、GC 情况：
    - ==若协程激增但 CPU 使用率低：大概率是协程阻塞（如 channel、WaitGroup）==；
    - ==若 CPU 和协程同步增长：可能是无限循环（如无退出条件的 for 循环）==。
- **量化协程数量**：通过以下方式获取实时 goroutine 数，确认是否持续增长（而非正常波动）：
	- 代码内埋点：用 `runtime.NumGoroutine()` 打印当前协程数（如日志输出、监控暴露）；
	- 监控面板：通过 Prometheus+Grafana 暴露指标（推荐使用 `prometheus/client_golang` 库，采集 `runtime_num_goroutine` 指标）；
	- 即时查询：通过 pprof 实时获取，执行命令

### 3.1.2 收集关键数据
核心是获取「泄漏协程的栈信息」，明确它们在做什么、阻塞在哪里。
#### 3.1.2.1 工具 1：pprof（首选，最直接）
使用 pprof 实时查看 goroutine 栈，可以先用 debug=2 查看协程状态和阻塞位置，然后用可视化火焰图查看调用链。哪些地方创建了很多协程、协程阻塞在哪里。

#### 3.1.2.2 工具 2：日志与上下文追踪
- 在 goroutine 创建时添加日志（记录 goroutine ID、业务场景）：
```go
// 注意：Go无直接获取goroutine ID的API，需通过runtime.Stack间接获取
func getGoroutineID() uint64 {
    var buf [64]byte
    runtime.Stack(buf[:], false)
    var id uint64
    fmt.Sscanf(string(buf[:]), "goroutine %d ", &id)
    return id
}

// 业务中创建协程时打印
go func() {
    log.Printf("goroutine start: id=%d, task=xxx", getGoroutineID())
    // 业务逻辑...
    log.Printf("goroutine exit: id=%d", getGoroutineID()) // 若未输出，说明泄漏
}()
```
- 若使用分布式追踪（如 Jaeger、Zipkin），可将 goroutine ID 与追踪 ID 绑定，定位泄漏协程对应的业务请求。

#### 3.1.2.3 工具 3：runtime/debug 包
在程序中主动 dump 栈信息（适用于生产环境无法直接执行 pprof 命令的场景）：
```go
// 当协程数超过阈值时，dump所有goroutine栈到文件
if runtime.NumGoroutine() > 10000 { // 警戒线，根据业务调整
    f, err := os.Create("goroutine_dump_" + time.Now().Format("20060102150405") + ".log")
    if err != nil {
        log.Printf("dump goroutine stack failed: %v", err)
        return
    }
    defer f.Close()
    debug.WriteGoroutineStacks(f)
    log.Printf("dump goroutine stack to file, current count: %d", runtime.NumGoroutine())
}
```

## 3.2 第二步：定位根因 —— 常见协程泄漏场景与识别
### 3.2.1 互斥锁未释放
可能由于忘记调用 `Unlock()`、临界区 panic 导致 `Unlock()` 未执行。

识别：
pprof 中协程状态为 `blocked`，栈信息显示阻塞在 `sync.Mutex.Lock`，例如：
```
goroutine 123 [blocked]:
sync.(*Mutex).Lock(0xc0000b4000)
    /usr/local/go/src/sync/mutex.go:81 +0x8a
main.processData(...)
    /path/main.go:66 +0x3c
created by main.main
    /path/main.go:22 +0x7d
```

解决方案：
- 强制用 `defer` 释放锁（核心规范）
- 避免在临界区执行可能 panic 的逻辑

### 3.2.2 死锁
死锁是「多个协程互相持有对方需要的资源，形成循环等待」，导致所有参与的协程**永久阻塞**（属于更严重的协程泄漏，且会导致业务逻辑卡死）。常见触发条件：
1. 至少两个协程；
2. 至少两个资源（锁、channel 等）；
3. 协程间互相持有资源并等待对方释放。

识别：
1. **pprof 栈信息**：多个协程阻塞在「资源等待」（如 `Mutex.Lock`、`chan receive`），且等待的资源被其他阻塞协程持有；
2. **程序表现**：CPU 使用率极低（协程都阻塞），业务功能卡死，协程数持续增长（若有新协程不断进入死锁循环）；
3. **工具检测**：使用 `go-deadlock` 库（替换 `sync.Mutex`），运行时自动检测死锁并打印详细日志。

解决方案：
- **统一资源获取顺序**（避免循环等待）：若多个协程需要获取多把锁 / 资源，强制按「固定顺序」获取（如按变量名字典序、资源 ID 升序），打破循环等待
- **给资源等待添加超时**：用 `context.WithTimeout` 或 `select+time.After` 限制资源等待时间，超时后释放已持有资源并退出协程
- **使用工具提前检测死锁**：开发 / 测试环境用 `go-deadlock` 库替换 `sync.Mutex`/`sync.RWMutex`，自动检测死锁并打印调用栈

### 3.2.3 锁竞争激烈导致大量协程阻塞
高并发场景下，**大量协程争抢同一把细粒度不足的锁**（如全局`sync.Mutex`），且锁的「持有时间较长」（如临界区包含 IO 操作、复杂计算），导致：
1. 锁释放频率远低于协程争抢频率，阻塞在`Lock()`的协程持续积压，形成「阻塞队列爆炸」；
2. 这些阻塞协程无法正常执行后续逻辑、无法退出，长期占用内存和协程调度资源，本质是「功能性协程泄漏」（虽锁最终会释放，但阻塞队列积压导致资源耗尽）。

程序表现：
- 协程数（`runtime.NumGoroutine()`）随 QPS 增长而暴涨（如 QPS 1000 时协程数破万）；
- 系统态 CPU（sys%）高（因协程在锁等待时会自旋尝试获取锁，消耗 CPU）；
- 接口响应时间（P99）骤升（大量请求阻塞在锁等待，无法进入临界区）。

解决方案：**减少锁竞争的 “强度” 和 “时长”**，通过「锁粒度拆分、读写分离、无锁设计」降低阻塞协程数。
1. 锁粒度拆分（核心方案）：将全局锁拆分为「局部锁 / 分片锁」，让不同请求争抢不同的锁，减少单锁的竞争压力
2. 读写分离（`sync.RWMutex`）：若业务中「读操作远多于写操作」，用 `sync.RWMutex` 替代 `sync.Mutex`
3. 缩短锁持有时间：将「耗时操作（如 IO、计算）移出临界区」，仅在必要时持有锁。
4. 无锁设计（彻底避免锁竞争）：用「原子操作」或「无锁数据结构」替代锁，彻底消除竞争
5. 协程池限流（兜底方案）：若锁竞争无法避免，用「协程池」限制并发协程数，避免阻塞队列无限制增长

### 3.2.4 无限循环无退出条件
协程内的 `for` 循环缺少退出逻辑（如未监听 `context.Done()`、未判断终止条件），导致协程永久运行。

识别：
pprof 栈信息中协程状态为 `running`，且栈跟踪指向循环逻辑。

解决方案：
循环中必须添加退出条件（如 `context.Done()`、计数器、外部信号）

### 3.2.5 channel使用不当
例如：
- 对nil channel进行读写
- 无缓冲 Channel「只发不收」或「只收不发」
- 带缓冲 Channel「缓冲满后持续发送」或「缓冲空后持续接收」

识别：
pprof 栈信息中出现 `chan send` 或 `chan receive` 阻塞，例如：
```
goroutine 123 [chan send]:
main.taskFunc(...)
    /path/main.go:45 +0x42
created by main.main
    /path/main.go:20 +0x8a
```

解决方案：
- 确保发送和接收成对出现；
- 若不确定接收者是否存在，用 `select+default` 避免阻塞
- 用带缓冲 Channel（需合理设置缓冲大小，避免缓冲满导致发送阻塞）；
- 用 `context.Context` 控制超时 / 取消
- 用「有界通道 + 限流」避免生产者压满缓冲（如使用 `golang.org/x/sync/semaphore` 限流）。

### 3.2.6 sync.WaitGroup 使用不当（Add/Done 不匹配）
`WaitGroup` 用于等待一组协程完成，若 `Add(n)` 后，`Done()` 调用次数少于 `n`，则 `Wait()` 会永久阻塞；若 `Done()` 调用次数多于 `Add(n)`，会导致 panic。

识别：
pprof 栈信息中出现 `sync.WaitGroup.Wait` 阻塞，例如：
```
goroutine 456 [sync.WaitGroup.Wait]:
sync.(*WaitGroup).Wait(0xc0000b4000)
    /usr/local/go/src/sync/waitgroup.go:136 +0x54
main.batchTask(...)
    /path/main.go:89 +0x12a
```

解决方案：
- 严格保证 `Add(n)` 与 `Done()` 次数一致，`Done()` 必须放在 `defer` 中（避免业务逻辑 panic 导致未调用）；
- 避免在循环中动态 `Add`（易出错），建议先计算协程数，一次性 `Add`；
- 若协程可能提前退出，需确保每个协程无论成功失败都调用 `Done()`。

### 3.2.7 select 语句无 default 分支，且所有 case 均无法触发
例如：
- select 监听未使用的 channel
- select 监听已关闭但无数据的 channel

识别：
协程状态为 `blocked`，栈信息显示阻塞在 `select` 语句，例如：
```
goroutine 456 [blocked]:
main.waitForData(...)
    /path/main.go:55 +0x90
created by main.main
    /path/main.go:25 +0x6a

// 对应代码第55行：select 无 default，所有 case 无法触发
```

解决：
- 根据场景选择是否加 default 分支：
    - 若期望「立即返回，不阻塞」：加 `default` 分支处理「无 case 触发」的情况；
    - 若期望「等待某个 case 触发，但需避免永久阻塞」：用 `context` 或 `time.After` 加超时。
- 确保至少一个 case 能触发，或者用 `context.Done()` 作为退出 case（推荐所有业务协程都加）

### 3.2.8 context 使用不当
`context.Context` 是协程间传递取消信号、超时控制的核心机制。若：
- 协程未接收 `context.Done()` 信号；
- 父 context 未取消 / 超时，子协程无退出条件；
- 未将 context 传递给子协程。
会导致协程永久运行。

识别：
pprof 栈信息中协程处于「运行中」或阻塞在业务逻辑（如循环），无 `context.Done()` 相关逻辑。

解决方案：
- 所有业务协程必须接收 `context.Context`，并通过 `select` 监听 `Done()` 信号；
- 父协程退出时，通过 `context.WithCancel` 主动取消子协程；
- 耗时操作（如 IO、循环）必须设置超时（`context.WithTimeout`）

### 3.2.9 Timer 泄漏（time.After 使用不当）
`time.After(d)` 会创建一个定时器，1 次触发后释放，但如果在循环中使用（如 `select` 里的 `case <-time.After(1*time.Second)`），每次循环都会创建新的 Timer，旧 Timer 需等待触发后才会被 GC，若循环频率高，会导致大量 Timer 和协程泄漏。

识别：
pprof 栈信息中出现大量 `time.sleep` 相关协程，例如：
```
goroutine 101 [sleep]:
time.Sleep(...)
    /usr/local/go/src/runtime/time.go:195 +0x105
time.AfterFunc.func1()
    /usr/local/go/src/time/sleep.go:176 +0x36
```

解决方案：
- 循环中使用 `time.NewTimer` 并手动 `Reset`，避免重复创建：
```go
func main() {
    go func() {
        timer := time.NewTimer(1 * time.Second)
        defer timer.Stop()
        for {
            select {
            case <-timer.C:
                log.Println("tick")
                timer.Reset(1 * time.Second) // 重置定时器
            }
        }
    }()
}
```
- 若需超时控制，优先使用 `context.WithTimeout` 而非 `time.After`。

### 3.2.10 IO 操作无超时（网络、数据库、Redis 等）
Go 的 IO 操作（如 `http.Get`、`sql.Query`、`redis.Do`）默认是**阻塞的**，若对方服务无响应、网络超时，协程会永久阻塞在 IO 等待上。

识别：
pprof 栈信息中阻塞在 IO 相关函数，例如：
```
goroutine 789 [IO wait]:
net.(*netFD).Read(...)
    /usr/local/go/src/net/fd_unix.go:202 +0x2a
net/http.(*conn).readRequest(...)
    /usr/local/go/src/net/http/server.go:1073 +0x12c
```

解决方案：所有 IO 操作必须设置超时：
- HTTP 请求：用 `http.Client{Timeout: 5 * time.Second}`；
- 数据库（如 MySQL）：连接时设置 `timeout`、`readTimeout`、`writeTimeout`；
- Redis（如 redigo）：用 `SetReadTimeout`、`SetWriteTimeout`，或通过 context 控制；
- RPC请求：需要设置超时，且超时时长不要太长，避免高并发下大量协程阻塞

## 3.3 第三步：解决泄漏后的验证
修复后需验证协程数量是否恢复正常：
1. 观察监控面板：协程数是否稳定在合理范围（无持续增长）；
2. 重复执行压测：模拟高并发场景，确认协程数不会随请求量无限增加；
3. 再次 dump 栈信息：通过 pprof 确认泄漏的协程已退出，无新增阻塞协程。

## 3.4 第四步：预防措施（长期避免泄漏）
### 3.4.1 编码规范（强制执行）
- 所有协程必须有「明确退出条件」：要么监听 `context.Done()`，要么有业务终止逻辑；
- 所有 IO 操作（HTTP、数据库、Redis、MQ）必须设置超时；
- Channel 使用原则：「发送方负责关闭，接收方负责判断关闭」，避免关闭后发送或接收；
- `sync.WaitGroup` 规范：`Done()` 必须放在 `defer` 中，`Add` 次数与协程数严格一致；
- 禁止在循环中创建无控制的协程（如 `for range data { go func() {}() }`），需用协程池（如 `ants` 库）限制并发数。

### 3.4.2 监控告警
- 暴露 `runtime_num_goroutine` 指标到 Prometheus，设置阈值告警（如超过 1 万触发告警）；
- 监控阻塞协程数：通过 pprof 采集 `goroutine:blocked` 指标，异常增长时告警；
- 监控 IO 超时率：若某类 IO 操作超时率激增，可能伴随协程泄漏。

### 3.4.3 测试与审查
- 单元测试：检测协程泄漏，例如：
```go
func TestTaskNoLeak(t *testing.T) {
    before := runtime.NumGoroutine()
    ctx, cancel := context.WithCancel(context.Background())
    defer cancel()
    // 执行被测函数（会创建协程）
    runTask(ctx)
    time.Sleep(2 * time.Second) // 等待协程退出
    after := runtime.NumGoroutine()
    if after > before+1 { // 允许1个协程误差（如pprof服务）
        t.Errorf("goroutine leak: before=%d, after=%d", before, after)
    }
}
```
- 代码审查：重点审查协程创建处（`go func()`），确认每个协程都有退出条件。

### 3.4.4 工具辅助
- 使用 `golangci-lint` 等静态检查工具，检测潜在的协程泄漏（如未使用的 Channel、WaitGroup 不匹配）；
- 生产环境开启 pprof 服务（限制访问权限），便于问题快速排查。

## 3.5 总结
- 首先需要确认是否协程泄漏和泄露的大致原因：如果协程数很高但是CPU不高，那么可能是有很多协程被阻塞了；如果协程数很高并且CPU也很高，那么可能是协程在低效循环或复杂计算
- 然后用 pprof 看一下协程的调用链路，看哪些地方创建了很多协程、协程阻塞在哪里
- 协程泄漏的原因主要有
	- 锁使用不当：例如有协程调用了lock，但是未调用unlock
	- 死锁问题：协程需要请求多个资源，不同协程间持有部分，导致循环依赖
		- 对多个资源的申请需要按照一定顺序进行
	- 锁竞争激烈：锁的力度太大，或者锁的时间很长，导致大量协程阻塞
		1. 减少锁的粒度
		2. 将复杂计算和IO从锁的临界区中移走
		3. 如果读比写操作多很多，可以用读写锁
		4. 用原子操作或无锁数据结构替代锁
		5. 协程池限流（兜底方案）：若锁竞争无法避免，那需要做好限流，避免协程一直增长
	- 无限循环无退出条件，导致协程一直在运行
	- channel 使用不当：例如无缓存channel发送、接受数量不匹配，导致协程阻塞
	- WaitGroup 使用不当：Add、Done操作数量不一致，导致协程阻塞在Wait
	- select使用不当，所有分支都被阻塞，有没有default，导致协程阻塞
	- Timer泄漏：例如在循环中使用time.After，创建大量的协程
	- IO 操作无超时：例如网络、数据库的操作没有设置超时时间，导致协程一直阻塞
	- context 使用不当：例如父协程通过context发送取消命令，但是子协程没有去监听，导致子协程一直不能停下来
- 预防措施：
	- 编码规范：可以依赖工具执行
	- 用协程池避免协程无限增长

# 4 CPU暴涨问题排查
线上 CPU 使用率暴涨是 Go 服务常见的性能问题，排查核心是**先定位 “CPU 高耗的进程 / 线程”，再锁定 “具体耗时的代码逻辑”**，同时要区分 “用户态（user）” 和 “内核态（sys）”CPU 高的不同原因。

## 4.1 第一步：确认 CPU 暴涨的现象（先搞清楚 “是什么问题”）
1. **看监控面板（优先）**：
    - 通过 Prometheus+Grafana 查看：
        - 服务的`process_cpu_seconds_total`（CPU 总耗时）、`process_cpu_usage`（CPU 使用率）；
        - **区分`user`（用户态）和`sys`（内核态）占比**：
            - `user高`：业务逻辑计算 / 循环导致；
            - `sys高`：协程调度、锁竞争、系统调用导致。
    - **确认是 “单实例高” 还是 “全集群高**”：单实例可能是节点异常，全集群可能是流量 / 业务逻辑问题。
2. **服务器端工具验证**：
    - 用`top`命令看进程 CPU 占比：找到自身服务的 PID（比如 Go 服务进程），看`%CPU`是否接近 100%/ 多核总和；
    - 用`pidstat -p [PID] 1 3`看进程内线程的 CPU：若某线程`%CPU`特别高，说明该线程对应的协程逻辑有问题；
    - 用`mpstat -P ALL 1 3`看 CPU 核的负载：是否某核被占满（比如死循环），还是多核均高（比如并发计算）。

## 4.2 第二步：采集 CPU 性能数据（核心用 Go pprof）
若服务未开启 pprof，用`pidstat -u -t -p [PID] 1 10`采集线程级 CPU 数据，记录高耗线程 ID，后续结合日志定位。

## 4.3 第三步：分析数据，定位耗时代码（pprof 交互模式）
进入 pprof 交互模式后，用以下命令定位根因：
1. **`top`：看耗时前 N 的函数**：
	- `flat%`：函数自身的 CPU 耗时占比；
	- `cum%`：函数 + 子函数 ** 的总耗时占比。
2. **`list [函数名]`：看具体代码行的耗时**
3. **`web`：生成火焰图（直观看调用链）**：横向越长的函数耗时越高，能快速找到调用链中的高耗节点。

## 4.4 常见导致 CPU 暴涨的原因（对应场景 + 定位点）
### 4.4.1 业务逻辑：无限 / 低效循环（user 占比高）
代码中循环条件错误（如`for ;;`）、循环终止条件永远为真（如`for i < 100000000`写成`for i > 100000000`）、大数据量无终止遍历。
`top`中业务函数的`flat%`极高（比如占 50%+），`list`能看到循环代码行耗时占比 100%。

### 4.4.2 业务逻辑：高频复杂计算（user 占比高）
大数据量排序 / 去重、高频序列化（如 JSON/Protobuf 序列化 GB 级对象）、复杂正则表达式（回溯导致 CPU 爆炸，比如`(a+)+`匹配长字符串）。
`top`中对应计算函数（如`sort.Sort`、`json.Marshal`）的`cum%`极高。

### 4.4.3 重试逻辑失控（user 占比高）
外部服务不可用（如 API 超时），重试逻辑无限制（比如`while !success { retry() }`），每次重试都执行计算逻辑。
`top`中重试函数（如`retryRequest`）的`cum%`高，同时日志中能看到大量重试请求。

### 4.4.4 并发问题：协程过多导致调度开销（sys 占比高）
无限制创建协程（比如`for range reqs { go handleReq() }`），导致 Go 调度器（M-P-G 模型）的调度开销增大（内核态 CPU 高）。
`top`中`runtime.schedule`、`runtime.mstart`等调度函数的`sys%`高，同时`runtime.NumGoroutine()`显示协程数过万。

### 4.4.5 并发问题：锁竞争 / 原子操作频繁（sys 占比高）
大量协程争抢同一把`sync.Mutex`（自旋锁 / 互斥锁等待），或高频调用`sync/atomic`操作（CPU 缓存一致性开销）。
`top`中`sync.(*Mutex).Lock`、`runtime.syncatomic`等函数的`flat%`高。

### 4.4.6 GC 频繁触发（user/sys 占比高）
高频创建短生命周期对象（比如在循环中`make([]byte, 1024)`）、大内存对象频繁分配，导致 GC 频繁执行（标记 - 清除开销）。
`top`中`runtime.gcBgMarkWorker`、`runtime.mallocgc`等 GC 相关函数的占比高，同时监控中`go_gc_duration_seconds`的次数 / 耗时激增。

## 4.5 总结
- 首先要确认CPU暴涨的现象
	- sys、usr哪个占用高：sys占用高，说明可能是锁竞争激烈、原子操作频繁、GC频繁等问题；usr占用高，说明可能是低效循环、复杂计算等问题
	- 单机器or多机器占用高：单机器占用高，说明可能是机器配置问题；多机器占用高，说明可能是流量大、代码逻辑问题
- 然后可以通过pprof，使用top、list、web命令，确认哪段逻辑CPU消耗高
- 原因：
	- 请求流量大
	- 低效循环
	- 高频复杂计算：大量数据排序去重、高频序列化操作、复杂正则匹配等
	- 协程泄漏
		- 低效循环、复杂计算
		- 过多导致调度开销大
	- 原子操作多，导致自旋频繁
	- 短生命周期对象频繁创建、大内存对象频繁分配，导致GC频繁

# 5 内存泄漏问题排查
1. 资源未关闭
	1. 文件、网络连接、数据库连接等未正确关闭
	2. 这些资源通常关联底层系统资源，未关闭，Go 的 GC 无法回收其占用的内存
2. 协程泄露
3. 切片或字符串的截取而引发的内存泄漏：
	1. 一个长的字符串被另一个字符串通过切片的方式截取，这两个字符串共用一个底层数组。
	2. 如果截取的字符串很小，但是原来的字符串很大，只要截取的小字符串还在活跃，则大的字符串将不会被回收，这样会造成临时的内存泄漏；
	3. 同理切片的截取也会存在这样的情况。
4. 全局变量或长生命周期对象的不当使用
	1. 全局变量（如 map、切片）或长生命周期对象（如单例）中存储了不再使用的数据，且未及时清理

# 6 程序响应变慢排查
请求耗时暴涨（响应变慢）的核心排查思路是 **「端到端分层定位」**—— 不能只局限于应用程序本身，还要覆盖「网络链路→依赖服务→系统资源→应用内部→配置参数」全链路。
每个环节都要对应「**具体工具 + 指标 + 常见原因**」，比如提到 “查数据库” 时，要说明用`show processlist`看慢查询、`explain`分析索引，而非只说 “看数据库”—— 这才是面试官想要的 “可落地的排查方案”。

## 6.1 第一步：先定位 “慢的范围”（缩小排查边界，避免盲目）
首先要明确 “慢” 是局部还是全局，避免一上来就查代码 /pprof，浪费时间。
核心方法：从监控 / 日志中提取「4 个关键维度」：

| 维度   | 排查点（看什么指标）                                        | 工具 / 方式                                          | 作用                                                           |
| ---- | ------------------------------------------------- | ------------------------------------------------ | ------------------------------------------------------------ |
| 接口维度 | 单个接口 vs 全量接口慢？<br><br>耗时分位值（P50/P90/P99）是否均高？     | 监控面板（Prometheus<br>+Grafana）、<br>日志聚合（ELK/ Loki） | 若只有单个接口慢→聚焦该接口的逻辑 / 依赖；<br><br>若全量慢→系统资源 / 全局依赖问题            |
| 实例维度 | 单台实例 vs 全集群实例慢？<br><br>慢实例的资源使用率是否异常？             | 集群监控（K8s Dashboard / 云厂商监控）、节点监控                 | 单实例慢→节点资源 / 实例配置问题；<br><br>全集群慢→流量 / 依赖服务 / 全局配置 / 程序逻辑问题    |
| 时间维度 | 突然变慢（突发）vs 逐渐变慢（累积）？<br><br>是否和流量 / 发布 / 配置变更强相关？ | 监控时序图（对比 QPS、发布记录、配置变更日志）                        | 突发慢→流量突增 / 依赖服务故障 / 网络抖动 / 发布了慢的代码；<br><br>逐渐变慢→内存泄漏 / 连接池耗尽 |
| 错误维度 | 慢的同时是否伴随错误率上涨？（如 5xx/4xx）                         | 监控面板（错误率指标）、应用日志（错误栈）                            | 若错误率 + 耗时同步涨→依赖服务不可用 / 接口逻辑报错重试；<br><br>无错误率涨→纯性能瓶颈          |

## 6.2 第二步：排查 “外部依赖”（最常见原因，先排除）
请求慢 80% 的原因是「外部依赖故障」，而非应用本身。需逐一排查所有依赖的服务：

### 6.2.1 场景 1：数据库（MySQL/PostgreSQL 等）
排查方法：
1. 查数据库耗时：从应用日志 / 监控中提取「数据库操作耗时」（如 SQL 执行时间），看是否超过阈值（如正常 10ms，现在 500ms）；
2. 查慢查询：
    - MySQL：执行 `show processlist;` 看当前运行的 SQL，执行 `select * from slow_log where time > 1000;` （单位 ms）提取慢查询；
    - 用 `explain + 慢SQL` 分析执行计划：是否走索引（key 字段非 NULL）、rows 字段是否过大（全表扫描）、是否有 join / 排序无索引；
3. 查数据库状态：
    - 连接池：`show status like 'Threads_connected';` （是否超过最大连接数，导致排队）；
    - 锁等待：`show engine innodb status;` （查看行锁 / 表锁等待情况）；
    - 资源使用率：数据库服务器的 CPU / 内存 / 磁盘 IO（用 `top`/`iostat` 查看，是否因资源满导致 SQL 执行慢）。

常见原因：
- 慢 SQL（全表扫描、无索引、复杂 join）；
- 数据库锁竞争（如大量 update 同一行数据，导致行锁等待）；
- 数据库连接池耗尽（应用侧连接池配置过小，请求排队等待连接）；
- 数据库主从延迟（读从库时，数据同步延迟导致查询慢）。

### 6.2.2 场景 2：缓存（Redis/Memcached）
排查方法：
1. 查缓存操作耗时：应用日志中打印 `redis.Do()` 耗时，看是否超过 10ms；
2. 查 Redis 阻塞：
    - 执行 `redis-cli info stats` ：看 `keyspace_hits/keyspace_misses`（缓存命中率是否骤降，导致大量穿透到 DB）；
    - 执行 `redis-cli slowlog get 10` ：提取慢查询（如 `keys/*`、hgetall 大 key、zrange 大集合）；
    - 执行 `redis-cli info clients` ：看 `blocked_clients`（是否有大量客户端因 BLPOP/BRPOP 阻塞）；
3. 查 Redis 资源：Redis 服务器的 CPU（是否因大 key 序列化 / 反序列化占满）、内存（是否达到 maxmemory，频繁淘汰 key）。

常见原因：
- 缓存命中率骤降（key 过期 / 缓存穿透，大量请求打到 DB）；
- Redis 慢操作（大 key 查询 / 删除、复杂命令如 `keys/*`、lua 脚本执行慢）；
- Redis 集群分片不均衡（某分片负载过高）；
- 缓存连接池耗尽（应用侧连接数配置不足）。

### 6.2.3 场景 3：第三方 API / 消息队列（Kafka/RabbitMQ）
排查方法：
1. 查第三方 API 耗时：应用中打印调用第三方接口的耗时（如 HTTP/GRPC 请求），用 `curl -w "%{time_total}\n" -o /dev/null https://third-api.com` 直接测试接口耗时；
2. 查依赖服务状态：第三方 API 的健康检查接口、官方状态页（是否服务降级 / 故障）。
3. 查消息队列状态：
    - Kafka：`kafka-topics.sh --describe --topic xxx --bootstrap-server xxx` （看分区 ISR 是否完整、消费者组 lag 是否过大）；
    - RabbitMQ：管理界面看队列堆积数、消费者 ack 超时、交换机路由失败；

常见原因：
- 第三方 API 超时（服务不可用、网络抖动）；
- 消息队列堆积（消费者消费速度跟不上生产速度，导致请求等待消息处理）；
- 消息队列消费者 ack 超时（处理消息耗时过长，导致队列阻塞）。

## 6.3 第三步：排查 “网络链路”（容易被忽略，关键环节）
网络延迟 / 阻塞是请求慢的 “隐形杀手”，需排查从「客户端→负载均衡→应用服务器→依赖服务」的全链路网络：
排查方法（工具 + 命令，按优先级）：
1. 测试直连耗时（排除负载均衡 / 网关问题）：
    - 在应用服务器上执行 `curl -w "DNS:%{time_namelookup}, 连接:%{time_connect}, 传输:%{time_transfer}, 总耗时:%{time_total}\n" -o /dev/null http://localhost:8080/your-api` （直连本地接口，看耗时是否正常）；
    - 若本地直连快，远程调用慢→负载均衡 / 网关 / 公网网络问题。
2. 排查 DNS 解析：
    - 执行 `nslookup third-api.com` （看解析是否超时、是否解析到异常 IP）；
    - 执行 `dig third-api.com +trace` （排查 DNS 递归解析是否有延迟）。
3. 排查网络延迟 / 丢包：
    - 跨节点测试：`ping -c 10 目标IP` （看丢包率，正常应 < 1%）、`mtr 目标IP` （比 ping 更详细，看路由各节点延迟）；
    - 端口连通性：`telnet 目标IP 端口` （看是否能连通，是否有连接延迟）、`ss -tulwn | grep 端口` （看端口监听状态）。
4. 排查 TCP 连接问题：
    - 执行 `ss -s` （看 TCP 连接状态：ESTABLISHED/CLOSE_WAIT/TIME_WAIT 是否过多）；
    - 若 CLOSE_WAIT 过多→应用未正确关闭 TCP 连接（如 HTTP 客户端未关闭连接池）；
    - 若 TIME_WAIT 过多→端口耗尽，新连接无法建立（需调整内核参数如 net.ipv4.tcp_tw_reuse）。
5. 抓包分析（精准定位网络阻塞）：
    - 执行 `tcpdump -i eth0 host 目标IP and port 8080 -w network.pcap` （抓取接口通信包）；
    - 用 Wireshark 打开 pcap 文件，分析 TCP 三次握手 / 四次挥手耗时、数据包传输延迟。

常见原因：
- DNS 解析超时（如 DNS 服务器故障、域名配置错误）；
- 网络链路延迟（跨区域调用、网关 / 负载均衡瓶颈）；
- TCP 连接泄漏（CLOSE_WAIT 过多，端口耗尽）；
- 防火墙 / 安全组拦截（导致数据包丢弃，重试耗时）。

## 6.4 第四步：排查 “系统资源”（应用运行的基础环境）
若外部依赖和网络都正常，需排查应用服务器的系统资源是否瓶颈。

排查方法（Linux 工具，核心看 4 类资源）：
1. CPU 资源：
    - 命令：`top` （看应用进程 % CPU 是否接近 100%/ 多核总和）、`mpstat -P ALL 1` （看单个 CPU 核是否被占满）；
    - 关键指标：若 % us（用户态 CPU）高→应用计算密集；% sy（内核态 CPU）高→协程调度 / 锁竞争 / 系统调用频繁。
2. 内存资源：
    - 命令：`free -m` （看内存是否耗尽，是否有 Swap 频繁使用）、`vmstat 1` （看 si/so 值，非 0 则 Swap 频繁读写）；
    - 关键指标：Swap 使用率高→内存不足，应用频繁换页，导致耗时暴涨。
3. 磁盘 IO 资源：
    - 命令：`iostat -x 1` （看 % util（磁盘使用率）、await（IO 等待时间））；
    - 关键指标：% util 接近 100%→磁盘 IO 饱和（如应用频繁写日志、刷盘，或磁盘故障）。
4. 网络带宽：
    - 命令：`iftop -i eth0` （看网卡带宽使用率）、`sar -n DEV 1` （看 rxpck/s/txpck/s（包量）、rxkB/s/txkB/s（流量））；
    - 关键指标：带宽使用率接近 100%→网络瓶颈（如大量大文件传输、日志刷屏）。

常见原因：
- CPU 被占满（如之前提到的死循环、高频计算）；
- 内存不足（Swap 频繁使用，应用 GC 耗时增加）；
- 磁盘 IO 饱和（应用日志输出过多、数据库 / 缓存刷盘频繁）；
- 网络带宽耗尽（大流量请求、文件上传下载无限流）。

## 6.5 第五步：排查 “应用内部”（不止 pprof）
若前面 4 步都排除，再查应用本身。当然，如果第一步定位中发现有代码变更等逻辑问题，也可以优先看下应用内部。
此时 pprof 是核心，但要结合「日志追踪」「上下文分析」，而非只看调用栈。

### 6.5.1 用 pprof 定位耗时逻辑（精准到函数 / 代码行）
- 采集 CPU 耗时（看计算密集型逻辑）：交互模式下用 `top`（看耗时前 N 函数）、`list 函数名`（看代码行耗时）、`web`（火焰图）。
- 采集阻塞耗时（看 channel / 锁 / IO 等待）block：重点看 `sync.Mutex.Lock`、`chan receive` 等阻塞函数，是否有大量协程等待。
- 采集协程状态（看是否协程泄漏导致调度慢）：若协程数过万，且大量处于`runnable`状态→调度开销大，请求排队。

应用内部常见原因（pprof 可定位）：
- 无退出条件的循环（死循环 / 低效循环）；
- 锁竞争激烈（大量协程抢同一把 Mutex，自旋 / 等待耗时）；
- 协程泄漏（协程数暴增，调度器压力大）；
- 大量数据处理
- 高频序列化 / 反序列化（如 JSON/Protobuf 处理大对象）；
- 复杂数据结构操作（如超大切片排序、map 遍历删除）。

### 6.5.2 日志追踪（定位慢请求的完整链路）
在应用中打印「请求全链路耗时」，包括每个步骤的耗时（如参数解析、缓存查询、DB 操作、第三方调用）：
- 筛选出慢请求的`request_id`，追踪所有步骤的耗时；
- 若某步骤（如`redis_get`）耗时突增→回到第二步排查对应依赖；
- 若所有步骤都快，但总耗时高→可能是协程调度慢（协程数过多）或上下文切换频繁。

### 6.5.3 上下文与超时控制（容易忽略的 “隐形耗时”）
- 上下文未传递（父请求超时，子协程仍在执行，导致后续请求排队）；
- 超时设置过大（如 HTTP 超时设为 30s，依赖服务故障时，请求一直等待直到超时）；
- 无超时的阻塞操作（如`<-ch`无接收者，协程永久阻塞，占用资源）。

### 6.5.4 连接池配置（应用侧资源瓶颈）
- HTTP 连接池：`http.Client`是否复用连接？`MaxIdleConns`（最大空闲连接）、`MaxConnsPerHost`（每个主机最大连接）是否过小？
- 数据库连接池：`sql.Open`的`SetMaxOpenConns`（最大打开连接）、`SetMaxIdleConns`（最大空闲连接）是否匹配 QPS？
- 缓存连接池：Redis / 其他缓存的连接数是否足够？是否有连接泄漏（未归还连接池）？

## 6.6 第六步：排查 “配置参数”（容易被忽略的优化点）
部分请求慢是因为「配置参数不合理」，而非代码问题，需检查：
### 6.6.1 应用配置
- `GOMAXPROCS`：是否设置为 CPU 核心数（默认是，若手动改小→并发调度慢）；
- HTTP 服务配置：`ReadTimeout`/`WriteTimeout`（是否过小导致频繁超时重试，或过大导致请求挂起）；
- 垃圾回收配置：`GOGC`（默认 100，若改小→GC 频繁，耗时增加；改大→内存占用高，GC 单次耗时高）。

### 6.6.2 依赖服务配置
- 数据库连接池：`SetConnMaxLifetime`（连接最大存活时间）是否过短，导致频繁创建销毁连接；
- Redis 配置：`maxmemory-policy`（内存淘汰策略）是否合理，是否频繁淘汰热点 key；
- 负载均衡配置：是否开启会话保持（粘性会话），导致某台实例负载过高。

## 6.7 应急措施（线上优先保障可用性）
排查的同时，需同步执行应急措施，避免服务雪崩：
1. 降级：关闭非核心功能（如推荐、统计），减少依赖调用；
2. 限流：对慢接口设置限流（如用`golang.org/x/sync/semaphore`），避免大流量压垮服务；
3. 切换依赖：若第三方 API / 从库慢，切换到备用节点 / 主库（需评估数据一致性）；
4. 重启实例：若为单实例慢（如内存泄漏、协程泄漏），临时重启缓解，为排查争取时间。
5. 回滚代码：近期变更引起的问题

## 6.8 总结
- 首先需要确认变慢的范围：
	- 单个或几个接口慢还是所有接口慢：前者可能是代码逻辑或依赖问题，后者可能是流量大的问题
	- 单个机器慢还是所有机器慢：前者可能是机器配置问题，后者可能是流量大、代码逻辑或依赖问题
	- 有没有错误：如果报错很多，那可能是依赖问题
	- 慢的情况是缓慢增长还是突增：缓慢增长，那可能是内存、协程泄漏的问题；突增，那可能是QPS高、流量大、依赖或线上变更问题
- 通过定位慢的范围，然后根据情况先看下程序问题或依赖问题
	- 系统资源：
		- CPU 被占满：如死循环、高频计算、协程泄漏；
		- 内存不足：内存泄漏；
		- 磁盘 IO 饱和：如应用日志输出过多、文件操作频繁；
		- 网络带宽耗尽：大流量请求、文件上传下载无限流。
	- 程序问题：
		- 近期变更：如果近期有代码或配置变更，那就优先看下改动的部分是不是有问题
		- pprof：然后可以用 pprof 定位看下CPU耗时和协程状态，定位慢的逻辑，是否有协程泄漏、锁竞争激烈、低效循环、大量数据处理等问题
		- 日志：同时看下日志，筛选慢的请求，排查其中每个步骤的耗时，看是哪里慢
	- 依赖问题
		- 数据库：查看有没有慢查询、数据库资源占用情况怎么样、数据库连接池有没有被打满
		- 缓存：查看有没有慢查询、缓存命中率怎么样、缓存负载如何
		- 消息队列：看看有没有消费堆积的情况
		- 三方API：是不是下游比较慢，频繁超时
- 应急措施：
	- 回滚代码：近期变更
	- 限流、降级：QPS高
	- 重启实例：泄漏
	- 切换依赖：第三方慢，看能不能切换节点、备库之类的

---
# 7 引用