2025-10-31 10:54
Status: #idea
Tags:

# 1 编码
## 1.1 字符编码
### 1.1.1 Unicode
在很久以前，世界还是比较简单的，起码计算机世界就只有一个ASCII字符集：美国信息交换标准代码。ASCII，更准确地说是美国的ASCII，使用7bit来表示128个字符：包含英文字母的大小写、数字、各种标点符号和设备控制符。对于早期的计算机程序来说，这些就足够了，但是这也导致了世界上很多其他地区的用户无法直接使用自己的符号系统。随着互联网的发展，混合多种语言的数据变得很常见。
答案就是使用Unicode（ [http://unicode.org](http://unicode.org/) ），它收集了这个世界上所有的符号系统，包括重音符号和其它变音符号，制表符和回车符，还有很多神秘的符号，每个符号都分配一个唯一的Unicode码点，Unicode码点对应Go语言中的rune整数类型（译注：rune是int32等价类型）。
在第八版本的Unicode标准里收集了超过120,000个字符，涵盖超过100多种语言。这些在计算机程序和数据中是如何体现的呢？通用的表示一个Unicode码点的数据类型是int32，也就是Go语言中rune对应的类型；它的同义词rune符文正是这个意思。
我们可以将一个符文序列表示为一个int32序列。这种编码方式叫UTF-32或UCS-4，每个Unicode码点都使用同样大小的32bit来表示。这种方式比较简单统一，但是它会浪费很多存储空间，因为大多数计算机可读的文本是ASCII字符，本来每个ASCII字符只需要8bit或1字节就能表示。而且即使是常用的字符也远少于65,536个，也就是说用16bit编码方式就能表达常用字符。但是，还有其它更好的编码方法吗？

### 1.1.2 UTF-8
UTF8是一个将Unicode码点编码为字节序列的变长编码。UTF8编码是由Go语言之父Ken Thompson和Rob Pike共同发明的，现在已经是Unicode的标准。
UTF8编码使用1到4个字节来表示每个Unicode码点，ASCII部分字符只使用1个字节，常用字符部分使用2或3个字节表示。每个符号编码后第一个字节的高端bit位用于表示编码总共有多少个字节。如果第一个字节的高端bit为0，则表示对应7bit的ASCII字符，ASCII字符每个字符依然是一个字节，和传统的ASCII编码兼容。如果第一个字节的高端bit是110，则说明需要2个字节；后续的每个高端bit都以10开头。更大的Unicode码点也是采用类似的策略处理。
```
0xxxxxxx                             runes 0-127    (ASCII)
110xxxxx 10xxxxxx                    128-2047       (values <128 unused)
1110xxxx 10xxxxxx 10xxxxxx           2048-65535     (values <2048 unused)
11110xxx 10xxxxxx 10xxxxxx 10xxxxxx  65536-0x10ffff (other values unused)
```

变长的编码无法直接通过索引来访问第n个字符，但是UTF8编码获得了很多额外的优点：
1. 首先UTF8编码比较紧凑，完全兼容ASCII码
2. 并且可以自动同步：它可以通过向前回朔最多3个字节就能确定当前字符编码的开始字节的位置。
3. 它也是一个前缀编码，所以当从左向右解码时不会有任何歧义也并不需要向前查看（译注：像GBK之类的编码，如果不知道起点位置则可能会出现歧义）。
4. 没有任何字符的编码是其它字符编码的子串，或是其它编码序列的字串，因此搜索一个字符时只要搜索它的字节编码序列即可，不用担心前后的上下文会对搜索结果产生干扰。
5. 同时UTF8编码的顺序和Unicode码点的顺序一致，因此可以直接排序UTF8编码序列。
6. 同时因为没有嵌入的NUL(0)字节，可以很好地兼容那些使用NUL作为字符串结尾的编程语言。


# 2 数据分片方式
数据分片就是按照一定的规则，将数据集划分成相互独立正交的数据子集。然后将数据子集分布到不同的节点上，通过设计合理的数据分片规则，可将系统中的数据分布在不同的物理数据库中，达到提升应用系统数据处理速度的目的。
一般来讲，分片算法常见的就是 `Hash` 分片、一致性 `Hash` 分片和按照范围数据分片三种。我们以缓存为例子。

## 2.1 Hash分片
`Hash` 分片的算法就是对缓存的 `Key` 做哈希计算，然后对总的缓存节点个数取余。这个算法最大的优点就是简单易理解，缺点是当增加或者减少缓存节点时，缓存总的节点个数变化造成计算出来的节点发生变化，从而造成缓存失效不可用。
所以我建议你，如果采用这种方法，最好建立在你对于这组缓存命中率下降不敏感，比如下面还有另外一层缓存来兜底的情况下。

## 2.2 一致性Hash分片
用一致性 `Hash` 算法可以很好地解决增加和删减节点时，命中率下降的问题。
在这个算法中，我们将整个 `Hash` 值空间组织成一个虚拟的圆环，然后将缓存节点的 `IP` 地址或者主机名做 `Hash` 取值后，放置在这个圆环上。当我们需要确定某一个 `Key` 需要存取到哪个节点上的时候，先对这个 `Key` 做同样的 Hash 取值，确定在环上的位置，然后按照顺时针方向在环上“行走”，遇到的第一个缓存节点就是要访问的节点。
比方说下面这张图里面，`Key 1` 和 `Key 2` 会落入到 `Node 1` 中，`Key 3`、`Key 4` 会落入到 `Node 2` 中，`Key 5` 落入到 `Node 3` 中，`Key 6` 落入到 `Node 4` 中。
![[image-173.png]]

这时如果在 `Node 1` 和 `Node 2` 之间增加一个 `Node 5`，你可以看到原本命中 `Node 2` 的 `Key 3` 现在命中到 `Node 5`，而其它的 `Key` 都没有变化；同样的道理，如果我们把 `Node 3` 从集群中移除，那么只会影响到 `Key 5` 。所以你看，在增加和删除节点时，只有少量的 `Key` 会 **漂移** 到其它节点上，而大部分的 `Key` 命中的节点还是会保持不变，从而可以保证命中率不会大幅下降。
![[image-174.png]]

不过，事物总有两面性。虽然这个算法对命中率的影响比较小，但它还是存在问题：
- 缓存节点在圆环上分布不平均，会造成部分缓存节点的压力较大；
- 当某个节点故障时，这个节点所要承担的所有访问都会被顺移到另一个节点上，会对后面这个节点造成压力。

极端情况下，比如一个有三个节点 `A、B、C` 承担整体的访问，每个节点的访问量平均，`A` 故障后，`B` 将承担双倍的压力（A 和 B 的全部请求），当 `B` 承担不了流量 `Crash` 后，`C` 也将因为要承担原先三倍的流量而 `Crash`，这就造成了整体缓存系统的雪崩。
你可以在一致性 `Hash` 算法中引入**虚拟节点**的概念。**它将一个缓存节点计算多个 `Hash` 值分散到圆环的不同位置，这样既实现了数据的平均**，而且当某一个节点故障或者退出的时候，它原先承担的 `Key` 将以更加平均的方式分配到其他节点上，从而避免雪崩的发生。

### 2.2.1 一致性 Hash 算法的脏数据问题
为什么会产生脏数据呢？ 比方说，在集群中有两个节点 `A` 和 `B`，客户端初始写入一个 `Key` 为 k，值为 `3` 的缓存数据到 `Cache A` 中。这时如果要更新 `k` 的值为 `4`，但是缓存 `A` 恰好和客户端连接出现了问题，那这次写入请求会写入到 `Cache B` 中。接下来缓存 `A` 和客户端的连接恢复，当客户端要获取 `k` 的值时，就会获取到存在 `Cache A` 中的脏数据 3，而不是 `Cache B` 中的 `4`。
所以，**在使用一致性 `Hash` 算法时一定要设置缓存的过期时间，这样当发生漂移时，之前存储的脏数据可能已经过期，就可以减少存在脏数据的几率**。

## 2.3 按照数据范围分片
常见场景就是按照 `时间区间` 或 `ID区间` 来切分。例如：按日期将不同月甚至是日的数据分散到不同的库中；将 `userId` 为 `1~9999` 的记录分到第一个库， `10000~20000` 的分到第二个库，以此类推。
某种意义上，某些系统中使用的 **"冷热数据分离"** ，将一些使用较少的历史数据迁移到其他库中，业务功能上只提供热点数据的查询，也是类似的实践。

这样的优点在于：
- 单表大小可控
- 天然便于水平扩展，后期如果想对整个分片集群扩容时，只需要添加节点即可，无需对其他分片的数据进行迁移
- 使用分片字段进行范围查找时，连续分片可快速定位分片进行快速查询，有效避免跨分片查询的问题。

缺点：
- 热点数据成为性能瓶颈。连续分片可能存在数据热点，例如按时间字段分片，有些分片存储最近时间段内的数据，可能会被频繁的读写，而有些分片存储的历史数据，则很少被查询

## 2.4 哈希槽分片

# 3 IO 模型
## 3.1 IO 的核心两个阶段（所有 IO 模型的底层基础）
不管是网络 IO（如 Kitex/Thrift 的 TCP 连接）还是文件 IO，核心都分为**两个阶段**，所有 IO 模型的差异都集中在「第一阶段」：
1. **等待数据就绪**（核心耗时阶段）：比如 TCP 连接中，等待客户端发送数据到内核缓冲区（网卡→内核态内存）；
2. **数据拷贝**（耗时短）：把内核缓冲区的数据拷贝到用户态内存（应用程序能访问的内存）。

阻塞 / 非阻塞 IO 的区别，在于「等待数据就绪」阶段是否阻塞线程；而 IO 多路复用是**管理多个非阻塞 IO 的 “高效管家”**。

## 3.2 阻塞 IO（Blocking IO，BIO）
发起 IO 操作后，**线程 / 进程会被挂起（阻塞）**，直到 “数据就绪 + 数据拷贝” 完成才返回；在等待阶段，线程完全无法做任何其他事。
例如原生 Thrift 默认用 BIO + 线程池：每个 TCP 连接分配一个线程，线程阻塞等待数据。高并发下（比如 10000 个连接），需要创建 10000 个线程，带来两个致命问题：
- 线程上下文切换开销极大（CPU 大量时间浪费在切换线程，而非处理业务）；
- 线程栈占用大量内存（每个 Go 线程栈默认 2MB，10000 个线程就是 20GB）。

## 3.3 非阻塞 IO（Non-Blocking IO，NIO）—— 解决 “等待阶段阻塞” 的问题
发起 IO 操作后，**无论数据是否就绪，都立即返回**：
- 数据未就绪：返回错误码（Linux 下是`EAGAIN/EWOULDBLOCK`），线程不阻塞，可去做其他事；
- 数据已就绪：立即完成数据拷贝，返回成功。

例如网络 IO：
1. 先把socket设置为非阻塞模式（fcntl设置O_NONBLOCK）；
2. 线程调用 recv()，内核检查缓冲区：
	- 无数据：返回EAGAIN，线程不阻塞，去处理其他连接；
	- 有数据：拷贝数据到用户态 → recv()返回，线程处理请求；
3. 线程定期轮询（或结合多路复用），再次调用recv()检查数据。

优缺点：
- 优点：线程不阻塞，一个线程可处理多个连接；
- 缺点：纯轮询会浪费 CPU（比如线程频繁调用 recv ()，但大部分时候返回 EAGAIN）—— 这就是为什么非阻塞 IO 必须配合「IO 多路复用」使用。

## 3.4 IO 多路复用（IO Multiplexing）—— 非阻塞 IO 的 “高效管家”
核心是**用一个线程 / 进程管理多个非阻塞 IO 通道（文件描述符 fd）**，通过内核提供的接口（Linux：epoll；BSD：kqueue；Windows：IOCP），让内核帮我们监听多个 fd 的 “数据就绪事件”。只有当 fd 就绪时，才通知应用线程处理 —— 彻底解决非阻塞 IO “轮询浪费 CPU” 的问题。
把 “轮询多个 fd 是否就绪” 的工作从「用户态线程」交给「内核」，内核更高效（内核可直接感知硬件 / 网络状态），用户态线程只需处理 “内核通知的就绪 fd”，CPU 利用率拉满。

### 3.4.1 Linux 下的三种多路复用模型

| 模型     | 核心原理                       | 缺点                      | 适用场景       |
| ------ | -------------------------- | ----------------------- | ---------- |
| select | 监听 fd 集合（最多 1024 个），轮询检查就绪 | 1. fd 数量上限；2. 轮询遍历所有 fd | 低并发、兼容性要求高 |
| poll   | 用链表存储 fd，突破数量上限            | 仍需轮询遍历所有 fd             | 中低并发       |
| epoll  | 红黑树存 fd + 就绪链表，事件驱动        | 无数量上限，仅遍历就绪 fd          | 高并发        |

### 3.4.2 epoll 的底层工作机制
epoll 是 Linux 下最高效的多路复用模型，分为三个核心步骤：
1. 创建epoll实例 → epoll_create()：内核创建两个核心结构：
	- 红黑树：存储需要监听的fd和事件（如读事件EPOLLIN、写事件EPOLLOUT）；
		- 高并发下会频繁添加 / 删除 / 修改 fd，红黑树的增删改查时间复杂度是 O (logn)，远优于链表的 O (n)；
		- 红黑树是平衡二叉树，避免极端情况下的性能退化。
	- 就绪链表：存储已经就绪的fd（内核主动把就绪fd加入链表）。
2. 注册监听事件 → epoll_ctl()：把需要监听的fd（如8888端口）、客户端连接fd，注册到epoll的红黑树中，指定监听“读事件”（客户端发数据）。
3. 等待就绪事件 → epoll_wait()：线程调用epoll_wait()，内核阻塞（仅这一个线程阻塞），直到：
	- 某个fd就绪（如客户端发数据到内核缓冲区）；
	- 超时/被中断。
	内核把就绪fd从红黑树移到就绪链表，epoll_wait()返回就绪fd列表。
4. 处理就绪fd：调用非阻塞recv()拷贝数据，处理请求—— 仅处理“有数据的fd”，无任何无效轮询。

#### 3.4.2.1 epoll 的内核核心数据结构
当调用`epoll_create()`创建 epoll 实例时，内核会为这个实例初始化一个核心结构体（简化版）：
```c
struct eventpoll {
    struct rb_root rbr;          // 红黑树：存储所有需要监听的fd及事件（核心是epitem节点）
    struct list_head rdllist;    // 就绪链表：存储已经就绪的fd对应的epitem节点
    wait_queue_head_t wq;        // epoll的等待队列：epoll_wait()时挂起当前线程
    // 其他：如回调函数、锁等
};

// 红黑树的每个节点（epitem）—— 绑定fd、事件、回调等关键信息
struct epitem {
    struct rb_node rbn;          // 红黑树节点（用于插入红黑树）
    struct list_head rdllink;    // 就绪链表节点（用于插入就绪链表）
    int fd;                      // 被监听的文件描述符（如socket fd）
    struct epoll_event event;    // 监听的事件（如EPOLLIN读事件）
    // 关键：fd的等待队列回调函数（fd就绪时内核触发）
    void (*callback)(struct epitem *epi, unsigned int events);
};
```
简单说：
- 红黑树（rbr）：是 epoll 的 “监听注册表”，存所有需要监听的 fd；
- 就绪链表（rdllist）：是 epoll 的 “就绪待处理列表”，存已经有数据 / 事件的 fd；
- epitem：是连接红黑树和就绪链表的 “桥梁”，每个被监听的 fd 对应一个 epitem。
	- 注意，fd 就绪时**不是从红黑树中 “取出”**，而是红黑树中的 fd 节点会被 “关联” 到就绪链表，红黑树中的节点始终保留（直到主动删除）。

#### 3.4.2.2 详细流程
以服务端监听 8888 端口、处理客户端连接的场景为例，拆解红黑树和就绪链表的工作流程：
##### 3.4.2.2.1 阶段 1：创建 epoll 实例—— 初始化 “注册表 + 待处理列表”
服务端启动时，底层会调用`epoll_create(1)`：
1. 内核为 epoll 实例分配内存，初始化`eventpoll`结构体；
2. 初始化红黑树（rbr）为空，就绪链表（rdllist）为空；
3. 返回一个 epoll_fd（如 3）给用户态，后续通过这个 fd 操作 epoll 实例。

##### 3.4.2.2.2 阶段 2：注册 fd 到红黑树—— 把监听目标加入 “注册表”
服务端会把两类 fd 注册到 epoll：
- 监听 fd：8888 端口的 socket fd（如 fd=5），监听 “连接事件”；
- 客户端连接 fd：新建立的 TCP 连接 fd（如 fd=6），监听 “读事件”。

以注册监听 fd=5 为例，`epoll_ctl(epoll_fd, EPOLL_CTL_ADD, 5, &ev)`的内核流程：
1. **用户态传参校验**：内核检查 epoll_fd、fd=5 是否合法（如 fd 是否打开、是否已注册）；
2. **创建 epitem 节点**：内核为 fd=5 创建`epitem`节点，填充：
    - fd=5、事件 = EPOLL_CTL_ADD（连接事件）；
    - 绑定回调函数`ep_poll_callback`（epoll 内置的回调，核心是把节点加入就绪链表）；
3. **插入红黑树**：把 epitem 的`rb_node`插入`eventpoll`的红黑树（rbr）—— 红黑树按 fd 排序，插入时间复杂度 O (logn)，适合高并发；
4. **注册回调到 fd 的等待队列**：这是最关键的一步！每个 socket fd 都有自己的 “等待队列”，内核会把 epitem 的回调函数`ep_poll_callback`加入 fd=5 的等待队列。当 fd=5 就绪（如客户端发起连接）时，内核会自动调用这个回调函数。

##### 3.4.2.2.3 阶段 3：等待就绪事件（epoll_wait）—— 线程挂起，等待内核通知
服务端的 “主线程” 调用`epoll_wait(epoll_fd, events, MAX_EVENTS, -1)`，内核检查 epoll 实例的就绪链表（rdllist）：
- 如果就绪链表非空：直接把链表中的 epitem 节点的 fd / 事件拷贝到用户态的 events 数组，返回就绪 fd 数量；
- 如果就绪链表为空：把当前线程挂到 epoll 实例的等待队列（wq），并让出 CPU（线程阻塞，但仅这一个线程阻塞，这是 epoll 高效的核心）。

##### 3.4.2.2.4 阶段 4：fd 就绪，内核触发回调，填充就绪链表 —— 核心联动环节
当客户端向服务端的 8888 端口发起连接，fd=5（监听 fd）就绪（内核缓冲区有连接请求），内核执行以下操作：
1. **内核感知 fd 就绪**：TCP 协议栈检测到客户端 SYN 包，把连接请求放到 fd=5 的内核缓冲区，标记 fd=5 为 “读就绪”；
2. **触发回调函数**：内核遍历 fd=5 的读等待队列，执行其中的回调函数（即 epitem 绑定的`ep_poll_callback`）；
3. **填充就绪链表**：`ep_poll_callback`会把 fd=5 对应的 epitem 节点，通过`rdllink`插入到 epoll 实例的就绪链表（rdllist）；
4. **唤醒等待线程**：内核唤醒挂在 epoll 等待队列（wq）的服务端主线程，让`epoll_wait()`从阻塞状态返回。

##### 3.4.2.2.5 阶段 5：处理就绪 fd，红黑树 / 就绪链表的后续状态
1. `epoll_wait()`返回：内核把就绪链表（rdllist）中的 fd（如 5）和事件（EPOLLIN）拷贝到用户态 events 数组，清空就绪链表（或标记节点为 “已处理”）；
2. 服务端处理 fd=5：主线程接受客户端连接，创建新的连接 fd=6，重复阶段 2 把 fd=6 注册到红黑树；
3. 红黑树的持久化：fd=5 始终在红黑树中（直到调用`epoll_ctl EPOLL_CTL_DEL`删除），继续监听新的连接请求；
4. 就绪链表的复用：后续 fd=6 就绪（客户端发 RPC 请求）时，重复阶段 4，fd=6 的 epitem 节点被加入就绪链表，等待处理。

#### 3.4.2.3 epoll 的两个触发模式
- **水平触发（LT，默认）**：fd 就绪后，只要数据没读完，每次 epoll_wait () 都会通知；
- **边缘触发（ET，Kitex 优化用）**：fd 就绪后，仅通知一次，需一次性读完所有数据。后续除非有新数据写入，否则不再通知 —— 效率更高，减少重复通知。

**ET 是 “高性能但高门槛”，LT 是 “易用性优先且容错性高”** —— 两者并非替代关系，而是适配不同场景的设计，epoll 保留两种模式是为了兼顾「极致性能」和「开发效率 / 稳定性」。

##### 3.4.2.3.1 为什么需要 LT？——ET 的 “高性能” 有不可忽视的代价
ET 的性能优势（减少通知次数）是建立在「极高的开发规范」之上的，一旦规范没遵守，就会导致严重问题；而 LT 正是为了解决 ET 的 “高门槛”，满足大部分场景的 “易用性 / 稳定性” 需求。

ET 的核心代价（必须规避，否则出大问题）：
- **必须搭配非阻塞 IO**：如果用阻塞 IO，ET 模式下若一次性没读完数据，再次调用`recv()`会因无新数据而阻塞线程，直接卡死；
- **必须一次性读完所有数据**：要**循环调用**`recv()`直到返回`EAGAIN`（无数据），否则残留数据不会再被通知，导致 “数据丢失”；
- **开发成本高**：必须区分 “应用层包已完整” 和 “内核缓冲区已空”—— 比如 Thrift 帧已读完 104 字节，但内核缓冲区还有新的帧数据，还需继续读。相当于需在循环读中同时判断 “帧是否完整” 和 “是否还有数据”
- **调试难度大**：数据丢失问题（比如漏读）难以复现，排查成本高。

LT 的存在价值（解决 ET 的痛点）：
- **易用性拉满**：无需严格 “一次性读全数据”，即使这次只读了部分数据，下次`epoll_wait()`仍会通知，开发时无需处理复杂的循环读取逻辑；
- **容错性高**：即使代码有疏漏（比如漏读了几字节），也不会导致数据丢失，适合对 “稳定性优先于极致性能” 的场景；
- **兼容低成本**：从`select/poll`迁移到 epoll 时，直接用 LT 模式几乎无需修改原有 IO 逻辑（select/poll 本质是类 LT 的轮询），迁移成本极低；
- **支持阻塞 IO**：LT 可搭配阻塞 IO 使用（比如简单的小服务），无需额外设置非阻塞标志，进一步降低开发门槛。

##### 3.4.2.3.2 注意 epoll 不 “绑定” 非阻塞 IO
epoll 是**IO 多路复用机制**（核心是 “帮你监听多个 fd 的就绪事件”），而非 “强制要求 fd 是非阻塞的”——fd 的 “阻塞 / 非阻塞” 是独立属性（通过`fcntl`设置），和 epoll 的触发模式（LT/ET）是两层逻辑：
- epoll 的核心：告诉你 “哪些 fd 有数据 / 事件”；
- fd 的阻塞属性：决定 “当你调用`recv()`/`read()`时，如果 fd 没数据，是否阻塞线程”。

LT 模式能搭配阻塞 IO 的核心原因是：**LT 会持续通知 “就绪的 fd”，而我们只在 epoll_wait () 返回 “fd 就绪” 后，才调用阻塞 IO 函数 —— 此时 fd 已就绪，阻塞 IO 函数会立即完成，不会真的阻塞**。
而 ET 你需要自己循环调用 `recv()`，会因无新数据而阻塞线程，直接卡死。

##### 3.4.2.3.3 LT 的例子
下面是一个 LT 的代码例子：
```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <sys/epoll.h>

#define MAX_EVENTS 10
#define PORT 8888

int main() {
    // 1. 创建监听socket（默认是“阻塞模式”，无需额外设置）
    int listen_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (listen_fd == -1) { perror("socket"); exit(1); }

    // 2. 绑定+监听端口
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;
    addr.sin_addr.s_addr = INADDR_ANY;
    addr.sin_port = htons(PORT);
    bind(listen_fd, (struct sockaddr*)&addr, sizeof(addr));
    listen(listen_fd, 10);

    // 3. 创建epoll实例（epoll_fd用于后续操作）
    int epoll_fd = epoll_create1(0);
    if (epoll_fd == -1) { perror("epoll_create1"); exit(1); }

    // 4. 注册监听fd到epoll（默认LT模式，无需额外设置EPOLLET）
    struct epoll_event ev;
    ev.events = EPOLLIN;  // 监听“读事件”（客户端连接/发数据）
    ev.data.fd = listen_fd;
    epoll_ctl(epoll_fd, EPOLL_CTL_ADD, listen_fd, &ev);

    // 5. epoll循环等待事件
    struct epoll_event events[MAX_EVENTS];
    while (1) {
        // 阻塞等待就绪事件（LT模式核心：只返回就绪的fd）
        int nfds = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);
        if (nfds == -1) { perror("epoll_wait"); exit(1); }

        // 遍历所有就绪的fd
        for (int i = 0; i < nfds; i++) {
            int fd = events[i].data.fd;

            // 场景1：监听fd就绪（有新客户端连接）
            if (fd == listen_fd) {
                // accept是“阻塞函数”，但此时listen_fd已就绪（有连接），所以立即返回
                int conn_fd = accept(listen_fd, NULL, NULL);
                if (conn_fd == -1) { perror("accept"); continue; }
                printf("新客户端连接：conn_fd = %d\n", conn_fd);

                // 注册新连接fd到epoll（默认LT模式，conn_fd也是阻塞的）
                ev.events = EPOLLIN;
                ev.data.fd = conn_fd;
                epoll_ctl(epoll_fd, EPOLL_CTL_ADD, conn_fd, &ev);
            }

            // 场景2：客户端连接fd就绪（有数据发送）
            else {
                char buf[1024] = {0};
                // recv是“阻塞函数”，但此时conn_fd已就绪（有数据），所以立即返回
                ssize_t n = recv(fd, buf, sizeof(buf)-1, 0);
                if (n == -1) { perror("recv"); continue; }
                if (n == 0) {  // 客户端断开
                    printf("客户端断开：conn_fd = %d\n", fd);
                    epoll_ctl(epoll_fd, EPOLL_CTL_DEL, fd, NULL);
                    close(fd);
                    continue;
                }
                printf("收到数据（conn_fd=%d）：%s\n", fd, buf);

                // 【关键】即使只读取了部分数据（比如buf设为1字节），LT会再次通知
                // 比如把buf改成char buf[1]，下次epoll_wait仍会返回该fd，直到数据读完
            }
        }
    }

    close(listen_fd);
    close(epoll_fd);
    return 0;
}
```

## 3.5 阻塞 IO vs 非阻塞 IO vs IO 多路复用（核心差异）
| 维度       | 阻塞 IO（BIO） | 非阻塞 IO（NIO）    | IO 多路复用（epoll）      |
| -------- | ---------- | -------------- | ------------------- |
| 等待阶段是否阻塞 | 阻塞（线程挂起）   | 不阻塞（返回 EAGAIN） | 仅管理线程阻塞（epoll_wait） |
| CPU 利用率  | 低（线程闲置）    | 低（纯轮询浪费）       | 高（仅处理就绪 fd）         |
| 单线程处理连接数 | 1 个        | 多个（但轮询耗 CPU）   | 上万级                 |

## 3.6 总结
- IO（网络或文件）都有两个阶段：
	1. 等待数据就绪：例如TCP等待客户端数据
	2. 拷贝数据：把数据从内核拷贝到用户空间，耗时短
- 阻塞IO：两个阶段都是阻塞的，用户线程直到数据拷贝完毕才会从阻塞中恢复
- 非阻塞式IO：第一阶段不会阻塞，调用 `recv()` 没有数据时就返回错误码 `EAGAIN`，否则拷贝数据。缺点是需要不断轮询 `recv()`，或者搭配 IO 多路复用
- IO 多路复用：用一个内核线程管理多个 IO 通道（又叫文件描述符 fd），当  fd 就绪时再通知用户线程，避免轮询浪费 CPU。Linux有三种IO多路复用方法：
	- select：监听 fd 集合（最多 1024 个），轮询检查就绪。性能低，有数量上限
	- poll：用链表存储 fd，突破了数量上限，但还是轮询
	- epoll：用红黑树+就绪链表管理 fd，无数量上限，性能很高（只需要处理就绪的 fd）
	- 注意：IO 多路复用也可以和阻塞式 IO 搭配
- epoll 原理：
	- 数据结构：
		- 红黑树存储 fd，增删改查 fd 很快
		- 就绪链表关联红黑树的就绪 fd，让用户线程只需要遍历就绪 fd
	- 工作流程：以服务端监听并读取客户端为例
		1. 创建 epoll 实例：服务端调用 `epoll_create()`
		2. 注册 fd 到红黑树：先调用 `epoll_ctl()` 把监听 fd 插入红黑树，并把回调函数加入到 fd 的等待队列
		3. 等待就绪事件：调用 `epoll_wait()`，如果就绪队列中有就绪的fd，就拷贝到用户线程的 events 数组中；否则阻塞用户线程，把它加入到 epoll 实例的等待队列中
		4. fd 就绪：TCP协议栈把连接请求放到 fd 的内核缓冲区，标记它已就绪；然后内核线程遍历 fd 的等待队列，触发它的回调函数；回调函数把红黑树节点插入到就绪链表，然后唤醒等待队列中阻塞的用户线程
		5. 处理就绪 fd：用户线程接受客户端连接，创建新的连接 fd，再把连接 fd 注册到 epoll 实例，后面的等待&处理流程都是类似的了
	- epoll 的两个触发方式：
		- 水平触发（LT）：默认模式，fd 就绪后，只要数据没读完，每次 epoll_wait() 都会通知。用户程序实现简单，不需要自己轮询 `recv()`，并且还可以使用阻塞式IO
		- 边缘触发（ET）：fd 就绪后，用户程序必须轮询 `recv()` 读完所有数据，下次不会再通知。它性能更高，减少重复通知。但是需要用户程序自己处理轮询等操作，并且不支持阻塞式IO（最后一次 `revc()`会一直阻塞）

# 4 限流
## 4.1 漏桶
- 核心思想：类比 “底部有固定漏洞的桶”：请求（水流）进入桶中，桶以**固定速率**将请求漏出（处理），若请求量超过桶容量，多余请求直接溢出（拒绝或排队）。
- 数据结构：
	- 桶容量（Bucket Capacity）：最大可缓存的请求数（溢出阈值）；
	- 漏出速率（Leak Rate）：单位时间内可处理的最大请求数（固定，如 100 QPS）；
	- 当前水量（Current Water）：当前等待处理的请求数。

Go实现：
```go
import (
	"time"
	"errors"
)

// LeakyBucket 漏桶限流结构体
type LeakyBucket struct {
	capacity int           // 桶容量（最大缓冲请求数）
	leakRate int           // 漏出速率（请求/秒）
	bucket   chan struct{} // 模拟桶（缓冲通道）
	closeChan chan struct{}// 关闭信号
}

// NewLeakyBucket 创建漏桶实例
func NewLeakyBucket(capacity, leakRate int) *LeakyBucket {
	lb := &LeakyBucket{
		capacity: capacity,
		leakRate: leakRate,
		bucket:   make(chan struct{}, capacity),
		closeChan: make(chan struct{}),
	}
	// 启动定时器，固定速率漏出（处理请求）
	go lb.leak()
	return lb
}

// leak 固定速率从桶中取出请求（漏出）
func (lb *LeakyBucket) leak() {
	// 计算漏出间隔（秒/请求）
	interval := time.Second / time.Duration(lb.leakRate)
	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			// 每次漏出一个请求（从通道取一个元素）
			select {
			case <-lb.bucket:
			default:
				// 桶空时忽略
			}
		case <-lb.closeChan:
			return
		}
	}
}

// Allow 判断是否允许请求（放入桶中）
func (lb *LeakyBucket) Allow() error {
	select {
	case lb.bucket <- struct{}{}:
		return nil // 放入成功，允许请求
	default:
		return errors.New("too many requests") // 桶满，限流
	}
}

// Close 关闭漏桶
func (lb *LeakyBucket) Close() {
	close(lb.closeChan)
}
```
实现亮点
- 用缓冲通道`bucket`模拟桶，天然支持并发安全（通道操作是原子的）；
- 定时器`time.Ticker`保证固定漏出速率，无需锁或原子操作；
- 支持关闭机制，优雅退出 goroutine。


## 4.2 令牌桶
- 核心思想：类比 “定期生成令牌的桶”：系统以**固定速率**向桶中放入令牌，请求需先从桶中获取令牌才能被处理，无令牌则限流。桶内令牌数不超过容量（令牌可累积）。
- 数据结构：
	- 桶容量（Bucket Capacity）：最大可累积的令牌数（突发流量上限）；
	- 令牌生成速率（Token Rate）：单位时间内生成的令牌数（长期限流阈值，如 100 QPS）；
	- 当前令牌数（Current Tokens）：桶中可用于处理请求的令牌数。

Go实现：
```go
import (
	"sync/atomic"
	"time"
	"errors"
)

// TokenBucket 令牌桶限流结构体
type TokenBucket struct {
	capacity     int64           // 桶容量（最大令牌数）
	tokenRate    int64           // 令牌生成速率（令牌/秒）
	currentTokens int64          // 当前令牌数（原子操作保证并发安全）
	lastRefillTime int64         // 上次补充令牌的时间戳（纳秒，原子操作）
}

// NewTokenBucket 创建令牌桶实例
func NewTokenBucket(capacity, tokenRate int64) *TokenBucket {
	return &TokenBucket{
		capacity:     capacity,
		tokenRate:    tokenRate,
		currentTokens: capacity, // 初始时桶满
		lastRefillTime: time.Now().UnixNano(),
	}
}

// Allow 判断是否允许请求（获取令牌）
func (tb *TokenBucket) Allow() error {
	// 1. 补充令牌（根据时间差计算应生成的令牌数）
	now := time.Now().UnixNano()
	lastTime := atomic.LoadInt64(&tb.lastRefillTime)
	elapsed := now - lastTime // 时间差（纳秒）

	// 计算应生成的令牌数：tokenRate * (elapsed / 1e9)
	tokensToAdd := (elapsed * tb.tokenRate) / 1e9
	if tokensToAdd > 0 {
		// 原子更新上次补充时间和当前令牌数（避免并发冲突）
		atomic.StoreInt64(&tb.lastRefillTime, now)
		// 确保令牌数不超过容量
		atomic.AddInt64(&tb.currentTokens, tokensToAdd)
		if atomic.LoadInt64(&tb.currentTokens) > tb.capacity {
			atomic.StoreInt64(&tb.currentTokens, tb.capacity)
		}
	}

	// 2. 尝试获取1个令牌
	if atomic.AddInt64(&tb.currentTokens, -1) >= 0 {
		return nil // 获取成功
	}
	// 3. 令牌不足，回滚（把刚才减的1加回来）
	atomic.AddInt64(&tb.currentTokens, 1)
	return errors.New("too many requests")
}

// UpdateRate 动态调整令牌生成速率（生产级需求）
func (tb *TokenBucket) UpdateRate(newRate int64) {
	atomic.StoreInt64(&tb.tokenRate, newRate)
}
```
实现亮点
- 无 goroutine、无定时器，纯原子操作，并发安全且性能极高（适合高 QPS 场景）；
- 动态补充令牌，避免定时器的精度误差（如高并发下定时器触发不及时）；
- 支持动态调整令牌速率，满足生产环境中限流阈值动态配置的需求。

## 4.3 漏桶和令牌桶的区别
漏桶算法与令牌桶算法的区别在于：
- 漏桶算法严格限制**输出速率**，不允许突发流量。它属于**刚性限流**，适用于输出端速率敏感场景
- 而令牌算法允许**合理突发流量**（令牌累积），长期速率可控。它属于**弹性限流**，适用于允许短期峰值的场景

漏桶算法适用场景：
- **下游服务速率固定，需避免过载**：
	- 如向数据库写入数据（数据库 QPS 上限固定），用漏桶限制写入速率，防止数据库连接池耗尽；
	- 如调用微信支付 API（第三方限制每秒最多 100 次），用漏桶严格控制请求速率，避免触发第三方限流；
- **流量整形（Traffic Shaping）**：如视频流传输、日志上报，需将突发流量 “拉平” 为平稳流量，避免网络拥堵。

令牌桶算法适用场景：
- **接口限流（允许突发）**：如用户登录、商品查询接口，允许短期峰值（如秒杀开始时的突发请求），但长期速率不超过阈值；
- **网关层限流**：如 API 网关（Kong、Nginx、Go-Zero 网关）对下游服务的限流，支持不同接口配置不同令牌速率，兼顾灵活性和稳定性；
- **分布式限流**：结合 Redis 实现分布式令牌桶（如 Redisson 的 RRateLimiter），解决多实例部署时的限流一致性问题。

## 4.4 TODO：Redis分布式限流

---
# 5 引用